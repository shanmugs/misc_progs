{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating Spam from Ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spam.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly every email user has at some point encountered a \"spam\" email, which is an unsolicited message often advertising a product, containing links to malware, or attempting to scam the recipient. Roughly 80-90% of more than 100 billion emails sent each day are spam emails, most being sent from botnets of malware-infected computers. The remainder of emails are called \"ham\" emails.\n",
    "\n",
    "As a result of the huge number of spam emails being sent across the Internet each day, most email providers offer a spam filter that automatically flags likely spam messages and separates them from the ham. Though these filters use a number of techniques (e.g. looking up the sender in a so-called \"Blackhole List\" that contains IP addresses of likely spammers), most rely heavily on the analysis of the contents of an email via text analytics.\n",
    "\n",
    "In this homework problem, we will build and evaluate a spam filter using a publicly available dataset first described in the 2006 conference paper \"Spam Filtering with Naive Bayes -- Which Naive Bayes?\" by V. Metsis, I. Androutsopoulos, and G. Paliouras. The \"ham\" messages in this dataset come from the inbox of former Enron Managing Director for Research Vincent Kaminski, one of the inboxes in the Enron Corpus. One source of spam messages in this dataset is the SpamAssassin corpus, which contains hand-labeled spam messages contributed by Internet users. The remaining spam was collected by Project Honey Pot, a project that collects spam messages and identifies spammers by publishing email address that humans would know not to contact but that bots might target with spam. The full dataset we will use was constructed as roughly a 75/25 mix of the ham and spam messages.\n",
    "\n",
    "The dataset contains just two fields:\n",
    "\n",
    "    text: The text of the email.\n",
    "    spam: A binary variable indicating if the email was spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.1 - Loading the Dataset\n",
    "Begin by loading the dataset emails.csv into a data frame called emails. Remember to pass the stringsAsFactors=FALSE option when loading the data.\n",
    "\n",
    "**How many emails are in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 3 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>text</th><th scope=col>spam</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>Subject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate like esmark perspicuous ramble is segovia not group try slung kansas tanzania yes chameleon or continuant clothesman no  libretto is chesapeake but tight not waterway herald and hawthorn like chisel morristown superior is deoxyribonucleic not clockwork try hall incredible mcdougall yes hepburn or einsteinian earmark no  sapling is boar but duane not plain palfrey and inflexible like huzzah pepperoni bedtime is nameable not attire try edt chronography optima yes pirogue or diffusion albeit no                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </td><td>1</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>Subject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre - approved for a $ 454 , 169 home loan at a 3 . 72 fixed rate .  this offer is being extended to you unconditionally and your credit is in no way a factor .  to take advantage of this limited time opportunity  all we ask is that you visit our website and complete  the 1 minute post approval form  look foward to hearing from you ,  dorcas pittman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 3 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & text & spam\\\\\n",
       "  & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 \\% satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ not interested . . . \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ \\_ & 1\\\\\n",
       "\t2 & Subject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate like esmark perspicuous ramble is segovia not group try slung kansas tanzania yes chameleon or continuant clothesman no  libretto is chesapeake but tight not waterway herald and hawthorn like chisel morristown superior is deoxyribonucleic not clockwork try hall incredible mcdougall yes hepburn or einsteinian earmark no  sapling is boar but duane not plain palfrey and inflexible like huzzah pepperoni bedtime is nameable not attire try edt chronography optima yes pirogue or diffusion albeit no                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        & 1\\\\\n",
       "\t3 & Subject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre - approved for a \\$ 454 , 169 home loan at a 3 . 72 fixed rate .  this offer is being extended to you unconditionally and your credit is in no way a factor .  to take advantage of this limited time opportunity  all we ask is that you visit our website and complete  the 1 minute post approval form  look foward to hearing from you ,  dorcas pittman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 3 × 2\n",
       "\n",
       "| <!--/--> | text &lt;chr&gt; | spam &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| 1 | Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ | 1 |\n",
       "| 2 | Subject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate like esmark perspicuous ramble is segovia not group try slung kansas tanzania yes chameleon or continuant clothesman no  libretto is chesapeake but tight not waterway herald and hawthorn like chisel morristown superior is deoxyribonucleic not clockwork try hall incredible mcdougall yes hepburn or einsteinian earmark no  sapling is boar but duane not plain palfrey and inflexible like huzzah pepperoni bedtime is nameable not attire try edt chronography optima yes pirogue or diffusion albeit no                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 1 |\n",
       "| 3 | Subject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre - approved for a $ 454 , 169 home loan at a 3 . 72 fixed rate .  this offer is being extended to you unconditionally and your credit is in no way a factor .  to take advantage of this limited time opportunity  all we ask is that you visit our website and complete  the 1 minute post approval form  look foward to hearing from you ,  dorcas pittman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1 Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
       "2 Subject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate like esmark perspicuous ramble is segovia not group try slung kansas tanzania yes chameleon or continuant clothesman no  libretto is chesapeake but tight not waterway herald and hawthorn like chisel morristown superior is deoxyribonucleic not clockwork try hall incredible mcdougall yes hepburn or einsteinian earmark no  sapling is boar but duane not plain palfrey and inflexible like huzzah pepperoni bedtime is nameable not attire try edt chronography optima yes pirogue or diffusion albeit no                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3 Subject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre - approved for a $ 454 , 169 home loan at a 3 . 72 fixed rate .  this offer is being extended to you unconditionally and your credit is in no way a factor .  to take advantage of this limited time opportunity  all we ask is that you visit our website and complete  the 1 minute post approval form  look foward to hearing from you ,  dorcas pittman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "  spam\n",
       "1 1   \n",
       "2 1   \n",
       "3 1   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "emails = read.csv(\"data/emails.csv\", stringsAsFactors=FALSE)\n",
    "\n",
    "head(emails,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t5728 obs. of  2 variables:\n",
      " $ text: chr  \"Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market\"| __truncated__ \"Subject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate like esmar\"| __truncated__ \"Subject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre - approved\"| __truncated__ \"Subject: 4 color printing special  request additional information now ! click here  click here for a printable \"| __truncated__ ...\n",
      " $ spam: int  1 1 1 1 1 1 1 1 1 1 ...\n"
     ]
    }
   ],
   "source": [
    "# Examine the string emails\n",
    "str(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 5728 emails in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2 - Loading the Dataset\n",
    "**How many of the emails are spam?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1 \n",
       "4360 1368 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm = table(emails$spam)\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 1368 emails are spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3 - Loading the Dataset\n",
    "**Which word appears at the beginning of every email in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chr \"Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market\"| __truncated__\n"
     ]
    }
   ],
   "source": [
    "# Examine the string emails that are text\n",
    "str(emails$text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \"Subject\" appears at the beginning of every email in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4 - Loading the Dataset\n",
    "**Could a spam classifier potentially benefit from including the frequency of the word that appears in every email?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The frequency with which it appears might help us differentiate spam from ham, because the existence of some words increases the possibility that that email is a spam, especially if it is combined with other words in the same set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.5 - Loading the Dataset\n",
    "The nchar() function counts the number of characters in a piece of text. **How many characters are in the longest email in the dataset** (where longest is measured in terms of the maximum number of characters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "43952"
      ],
      "text/latex": [
       "43952"
      ],
      "text/markdown": [
       "43952"
      ],
      "text/plain": [
       "[1] 43952"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(nchar(emails$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.6 - Loading the Dataset\n",
    "**Which row contains the shortest email in the dataset?** (Just like in the previous problem, shortest is measured in terms of the fewest number of characters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1992"
      ],
      "text/latex": [
       "1992"
      ],
      "text/markdown": [
       "1992"
      ],
      "text/plain": [
       "[1] 1992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "which.min(nchar(emails$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Preparing the Corpus\n",
    "Follow the standard steps to build and pre-process the corpus:\n",
    "\n",
    "    1) Build a new corpus variable called corpus.\n",
    "\n",
    "    2) Using tm_map, convert the text to lowercase.\n",
    "\n",
    "    3) Using tm_map, remove all punctuation from the corpus.\n",
    "\n",
    "    4) Using tm_map, remove all English stopwords from the corpus.\n",
    "\n",
    "    5) Using tm_map, stem the words in the corpus.\n",
    "\n",
    "    6) Build a document term matrix from the corpus, called dtm.\n",
    "\n",
    "If the code length(stopwords(\"english\")) does not return 174 for you, then please run the line of code in this file, which will store the standard stop words in a variable called sw. When removing stop words, use tm_map(corpus, removeWords, sw) instead of tm_map(corpus, removeWords, stopwords(\"english\")).\n",
    "\n",
    "**How many terms are in dtm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<DocumentTermMatrix (documents: 5728, terms: 28687)>>\n",
       "Non-/sparse entries: 481719/163837417\n",
       "Sparsity           : 100%\n",
       "Maximal term length: 24\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a new corpus variable called corpus\n",
    "corpus = VCorpus(VectorSource(emails$text))\n",
    "\n",
    "# Convert the text to lowercase.\n",
    "corpus = tm_map(corpus, content_transformer(tolower))\n",
    "\n",
    "# Remove all punctuation from the corpus\n",
    "corpus = tm_map(corpus, removePunctuation)\n",
    "\n",
    "# Remove all English stopwords from the corpus\n",
    "corpus = tm_map(corpus, removeWords, stopwords(\"english\"))\n",
    "\n",
    "# Stem the words in the corpus\n",
    "corpus = tm_map(corpus, stemDocument)\n",
    "\n",
    "# Build a document term matrix from the corpus, called dtm\n",
    "dtm = DocumentTermMatrix(corpus)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: There are 28687 terms in dtm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 - Preparing the Corpus\n",
    "To obtain a more reasonable number of terms, limit dtm to contain terms appearing in at least 5% of documents, and store this result as spdtm (don't overwrite dtm, because we will use it in a later step of this homework). **How many terms are in spdtm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<DocumentTermMatrix (documents: 5728, terms: 330)>>\n",
       "Non-/sparse entries: 213551/1676689\n",
       "Sparsity           : 89%\n",
       "Maximal term length: 10\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove the sparse terms\n",
    "spdtm = removeSparseTerms(dtm, 0.95)\n",
    "spdtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: There are 330 terms in spdtm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 - Preparing the Corpus\n",
    "Build a data frame called emailsSparse from spdtm, and use the make.names function to make the variable names of emailsSparse valid.\n",
    "\n",
    "colSums() is an R function that returns the sum of values for each variable in our data frame. Our data frame contains the number of times each word stem (columns) appeared in each email (rows). Therefore, colSums(emailsSparse) returns the number of times a word stem appeared across all the emails in the dataset. **What is the word stem that shows up most frequently across all the emails in the dataset?** Hint: think about how you can use sort() or which.max() to pick out the maximum frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>enron:</strong> 92"
      ],
      "text/latex": [
       "\\textbf{enron:} 92"
      ],
      "text/markdown": [
       "**enron:** 92"
      ],
      "text/plain": [
       "enron \n",
       "   92 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build data frame called emailsSparse from spdtm\n",
    "\n",
    "emailsSparse = as.data.frame(as.matrix(spdtm))\n",
    "\n",
    "colnames(emailsSparse) = make.names(colnames(emailsSparse))\n",
    "\n",
    "# Word stem that is frequent\n",
    "\n",
    "frequency <- colSums(emailsSparse)\n",
    "\n",
    "which.max(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \"Enron\" is the most frequent word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4 - Preparing the Corpus\n",
    "Add a variable called \"spam\" to emailsSparse containing the email spam labels. You can do this by copying over the \"spam\" variable from the original data frame (remember how we did this in the Twitter lecture).\n",
    "\n",
    "**How many word stems appear at least 5000 times in the ham emails in the dataset?** Hint: in this and the next question, remember not to count the dependent variable we just added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>can</dt><dd>3426</dd><dt>thank</dt><dd>3558</dd><dt>com</dt><dd>4444</dd><dt>pleas</dt><dd>4494</dd><dt>kaminski</dt><dd>4801</dd><dt>X2000</dt><dd>4935</dd><dt>hou</dt><dd>5569</dd><dt>will</dt><dd>6802</dd><dt>vinc</dt><dd>8531</dd><dt>subject</dt><dd>8625</dd><dt>ect</dt><dd>11417</dd><dt>enron</dt><dd>13388</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[can] 3426\n",
       "\\item[thank] 3558\n",
       "\\item[com] 4444\n",
       "\\item[pleas] 4494\n",
       "\\item[kaminski] 4801\n",
       "\\item[X2000] 4935\n",
       "\\item[hou] 5569\n",
       "\\item[will] 6802\n",
       "\\item[vinc] 8531\n",
       "\\item[subject] 8625\n",
       "\\item[ect] 11417\n",
       "\\item[enron] 13388\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "can\n",
       ":   3426thank\n",
       ":   3558com\n",
       ":   4444pleas\n",
       ":   4494kaminski\n",
       ":   4801X2000\n",
       ":   4935hou\n",
       ":   5569will\n",
       ":   6802vinc\n",
       ":   8531subject\n",
       ":   8625ect\n",
       ":   11417enron\n",
       ":   13388\n",
       "\n"
      ],
      "text/plain": [
       "     can    thank      com    pleas kaminski    X2000      hou     will \n",
       "    3426     3558     4444     4494     4801     4935     5569     6802 \n",
       "    vinc  subject      ect    enron \n",
       "    8531     8625    11417    13388 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a variable called spam\n",
    "\n",
    "emailsSparse$spam = emails$spam\n",
    "\n",
    "# Sort the ham emails in dataset\n",
    "\n",
    "a = sort((colSums(subset(emailsSparse, spam == 0))))\n",
    "\n",
    "tail(a,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>hou</dt><dd>5569</dd><dt>will</dt><dd>6802</dd><dt>vinc</dt><dd>8531</dd><dt>subject</dt><dd>8625</dd><dt>ect</dt><dd>11417</dd><dt>enron</dt><dd>13388</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[hou] 5569\n",
       "\\item[will] 6802\n",
       "\\item[vinc] 8531\n",
       "\\item[subject] 8625\n",
       "\\item[ect] 11417\n",
       "\\item[enron] 13388\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "hou\n",
       ":   5569will\n",
       ":   6802vinc\n",
       ":   8531subject\n",
       ":   8625ect\n",
       ":   11417enron\n",
       ":   13388\n",
       "\n"
      ],
      "text/plain": [
       "    hou    will    vinc subject     ect   enron \n",
       "   5569    6802    8531    8625   11417   13388 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a[a>=5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 words stems appear at least 5000 times in the ham emails in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5 - Preparing the Corpus\n",
    "**How many word stems appear at least 1000 times in the spam emails in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>get</dt><dd>694</dd><dt>receiv</dt><dd>727</dd><dt>inform</dt><dd>818</dd><dt>can</dt><dd>831</dd><dt>email</dt><dd>865</dd><dt>busi</dt><dd>897</dd><dt>mail</dt><dd>917</dd><dt>com</dt><dd>999</dd><dt>compani</dt><dd>1065</dd><dt>spam</dt><dd>1368</dd><dt>will</dt><dd>1450</dd><dt>subject</dt><dd>1577</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[get] 694\n",
       "\\item[receiv] 727\n",
       "\\item[inform] 818\n",
       "\\item[can] 831\n",
       "\\item[email] 865\n",
       "\\item[busi] 897\n",
       "\\item[mail] 917\n",
       "\\item[com] 999\n",
       "\\item[compani] 1065\n",
       "\\item[spam] 1368\n",
       "\\item[will] 1450\n",
       "\\item[subject] 1577\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "get\n",
       ":   694receiv\n",
       ":   727inform\n",
       ":   818can\n",
       ":   831email\n",
       ":   865busi\n",
       ":   897mail\n",
       ":   917com\n",
       ":   999compani\n",
       ":   1065spam\n",
       ":   1368will\n",
       ":   1450subject\n",
       ":   1577\n",
       "\n"
      ],
      "text/plain": [
       "    get  receiv  inform     can   email    busi    mail     com compani    spam \n",
       "    694     727     818     831     865     897     917     999    1065    1368 \n",
       "   will subject \n",
       "   1450    1577 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the spam emails in the dataset\n",
    "b = sort((colSums(subset(emailsSparse, spam == 1))))\n",
    "\n",
    "tail(b,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>compani</dt><dd>1065</dd><dt>spam</dt><dd>1368</dd><dt>will</dt><dd>1450</dd><dt>subject</dt><dd>1577</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[compani] 1065\n",
       "\\item[spam] 1368\n",
       "\\item[will] 1450\n",
       "\\item[subject] 1577\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "compani\n",
       ":   1065spam\n",
       ":   1368will\n",
       ":   1450subject\n",
       ":   1577\n",
       "\n"
      ],
      "text/plain": [
       "compani    spam    will subject \n",
       "   1065    1368    1450    1577 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b[b>=1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 words stems appear at least 1000 times in the spam emails in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.6 - Preparing the Corpus\n",
    "**The lists of most common words are significantly different between the spam and ham emails. What does this likely imply?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: That we can use frequency of words as a method to separate spam and ham emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.7 - Preparing the Corpus\n",
    "Several of the most common word stems from the ham documents, such as \"enron\", \"hou\" (short for Houston), \"vinc\" (the word stem of \"Vince\") and \"kaminski\", are likely specific to Vincent Kaminski's inbox. **What does this mean about the applicability of the text analytics models we will train for the spam filtering problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We can use this to create a filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1 - Building machine learning models\n",
    "First, convert the dependent variable to a factor with \"emailsSparse$spam = as.factor(emailsSparse$spam)\".\n",
    "\n",
    "Next, set the random seed to 123 and use the sample.split function to split emailsSparse 70/30 into a training set called \"train\" and a testing set called \"test\". Make sure to perform this step on emailsSparse instead of emails.\n",
    "\n",
    "Using the training set, train the following three machine learning models. The models should predict the dependent variable \"spam\", using all other available variables as independent variables. Please be patient, as these models may take a few minutes to train.\n",
    "\n",
    "1) A logistic regression model called spamLog. You may see a warning message here - we'll discuss this more later.\n",
    "\n",
    "2) A CART model called spamCART, using the default parameters to train the model (don't worry about adding minbucket or cp). Remember to add the argument method=\"class\" since this is a binary classification problem.\n",
    "\n",
    "3) A random forest model called spamRF, using the default parameters to train the model (don't worry about specifying ntree or nodesize). Directly before training the random forest model, set the random seed to 123 (even though we've already done this earlier in the problem, it's important to set the seed right before training the model so we all obtain the same results. Keep in mind though that on certain operating systems, your results might still be slightly different).\n",
    "\n",
    "For each model, obtain the predicted spam probabilities for the training set. Be careful to obtain probabilities instead of predicted classes, because we will be using these values to compute training set AUC values. Recall that you can obtain probabilities for CART models by not passing any type parameter to the predict() function, and you can obtain probabilities from a random forest by adding the argument type=\"prob\". For CART and random forest, you need to select the second column of the output of the predict() function, corresponding to the probability of a message being spam.\n",
    "\n",
    "You may have noticed that training the logistic regression model yielded the messages \"algorithm did not converge\" and \"fitted probabilities numerically 0 or 1 occurred\". Both of these messages often indicate overfitting and the first indicates particularly severe overfitting, often to the point that the training set observations are fit perfectly by the model. Let's investigate the predicted probabilities from the logistic regression model.\n",
    "\n",
    "**How many of the training set predicted probabilities from spamLog are less than 0.00001?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dependent variable\n",
    "emailsSparse$spam = as.factor(emailsSparse$spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "set.seed(123)\n",
    "\n",
    "spl = sample.split(emailsSparse$spam, 0.7)\n",
    "\n",
    "train = subset(emailsSparse, spl == TRUE)\n",
    "test = subset(emailsSparse, spl == FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"\n",
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"\n"
     ]
    }
   ],
   "source": [
    "# Create the logistic Regression Model\n",
    "spamLog = glm(spam~., data=train, family=\"binomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "FALSE  TRUE \n",
       "  964  3046 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the predictions for Logistic Model\n",
    "predTrainLog = predict(spamLog, type=\"response\")\n",
    "\n",
    "to = table(predTrainLog < 0.00001)\n",
    "\n",
    "to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 3046 training set predicted probabilities from spamLog are less than 0.00001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many of the training set predicted probabilities from spamLog are more than 0.99999?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "FALSE  TRUE \n",
       " 3056   954 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp = table(predTrainLog > 0.99999)\n",
    "\n",
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 954 training set predicted probabilities from spamLog are more than 0.99999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many of the training set predicted probabilities from spamLog are between 0.00001 and 0.99999?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "FALSE  TRUE \n",
       " 4000    10 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tq = table(predTrainLog >= 0.00001 & predTrainLog <= 0.99999)\n",
    "\n",
    "tq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 10 training set predicted probabilities from spamLog are between 0.00001 and 0.99999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2 - Building Machine Learning Models\n",
    "**How many variables are labeled as significant (at the p=0.05 level) in the logistic regression summary output?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = spam ~ ., family = \"binomial\", data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-1.011   0.000   0.000   0.000   1.354  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)\n",
       "(Intercept) -3.082e+01  1.055e+04  -0.003    0.998\n",
       "X000         1.474e+01  1.058e+04   0.001    0.999\n",
       "X2000       -3.631e+01  1.556e+04  -0.002    0.998\n",
       "X2001       -3.215e+01  1.318e+04  -0.002    0.998\n",
       "X713        -2.427e+01  2.914e+04  -0.001    0.999\n",
       "X853        -1.212e+00  5.942e+04   0.000    1.000\n",
       "abl         -2.049e+00  2.088e+04   0.000    1.000\n",
       "access      -1.480e+01  1.335e+04  -0.001    0.999\n",
       "account      2.488e+01  8.165e+03   0.003    0.998\n",
       "addit        1.463e+00  2.703e+04   0.000    1.000\n",
       "address     -4.613e+00  1.113e+04   0.000    1.000\n",
       "allow        1.899e+01  6.436e+03   0.003    0.998\n",
       "alreadi     -2.407e+01  3.319e+04  -0.001    0.999\n",
       "also         2.990e+01  1.378e+04   0.002    0.998\n",
       "analysi     -2.405e+01  3.860e+04  -0.001    1.000\n",
       "anoth       -8.744e+00  2.032e+04   0.000    1.000\n",
       "applic      -2.649e+00  1.674e+04   0.000    1.000\n",
       "appreci     -2.145e+01  2.762e+04  -0.001    0.999\n",
       "approv      -1.302e+00  1.589e+04   0.000    1.000\n",
       "april       -2.620e+01  2.208e+04  -0.001    0.999\n",
       "area         2.041e+01  2.266e+04   0.001    0.999\n",
       "arrang       1.069e+01  2.135e+04   0.001    1.000\n",
       "ask         -7.746e+00  1.976e+04   0.000    1.000\n",
       "assist      -1.128e+01  2.490e+04   0.000    1.000\n",
       "associ       9.049e+00  1.909e+04   0.000    1.000\n",
       "attach      -1.037e+01  1.534e+04  -0.001    0.999\n",
       "attend      -3.451e+01  3.257e+04  -0.001    0.999\n",
       "avail        8.651e+00  1.709e+04   0.001    1.000\n",
       "back        -1.323e+01  2.272e+04  -0.001    1.000\n",
       "base        -1.354e+01  2.122e+04  -0.001    0.999\n",
       "begin        2.228e+01  2.973e+04   0.001    0.999\n",
       "believ       3.233e+01  2.136e+04   0.002    0.999\n",
       "best        -8.201e+00  1.333e+03  -0.006    0.995\n",
       "better       4.263e+01  2.360e+04   0.002    0.999\n",
       "book         4.301e+00  2.024e+04   0.000    1.000\n",
       "bring        1.607e+01  6.767e+04   0.000    1.000\n",
       "busi        -4.803e+00  1.000e+04   0.000    1.000\n",
       "buy          4.170e+01  3.892e+04   0.001    0.999\n",
       "call        -1.145e+00  1.111e+04   0.000    1.000\n",
       "can          3.762e+00  7.674e+03   0.000    1.000\n",
       "case        -3.372e+01  2.880e+04  -0.001    0.999\n",
       "chang       -2.717e+01  2.215e+04  -0.001    0.999\n",
       "check        1.425e+00  1.963e+04   0.000    1.000\n",
       "click        1.376e+01  7.077e+03   0.002    0.998\n",
       "com          1.936e+00  4.039e+03   0.000    1.000\n",
       "come        -1.166e+00  1.511e+04   0.000    1.000\n",
       "comment     -3.251e+00  3.387e+04   0.000    1.000\n",
       "communic     1.580e+01  8.958e+03   0.002    0.999\n",
       "compani      4.781e+00  9.186e+03   0.001    1.000\n",
       "complet     -1.363e+01  2.024e+04  -0.001    0.999\n",
       "confer      -7.503e-01  8.557e+03   0.000    1.000\n",
       "confirm     -1.300e+01  1.514e+04  -0.001    0.999\n",
       "contact      1.530e+00  1.262e+04   0.000    1.000\n",
       "continu      1.487e+01  1.535e+04   0.001    0.999\n",
       "contract    -1.295e+01  1.498e+04  -0.001    0.999\n",
       "copi        -4.274e+01  3.070e+04  -0.001    0.999\n",
       "corp         1.606e+01  2.708e+04   0.001    1.000\n",
       "corpor      -8.286e-01  2.818e+04   0.000    1.000\n",
       "cost        -1.938e+00  1.833e+04   0.000    1.000\n",
       "cours        1.665e+01  1.834e+04   0.001    0.999\n",
       "creat        1.338e+01  3.946e+04   0.000    1.000\n",
       "credit       2.617e+01  1.314e+04   0.002    0.998\n",
       "crenshaw     9.994e+01  6.769e+04   0.001    0.999\n",
       "current      3.629e+00  1.707e+04   0.000    1.000\n",
       "custom       1.829e+01  1.008e+04   0.002    0.999\n",
       "data        -2.609e+01  2.271e+04  -0.001    0.999\n",
       "date        -2.786e+00  1.699e+04   0.000    1.000\n",
       "day         -6.100e+00  5.866e+03  -0.001    0.999\n",
       "deal        -1.129e+01  1.448e+04  -0.001    0.999\n",
       "dear        -2.313e+00  2.306e+04   0.000    1.000\n",
       "depart      -4.068e+01  2.509e+04  -0.002    0.999\n",
       "deriv       -4.971e+01  3.587e+04  -0.001    0.999\n",
       "design      -7.923e+00  2.939e+04   0.000    1.000\n",
       "detail       1.197e+01  2.301e+04   0.001    1.000\n",
       "develop      5.976e+00  9.455e+03   0.001    0.999\n",
       "differ      -2.293e+00  1.075e+04   0.000    1.000\n",
       "direct      -2.051e+01  3.194e+04  -0.001    0.999\n",
       "director    -1.770e+01  1.793e+04  -0.001    0.999\n",
       "discuss     -1.051e+01  1.915e+04  -0.001    1.000\n",
       "doc         -2.597e+01  2.603e+04  -0.001    0.999\n",
       "don          2.129e+01  1.456e+04   0.001    0.999\n",
       "done         6.828e+00  1.882e+04   0.000    1.000\n",
       "due         -4.163e+00  3.532e+04   0.000    1.000\n",
       "ect          8.685e-01  5.342e+03   0.000    1.000\n",
       "edu         -2.122e-01  6.917e+02   0.000    1.000\n",
       "effect       1.948e+01  2.100e+04   0.001    0.999\n",
       "effort       1.606e+01  5.670e+04   0.000    1.000\n",
       "either      -2.744e+01  4.000e+04  -0.001    0.999\n",
       "email        3.833e+00  1.186e+04   0.000    1.000\n",
       "end         -1.311e+01  2.938e+04   0.000    1.000\n",
       "energi      -1.620e+01  1.646e+04  -0.001    0.999\n",
       "engin        2.664e+01  2.394e+04   0.001    0.999\n",
       "enron       -8.789e+00  5.719e+03  -0.002    0.999\n",
       "etc          9.470e-01  1.569e+04   0.000    1.000\n",
       "even        -1.654e+01  2.289e+04  -0.001    0.999\n",
       "event        1.694e+01  1.851e+04   0.001    0.999\n",
       "expect      -1.179e+01  1.914e+04  -0.001    1.000\n",
       "experi       2.460e+00  2.240e+04   0.000    1.000\n",
       "fax          3.537e+00  3.386e+04   0.000    1.000\n",
       "feel         2.596e+00  2.348e+04   0.000    1.000\n",
       "file        -2.943e+01  2.165e+04  -0.001    0.999\n",
       "final        8.075e+00  5.008e+04   0.000    1.000\n",
       "financ      -9.122e+00  7.524e+03  -0.001    0.999\n",
       "financi     -9.747e+00  1.727e+04  -0.001    1.000\n",
       "find        -2.623e+00  9.727e+03   0.000    1.000\n",
       "first       -4.666e-01  2.043e+04   0.000    1.000\n",
       "follow       1.766e+01  3.080e+03   0.006    0.995\n",
       "form         8.483e+00  1.674e+04   0.001    1.000\n",
       "forward     -3.484e+00  1.864e+04   0.000    1.000\n",
       "free         6.113e+00  8.121e+03   0.001    0.999\n",
       "friday      -1.146e+01  1.996e+04  -0.001    1.000\n",
       "full         2.125e+01  2.190e+04   0.001    0.999\n",
       "futur        4.146e+01  1.439e+04   0.003    0.998\n",
       "gas         -3.901e+00  4.160e+03  -0.001    0.999\n",
       "get          5.154e+00  9.737e+03   0.001    1.000\n",
       "gibner       2.901e+01  2.460e+04   0.001    0.999\n",
       "give        -2.518e+01  2.130e+04  -0.001    0.999\n",
       "given       -2.186e+01  5.426e+04   0.000    1.000\n",
       "good         5.399e+00  1.619e+04   0.000    1.000\n",
       "great        1.222e+01  1.090e+04   0.001    0.999\n",
       "group        5.264e-01  1.037e+04   0.000    1.000\n",
       "happi        1.939e-02  1.202e+04   0.000    1.000\n",
       "hear         2.887e+01  2.281e+04   0.001    0.999\n",
       "hello        2.166e+01  1.361e+04   0.002    0.999\n",
       "help         1.731e+01  2.791e+03   0.006    0.995\n",
       "high        -1.982e+00  2.554e+04   0.000    1.000\n",
       "home         5.973e+00  8.965e+03   0.001    0.999\n",
       "hope        -1.435e+01  2.179e+04  -0.001    0.999\n",
       "hou          6.852e+00  6.437e+03   0.001    0.999\n",
       "hour         2.478e+00  1.333e+04   0.000    1.000\n",
       "houston     -1.855e+01  7.305e+03  -0.003    0.998\n",
       "howev       -3.449e+01  3.562e+04  -0.001    0.999\n",
       "http         2.528e+01  2.107e+04   0.001    0.999\n",
       "idea        -1.845e+01  3.892e+04   0.000    1.000\n",
       "immedi       6.285e+01  3.346e+04   0.002    0.999\n",
       "import      -1.859e+00  2.236e+04   0.000    1.000\n",
       "includ      -3.454e+00  1.799e+04   0.000    1.000\n",
       "increas      6.476e+00  2.329e+04   0.000    1.000\n",
       "industri    -3.160e+01  2.373e+04  -0.001    0.999\n",
       "info        -1.255e+00  4.857e+03   0.000    1.000\n",
       "inform       2.078e+01  8.549e+03   0.002    0.998\n",
       "interest     2.698e+01  1.159e+04   0.002    0.998\n",
       "intern      -7.991e+00  3.351e+04   0.000    1.000\n",
       "internet     8.749e+00  1.100e+04   0.001    0.999\n",
       "interview   -1.640e+01  1.873e+04  -0.001    0.999\n",
       "invest       3.201e+01  2.393e+04   0.001    0.999\n",
       "invit        4.304e+00  2.215e+04   0.000    1.000\n",
       "involv       3.815e+01  3.315e+04   0.001    0.999\n",
       "issu        -3.708e+01  3.396e+04  -0.001    0.999\n",
       "john        -5.326e-01  2.856e+04   0.000    1.000\n",
       "join        -3.824e+01  2.334e+04  -0.002    0.999\n",
       "juli        -1.358e+01  3.009e+04   0.000    1.000\n",
       "just        -1.021e+01  1.114e+04  -0.001    0.999\n",
       "kaminski    -1.812e+01  6.029e+03  -0.003    0.998\n",
       "keep         1.867e+01  2.782e+04   0.001    0.999\n",
       "kevin       -3.779e+01  4.738e+04  -0.001    0.999\n",
       "know         1.277e+01  1.526e+04   0.001    0.999\n",
       "last         1.046e+00  1.372e+04   0.000    1.000\n",
       "let         -2.763e+01  1.462e+04  -0.002    0.998\n",
       "life         5.812e+01  3.864e+04   0.002    0.999\n",
       "like         5.649e+00  7.660e+03   0.001    0.999\n",
       "line         8.743e+00  1.236e+04   0.001    0.999\n",
       "link        -6.929e+00  1.345e+04  -0.001    1.000\n",
       "list        -8.692e+00  2.149e+03  -0.004    0.997\n",
       "locat        2.073e+01  1.597e+04   0.001    0.999\n",
       "london       6.745e+00  1.642e+04   0.000    1.000\n",
       "long        -1.489e+01  1.934e+04  -0.001    0.999\n",
       "look        -7.031e+00  1.563e+04   0.000    1.000\n",
       "lot         -1.964e+01  1.321e+04  -0.001    0.999\n",
       "made         2.820e+00  2.743e+04   0.000    1.000\n",
       "mail         7.584e+00  1.021e+04   0.001    0.999\n",
       "make         2.901e+01  1.528e+04   0.002    0.998\n",
       "manag        6.014e+00  1.445e+04   0.000    1.000\n",
       "mani         1.885e+01  1.442e+04   0.001    0.999\n",
       "mark        -3.350e+01  3.208e+04  -0.001    0.999\n",
       "market       7.895e+00  8.012e+03   0.001    0.999\n",
       "may         -9.434e+00  1.397e+04  -0.001    0.999\n",
       "mean         6.078e-01  2.952e+04   0.000    1.000\n",
       "meet        -1.063e+00  1.263e+04   0.000    1.000\n",
       "member       1.381e+01  2.343e+04   0.001    1.000\n",
       "mention     -2.279e+01  2.714e+04  -0.001    0.999\n",
       "messag       1.716e+01  2.562e+03   0.007    0.995\n",
       "might        1.244e+01  1.753e+04   0.001    0.999\n",
       "model       -2.292e+01  1.049e+04  -0.002    0.998\n",
       "monday      -1.034e+00  3.233e+04   0.000    1.000\n",
       "money        3.264e+01  1.321e+04   0.002    0.998\n",
       "month       -3.727e+00  1.112e+04   0.000    1.000\n",
       "morn        -2.645e+01  3.403e+04  -0.001    0.999\n",
       "move        -3.834e+01  3.011e+04  -0.001    0.999\n",
       "much         3.775e-01  1.392e+04   0.000    1.000\n",
       "name         1.672e+01  1.322e+04   0.001    0.999\n",
       "need         8.437e-01  1.221e+04   0.000    1.000\n",
       "net          1.256e+01  2.197e+04   0.001    1.000\n",
       "new          1.003e+00  1.009e+04   0.000    1.000\n",
       "next.        1.492e+01  1.724e+04   0.001    0.999\n",
       "note         1.446e+01  2.294e+04   0.001    0.999\n",
       "now          3.790e+01  1.219e+04   0.003    0.998\n",
       "number      -9.622e+00  1.591e+04  -0.001    1.000\n",
       "offer        1.174e+01  1.084e+04   0.001    0.999\n",
       "offic       -1.344e+01  2.311e+04  -0.001    1.000\n",
       "one          1.241e+01  6.652e+03   0.002    0.999\n",
       "onlin        3.589e+01  1.665e+04   0.002    0.998\n",
       "open         2.114e+01  2.961e+04   0.001    0.999\n",
       "oper        -1.696e+01  2.757e+04  -0.001    1.000\n",
       "opportun    -4.131e+00  1.918e+04   0.000    1.000\n",
       "option      -1.085e+00  9.325e+03   0.000    1.000\n",
       "order        6.533e+00  1.242e+04   0.001    1.000\n",
       "origin       3.226e+01  3.818e+04   0.001    0.999\n",
       "part         4.594e+00  3.483e+04   0.000    1.000\n",
       "particip    -1.154e+01  1.738e+04  -0.001    0.999\n",
       "peopl       -1.864e+01  1.439e+04  -0.001    0.999\n",
       "per          1.367e+01  1.273e+04   0.001    0.999\n",
       "person       1.870e+01  9.575e+03   0.002    0.998\n",
       "phone       -6.957e+00  1.172e+04  -0.001    1.000\n",
       "place        9.005e+00  3.661e+04   0.000    1.000\n",
       "plan        -1.830e+01  6.320e+03  -0.003    0.998\n",
       "pleas       -7.961e+00  9.484e+03  -0.001    0.999\n",
       "point        5.498e+00  3.403e+04   0.000    1.000\n",
       "posit       -1.543e+01  2.316e+04  -0.001    0.999\n",
       "possibl     -1.366e+01  2.492e+04  -0.001    1.000\n",
       "power       -5.643e+00  1.173e+04   0.000    1.000\n",
       "present     -6.163e+00  1.278e+04   0.000    1.000\n",
       "price        3.428e+00  7.850e+03   0.000    1.000\n",
       "problem      1.262e+01  9.763e+03   0.001    0.999\n",
       "process     -2.957e-01  1.191e+04   0.000    1.000\n",
       "product      1.016e+01  1.345e+04   0.001    0.999\n",
       "program      1.444e+00  1.183e+04   0.000    1.000\n",
       "project      2.173e+00  1.497e+04   0.000    1.000\n",
       "provid       2.422e-01  1.859e+04   0.000    1.000\n",
       "public      -5.250e+01  2.341e+04  -0.002    0.998\n",
       "put         -1.052e+01  2.681e+04   0.000    1.000\n",
       "question    -3.467e+01  1.859e+04  -0.002    0.999\n",
       "rate        -3.112e+00  1.319e+04   0.000    1.000\n",
       "read        -1.527e+01  2.145e+04  -0.001    0.999\n",
       "real         2.046e+01  2.358e+04   0.001    0.999\n",
       "realli      -2.667e+01  4.640e+04  -0.001    1.000\n",
       "receiv       5.765e-01  1.585e+04   0.000    1.000\n",
       "recent      -2.067e+00  1.780e+04   0.000    1.000\n",
       "regard      -3.668e+00  1.511e+04   0.000    1.000\n",
       "relat       -5.114e+01  1.793e+04  -0.003    0.998\n",
       "remov        2.325e+01  2.484e+04   0.001    0.999\n",
       "repli        1.538e+01  2.916e+04   0.001    1.000\n",
       "report      -1.482e+01  1.477e+04  -0.001    0.999\n",
       "request     -1.232e+01  1.167e+04  -0.001    0.999\n",
       "requir       5.004e-01  2.937e+04   0.000    1.000\n",
       "research    -2.826e+01  1.553e+04  -0.002    0.999\n",
       "resourc     -2.735e+01  3.522e+04  -0.001    0.999\n",
       "respond      2.974e+01  3.888e+04   0.001    0.999\n",
       "respons     -1.960e+01  3.667e+04  -0.001    1.000\n",
       "result      -5.002e-01  3.140e+04   0.000    1.000\n",
       "resum       -9.219e+00  2.100e+04   0.000    1.000\n",
       "return       1.745e+01  1.844e+04   0.001    0.999\n",
       "review      -4.825e+00  1.013e+04   0.000    1.000\n",
       "right        2.312e+01  1.590e+04   0.001    0.999\n",
       "risk        -4.001e+00  1.718e+04   0.000    1.000\n",
       "robert      -2.096e+01  2.907e+04  -0.001    0.999\n",
       "run         -5.162e+01  4.434e+04  -0.001    0.999\n",
       "say          7.366e+00  2.217e+04   0.000    1.000\n",
       "schedul      1.919e+00  3.580e+04   0.000    1.000\n",
       "school      -3.870e+00  2.882e+04   0.000    1.000\n",
       "secur       -1.604e+01  2.201e+03  -0.007    0.994\n",
       "see         -1.120e+01  1.293e+04  -0.001    0.999\n",
       "send        -2.427e+01  1.222e+04  -0.002    0.998\n",
       "sent        -1.488e+01  2.195e+04  -0.001    0.999\n",
       "servic      -7.164e+00  1.235e+04  -0.001    1.000\n",
       "set         -9.353e+00  2.627e+04   0.000    1.000\n",
       "sever        2.041e+01  3.093e+04   0.001    0.999\n",
       "shall        1.930e+01  3.075e+04   0.001    0.999\n",
       "shirley     -7.133e+01  6.329e+04  -0.001    0.999\n",
       "short       -8.974e+00  1.721e+04  -0.001    1.000\n",
       "sinc        -3.438e+00  3.546e+04   0.000    1.000\n",
       "sincer      -2.073e+01  3.515e+04  -0.001    1.000\n",
       "site         8.689e+00  1.496e+04   0.001    1.000\n",
       "softwar      2.575e+01  1.059e+04   0.002    0.998\n",
       "soon         2.350e+01  3.731e+04   0.001    0.999\n",
       "sorri        6.036e+00  2.299e+04   0.000    1.000\n",
       "special      1.777e+01  2.755e+04   0.001    0.999\n",
       "specif      -2.337e+01  3.083e+04  -0.001    0.999\n",
       "start        1.437e+01  1.897e+04   0.001    0.999\n",
       "state        1.221e+01  1.677e+04   0.001    0.999\n",
       "still        3.878e+00  2.622e+04   0.000    1.000\n",
       "stinson     -4.345e+01  2.697e+04  -0.002    0.999\n",
       "student     -1.815e+01  2.186e+04  -0.001    0.999\n",
       "subject      3.041e+01  1.055e+04   0.003    0.998\n",
       "success      4.344e+00  2.783e+04   0.000    1.000\n",
       "suggest     -3.842e+01  4.475e+04  -0.001    0.999\n",
       "support     -1.539e+01  1.976e+04  -0.001    0.999\n",
       "sure        -5.503e+00  2.078e+04   0.000    1.000\n",
       "system       3.778e+00  9.149e+03   0.000    1.000\n",
       "take         5.731e+00  1.716e+04   0.000    1.000\n",
       "talk        -1.011e+01  2.021e+04  -0.001    1.000\n",
       "team         7.940e+00  2.570e+04   0.000    1.000\n",
       "term         2.013e+01  2.303e+04   0.001    0.999\n",
       "thank       -3.890e+01  1.059e+04  -0.004    0.997\n",
       "thing        2.579e+01  1.341e+04   0.002    0.998\n",
       "think       -1.218e+01  2.077e+04  -0.001    1.000\n",
       "thought      1.243e+01  3.023e+04   0.000    1.000\n",
       "thursday    -1.491e+01  3.262e+04   0.000    1.000\n",
       "time        -5.921e+00  8.335e+03  -0.001    0.999\n",
       "today       -1.762e+01  1.965e+04  -0.001    0.999\n",
       "togeth      -2.355e+01  1.869e+04  -0.001    0.999\n",
       "trade       -1.755e+01  1.483e+04  -0.001    0.999\n",
       "tri          9.278e-01  1.282e+04   0.000    1.000\n",
       "tuesday     -2.808e+01  3.959e+04  -0.001    0.999\n",
       "two         -2.573e+01  1.844e+04  -0.001    0.999\n",
       "type        -1.447e+01  2.755e+04  -0.001    1.000\n",
       "understand   9.307e+00  2.342e+04   0.000    1.000\n",
       "unit        -4.020e+00  3.008e+04   0.000    1.000\n",
       "univers      1.228e+01  2.197e+04   0.001    1.000\n",
       "updat       -1.510e+01  1.448e+04  -0.001    0.999\n",
       "use         -1.385e+01  9.382e+03  -0.001    0.999\n",
       "valu         9.024e-01  1.360e+04   0.000    1.000\n",
       "version     -3.606e+01  2.939e+04  -0.001    0.999\n",
       "vinc        -3.735e+01  8.647e+03  -0.004    0.997\n",
       "visit        2.585e+01  1.170e+04   0.002    0.998\n",
       "vkamin      -6.649e+01  5.703e+04  -0.001    0.999\n",
       "want        -2.555e+00  1.106e+04   0.000    1.000\n",
       "way          1.339e+01  1.138e+04   0.001    0.999\n",
       "web          2.791e+00  1.686e+04   0.000    1.000\n",
       "websit      -2.563e+01  1.848e+04  -0.001    0.999\n",
       "wednesday   -1.526e+01  2.642e+04  -0.001    1.000\n",
       "week        -6.795e+00  1.046e+04  -0.001    0.999\n",
       "well        -2.222e+01  9.713e+03  -0.002    0.998\n",
       "will        -1.119e+01  5.980e+03  -0.002    0.999\n",
       "wish         1.173e+01  3.175e+04   0.000    1.000\n",
       "within       2.900e+01  2.163e+04   0.001    0.999\n",
       "without      1.942e+01  1.763e+04   0.001    0.999\n",
       "work        -1.099e+01  1.160e+04  -0.001    0.999\n",
       "write        4.406e+01  2.825e+04   0.002    0.999\n",
       "www         -7.867e+00  2.224e+04   0.000    1.000\n",
       "year        -1.010e+01  1.039e+04  -0.001    0.999\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 4409.49  on 4009  degrees of freedom\n",
       "Residual deviance:   13.46  on 3679  degrees of freedom\n",
       "AIC: 675.46\n",
       "\n",
       "Number of Fisher Scoring iterations: 25\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output the summary\n",
    "summary(spamLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0 variables are labeled as significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3 - Building Machine Learning Models\n",
    "**How many of the word stems \"enron\", \"hou\", \"vinc\", and \"kaminski\" appear in the CART tree?** Recall that we suspect these word stems are specific to Vincent Kaminski and might affect the generalizability of a spam filter built with his ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      },
      "text/plain": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification and Regression Trees (CART)\n",
    "spamCART = rpart(spam ~ ., data=train, method='class')\n",
    "\n",
    "# Plot the Tree\n",
    "prp(spamCART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.4 - Building Machine Learning Models\n",
    "**What is the training set accuracy of spamLog, using a threshold of 0.5 for predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "    FALSE TRUE\n",
       "  0  3052    0\n",
       "  1     4  954"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tabulate the spam in the training set and predictTrainLog (Logistic Regression)\n",
    "cmLR = table(train$spam, predTrainLog > 0.5)\n",
    "cmLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows are labeled with the actual outcome, and the columns are labeled with the predicted outcome.\n",
    "\n",
    "                      Predict 0       Predict 1\n",
    "        Actual 0    True Negative   False Positive\n",
    "        Actual 1    False Negative  True Positive\n",
    "\n",
    "        cmLR = [1][3]\n",
    "               [2][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy Logistic Regression: 0.999002'"
      ],
      "text/latex": [
       "'Accuracy Logistic Regression: 0.999002'"
      ],
      "text/markdown": [
       "'Accuracy Logistic Regression: 0.999002'"
      ],
      "text/plain": [
       "[1] \"Accuracy Logistic Regression: 0.999002\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression Accuracy\n",
    "accurLR = sum(diag(cmLR))/sum(cmLR)\n",
    "paste(\"Accuracy Logistic Regression:\", round(accurLR,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.5 - Building Machine Learning Models\n",
    "**What is the training set AUC of spamLog?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ROCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'AUC Logistic Regression: 0.999996'"
      ],
      "text/latex": [
       "'AUC Logistic Regression: 0.999996'"
      ],
      "text/markdown": [
       "'AUC Logistic Regression: 0.999996'"
      ],
      "text/plain": [
       "[1] \"AUC Logistic Regression: 0.999996\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the training set AUC\n",
    "ROCRpred = prediction(predTrainLog, train$spam)\n",
    "\n",
    "# Area Under the ROC Curve (AUC)\n",
    "AUCLR = as.numeric(performance(ROCRpred, \"auc\")@y.values)\n",
    "paste(\"AUC Logistic Regression:\", round(AUCLR,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.6 - Building Machine Learning Models\n",
    "**What is the training set accuracy of spamCART, using a threshold of 0.5 for predictions?** (Remember that if you used the type=\"class\" argument when making predictions, you automatically used a threshold of 0.5. If you did not add in the type argument to the predict function, the probabilities are in the second column of the predict output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "    FALSE TRUE\n",
       "  0  2885  167\n",
       "  1    64  894"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the training set accuracy CART\n",
    "predTrainCART = predict(spamCART)[,2]\n",
    "\n",
    "# Tabulate the spam in the training set and predictions\n",
    "cmCA = table(train$spam, predTrainCART > 0.5)\n",
    "cmCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy CART: 0.942394'"
      ],
      "text/latex": [
       "'Accuracy CART: 0.942394'"
      ],
      "text/markdown": [
       "'Accuracy CART: 0.942394'"
      ],
      "text/plain": [
       "[1] \"Accuracy CART: 0.942394\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CART Accuracy\n",
    "accurCA = sum(diag(cmCA))/sum(cmCA)\n",
    "paste(\"Accuracy CART:\", round(accurCA,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.7 - Building Machine Learning Models\n",
    "**What is the training set AUC of spamCART?** (Remember that you have to pass the prediction function predicted probabilities, so don't include the type argument when making predictions for your CART model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'AUC CART: 0.969604'"
      ],
      "text/latex": [
       "'AUC CART: 0.969604'"
      ],
      "text/markdown": [
       "'AUC CART: 0.969604'"
      ],
      "text/plain": [
       "[1] \"AUC CART: 0.969604\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the training set AUC\n",
    "ROCRpredCA = prediction(predTrainCART, train$spam)\n",
    "\n",
    "# Area Under the ROC Curve (AUC)\n",
    "AUCCA = as.numeric(performance(ROCRpredCA, \"auc\")@y.values)\n",
    "\n",
    "paste(\"AUC CART:\", round(AUCCA,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.8 - Building Machine Learning Models\n",
    "**What is the training set accuracy of spamRF, using a threshold of 0.5 for predictions?** (Remember that your answer might not match ours exactly, due to random behavior in the random forest algorithm on different operating systems.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Random Forest (RF) algorithm\n",
    "set.seed(123)\n",
    "\n",
    "spamRF = randomForest(spam~., data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "    FALSE TRUE\n",
       "  0  3015   37\n",
       "  1    42  916"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions using the RF model\n",
    "predTrainRF = predict(spamRF, type=\"prob\")[,2]\n",
    "\n",
    "# Tabulate the spam in the training and predictions\n",
    "cmRF = table(train$spam, predTrainRF > 0.5)\n",
    "cmRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy Random Forest: 0.980299'"
      ],
      "text/latex": [
       "'Accuracy Random Forest: 0.980299'"
      ],
      "text/markdown": [
       "'Accuracy Random Forest: 0.980299'"
      ],
      "text/plain": [
       "[1] \"Accuracy Random Forest: 0.980299\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forest  Accuracy\n",
    "accurRF = sum(diag(cmRF))/sum(cmRF)\n",
    "paste(\"Accuracy Random Forest:\", round(accurRF,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.9 - Building Machine Learning Models\n",
    "**What is the training set AUC of spamRF?** (Remember to pass the argument type=\"prob\" to the predict function to get predicted probabilities for a random forest model. The probabilities will be the second column of the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'AUC Random Forest: 0.997816'"
      ],
      "text/latex": [
       "'AUC Random Forest: 0.997816'"
      ],
      "text/markdown": [
       "'AUC Random Forest: 0.997816'"
      ],
      "text/plain": [
       "[1] \"AUC Random Forest: 0.997816\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the training set AUC\n",
    "ROCRpredRF = prediction(predTrainRF, train$spam)\n",
    "\n",
    "# Area Under the ROC Curve (AUC)\n",
    "AUCRF = as.numeric(performance(ROCRpredRF, \"auc\")@y.values)\n",
    "\n",
    "paste(\"AUC Random Forest:\", round(AUCRF,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.10 - Building Machine Learning Models\n",
    "**Which model had the best training set performance, in terms of accuracy and AUC?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy Logistic Regression: 0.999002'"
      ],
      "text/latex": [
       "'Accuracy Logistic Regression: 0.999002'"
      ],
      "text/markdown": [
       "'Accuracy Logistic Regression: 0.999002'"
      ],
      "text/plain": [
       "[1] \"Accuracy Logistic Regression: 0.999002\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Accuracy CART: 0.942394'"
      ],
      "text/latex": [
       "'Accuracy CART: 0.942394'"
      ],
      "text/markdown": [
       "'Accuracy CART: 0.942394'"
      ],
      "text/plain": [
       "[1] \"Accuracy CART: 0.942394\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Accuracy Random Forest: 0.980299'"
      ],
      "text/latex": [
       "'Accuracy Random Forest: 0.980299'"
      ],
      "text/markdown": [
       "'Accuracy Random Forest: 0.980299'"
      ],
      "text/plain": [
       "[1] \"Accuracy Random Forest: 0.980299\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'AUC Logistic Regression: 0.999996'"
      ],
      "text/latex": [
       "'AUC Logistic Regression: 0.999996'"
      ],
      "text/markdown": [
       "'AUC Logistic Regression: 0.999996'"
      ],
      "text/plain": [
       "[1] \"AUC Logistic Regression: 0.999996\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'AUC CART: 0.969604'"
      ],
      "text/latex": [
       "'AUC CART: 0.969604'"
      ],
      "text/markdown": [
       "'AUC CART: 0.969604'"
      ],
      "text/plain": [
       "[1] \"AUC CART: 0.969604\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'AUC Random Forest: 0.997816'"
      ],
      "text/latex": [
       "'AUC Random Forest: 0.997816'"
      ],
      "text/markdown": [
       "'AUC Random Forest: 0.997816'"
      ],
      "text/plain": [
       "[1] \"AUC Random Forest: 0.997816\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paste(\"Accuracy Logistic Regression:\", round(accurLR,digits=6))\n",
    "paste(\"Accuracy CART:\", round(accurCA,digits=6))\n",
    "paste(\"Accuracy Random Forest:\", round(accurRF,digits=6))\n",
    "\n",
    "paste(\"AUC Logistic Regression:\", round(AUCLR,digits=6))\n",
    "paste(\"AUC CART:\", round(AUCCA,digits=6))\n",
    "paste(\"AUC Random Forest:\", round(AUCRF,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "    Best Accuracy: Logistic Regression.\n",
    "    Best Area Under the ROC curve: Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1 - Evaluating on the Test Set\n",
    "Obtain predicted probabilities for the testing set for each of the models, again ensuring that probabilities instead of classes are obtained.\n",
    "\n",
    "**What is the testing set accuracy of spamLog, using a threshold of 0.5 for predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "    FALSE TRUE\n",
       "  0  1257   51\n",
       "  1    34  376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions using Logistic Regression\n",
    "predTestLog = predict(spamLog, newdata = test, type=\"response\")\n",
    "\n",
    "# Tabulate the spam in the testing set and predictions\n",
    "cmLGT = table(test$spam, predTestLog > 0.5)\n",
    "cmLGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy Testing Set Logistic Regression: 0.950524'"
      ],
      "text/latex": [
       "'Accuracy Testing Set Logistic Regression: 0.950524'"
      ],
      "text/markdown": [
       "'Accuracy Testing Set Logistic Regression: 0.950524'"
      ],
      "text/plain": [
       "[1] \"Accuracy Testing Set Logistic Regression: 0.950524\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute Testing set Logistic Regression Accuracy\n",
    "accurLGT = sum(diag(cmLGT))/sum(cmLGT)\n",
    "paste(\"Accuracy Testing Set Logistic Regression:\", round(accurLGT,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2 - Evaluating on the Test Set\n",
    "**What is the testing set AUC of spamLog?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'AUC testing set Logistic Regression: 0.997816'"
      ],
      "text/latex": [
       "'AUC testing set Logistic Regression: 0.997816'"
      ],
      "text/markdown": [
       "'AUC testing set Logistic Regression: 0.997816'"
      ],
      "text/plain": [
       "[1] \"AUC testing set Logistic Regression: 0.997816\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the testing set AUC \n",
    "ROCRpredLRT = prediction(predTestLog, test$spam)\n",
    "\n",
    "# Area Under the ROC Curve (AUC)\n",
    "AUCLRT = as.numeric(performance(ROCRpredLRT, \"auc\")@y.values)\n",
    "\n",
    "paste(\"AUC testing set Logistic Regression:\", round(AUCRF,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3 - Evaluating on the Test Set\n",
    "**What is the testing set accuracy of spamCART, using a threshold of 0.5 for predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "    FALSE TRUE\n",
       "  0  1228   80\n",
       "  1    24  386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making predictions using the CART model\n",
    "predTestCART = predict(spamCART, newdata = test)[,2]\n",
    "\n",
    "# Tabulate the spam in the testing set and predictTestCART\n",
    "cmCAT = table(test$spam, predTestCART > 0.5)\n",
    "cmCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy Testing Set CART: 0.939464'"
      ],
      "text/latex": [
       "'Accuracy Testing Set CART: 0.939464'"
      ],
      "text/markdown": [
       "'Accuracy Testing Set CART: 0.939464'"
      ],
      "text/plain": [
       "[1] \"Accuracy Testing Set CART: 0.939464\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute Testing set Classification and Regression Trees Accuracy\n",
    "accurCAT = sum(diag(cmCAT))/sum(cmCAT)\n",
    "paste(\"Accuracy Testing Set CART:\", round(accurCAT,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.4 - Evaluating on the Test Set\n",
    "**What is the testing set AUC of spamCART?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'AUC testing set CART: 0.963176'"
      ],
      "text/latex": [
       "'AUC testing set CART: 0.963176'"
      ],
      "text/markdown": [
       "'AUC testing set CART: 0.963176'"
      ],
      "text/plain": [
       "[1] \"AUC testing set CART: 0.963176\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the testing set AUC \n",
    "ROCRpredCAT = prediction(predTestCART, test$spam)\n",
    "\n",
    "# Area Under the ROC Curve (AUC)\n",
    "AUCCAT = as.numeric(performance(ROCRpredCAT, \"auc\")@y.values)\n",
    "\n",
    "paste(\"AUC testing set CART:\", round(AUCCAT,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4.5 - Evaluating on the Test Set\n",
    "**What is the testing set accuracy of spamRF, using a threshold of 0.5 for predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "    FALSE TRUE\n",
       "  0  1291   17\n",
       "  1    23  387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions using random forest\n",
    "predTestRF = predict(spamRF, newdata = test, type=\"prob\")[,2]\n",
    "\n",
    "# Tabulate the spam in the testing set and predictTestRF\n",
    "cmRFT = table(test$spam, predTestRF > 0.5)\n",
    "cmRFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy Testing Set Random Forest: 0.976717'"
      ],
      "text/latex": [
       "'Accuracy Testing Set Random Forest: 0.976717'"
      ],
      "text/markdown": [
       "'Accuracy Testing Set Random Forest: 0.976717'"
      ],
      "text/plain": [
       "[1] \"Accuracy Testing Set Random Forest: 0.976717\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute Testing set Random Forest Accuracy\n",
    "accurRFT = sum(diag(cmRFT))/sum(cmRFT)\n",
    "paste(\"Accuracy Testing Set Random Forest:\", round(accurRFT,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.6 - Evaluating on the Test Set\n",
    "**What is the testing set AUC of spamRF?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'AUC testing set Random Forest: 0.99759'"
      ],
      "text/latex": [
       "'AUC testing set Random Forest: 0.99759'"
      ],
      "text/markdown": [
       "'AUC testing set Random Forest: 0.99759'"
      ],
      "text/plain": [
       "[1] \"AUC testing set Random Forest: 0.99759\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the testing set AUC \n",
    "ROCRpredRFT = prediction(predTestRF, test$spam)\n",
    "\n",
    "# Area Under the ROC Curve (AUC)\n",
    "AUCRFT = as.numeric(performance(ROCRpredRFT, \"auc\")@y.values)\n",
    "\n",
    "paste(\"AUC testing set Random Forest:\", round(AUCRFT,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.7 - Evaluating on the Test Set\n",
    "**Which model had the best testing set performance, in terms of accuracy and AUC?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Accuracy testing set Logistic Regression: 0.950524'"
      ],
      "text/latex": [
       "'Accuracy testing set Logistic Regression: 0.950524'"
      ],
      "text/markdown": [
       "'Accuracy testing set Logistic Regression: 0.950524'"
      ],
      "text/plain": [
       "[1] \"Accuracy testing set Logistic Regression: 0.950524\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Accuracy testing set CART: 0.939464'"
      ],
      "text/latex": [
       "'Accuracy testing set CART: 0.939464'"
      ],
      "text/markdown": [
       "'Accuracy testing set CART: 0.939464'"
      ],
      "text/plain": [
       "[1] \"Accuracy testing set CART: 0.939464\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Accuracy testing set Random Forest: 0.976717'"
      ],
      "text/latex": [
       "'Accuracy testing set Random Forest: 0.976717'"
      ],
      "text/markdown": [
       "'Accuracy testing set Random Forest: 0.976717'"
      ],
      "text/plain": [
       "[1] \"Accuracy testing set Random Forest: 0.976717\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'AUC testing set Logistic Regression: 0.962752'"
      ],
      "text/latex": [
       "'AUC testing set Logistic Regression: 0.962752'"
      ],
      "text/markdown": [
       "'AUC testing set Logistic Regression: 0.962752'"
      ],
      "text/plain": [
       "[1] \"AUC testing set Logistic Regression: 0.962752\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'AUC testing set CART: 0.963176'"
      ],
      "text/latex": [
       "'AUC testing set CART: 0.963176'"
      ],
      "text/markdown": [
       "'AUC testing set CART: 0.963176'"
      ],
      "text/plain": [
       "[1] \"AUC testing set CART: 0.963176\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'AUC testing set Random Forest: 0.99759'"
      ],
      "text/latex": [
       "'AUC testing set Random Forest: 0.99759'"
      ],
      "text/markdown": [
       "'AUC testing set Random Forest: 0.99759'"
      ],
      "text/plain": [
       "[1] \"AUC testing set Random Forest: 0.99759\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paste(\"Accuracy testing set Logistic Regression:\", round(accurLGT,digits=6))\n",
    "paste(\"Accuracy testing set CART:\", round(accurCAT,digits=6))\n",
    "paste(\"Accuracy testing set Random Forest:\", round(accurRFT,digits=6))\n",
    "\n",
    "paste(\"AUC testing set Logistic Regression:\", round(AUCLRT,digits=6))\n",
    "paste(\"AUC testing set CART:\", round(AUCCAT,digits=6))\n",
    "paste(\"AUC testing set Random Forest:\", round(AUCRFT,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "    Best Accuracy: Random Forest.\n",
    "    Best Area Under the ROC curve: Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4.8 - Evaluating on the Test Set\n",
    "**Which model demonstrated the greatest degree of overfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Random Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
