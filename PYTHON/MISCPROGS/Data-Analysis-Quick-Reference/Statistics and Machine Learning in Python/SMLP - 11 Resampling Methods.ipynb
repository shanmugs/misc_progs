{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e8aa49-4745-40ed-80fb-c52cb7510fa9",
   "metadata": {},
   "source": [
    "# Chapter 5.6 - Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ebff2c-7587-4745-95f9-619d08532096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a54416-d40f-4920-8e67-c4d9e2578dfc",
   "metadata": {},
   "source": [
    "## CV for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef305527-b291-40ca-b3b5-cf446c5d2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r^2: 0.99\n",
      " Test r^2: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a regression dataset with 100 samples and 100 features, 10 of which are informative\n",
    "X, y = datasets.make_regression(n_samples=100, n_features=100, n_informative=10, random_state=42)\n",
    "\n",
    "# Initialize Ridge regression model with alpha=10 (regularization strength)\n",
    "estimator = lm.Ridge(alpha=10)\n",
    "\n",
    "# Initialize KFold cross-validation with 5 splits and shuffle the data\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Set shuffle=True for random splits\n",
    "\n",
    "# Initialize lists to store R-squared scores for training and testing sets\n",
    "r2_train, r2_test = list(), list()\n",
    "\n",
    "# Perform KFold cross-validation\n",
    "for train, test in cv.split(X):\n",
    "    estimator.fit(X[train, :], y[train])                                         # Train the model on the training split\n",
    "    r2_train.append(metrics.r2_score(y[train], estimator.predict(X[train, :])))  # Calculate R2 on training data\n",
    "    r2_test.append(metrics.r2_score(y[test], estimator.predict(X[test, :])))     # Calculate R2 on test data\n",
    "\n",
    "# Print the mean R-squared scores for training and testing sets\n",
    "print(\"Train r^2: %.2f\" % np.mean(r2_train))  # Print average training R2 score\n",
    "print(\" Test r^2: %.2f\" % np.mean(r2_test))    # Print average testing R2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf9ef1-f523-4979-a317-417784c4d0f4",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "`Ridge regression`: This is a linear regression model with L2 regularization. The alpha=10 controls the regularization strength.\n",
    "\n",
    "`KFold cross-validation`: It splits the dataset into n_splits=5 parts and ensures that each fold is used for testing exactly once while the remaining 4 parts are used for training.\n",
    "\n",
    "    - shuffle=True: The data is shuffled before being split, ensuring random splitting.\n",
    "    - random_state=42: A fixed random seed ensures reproducibility of the shuffling.\n",
    "    \n",
    "`R-squared (r2) score`: This score measures how well the regression model fits the data. It ranges from negative values to 1, where 1 indicates perfect prediction.\n",
    "\n",
    "`Mean R-squared`: After training and testing on each split, the mean R2 score is computed and printed to give an overall evaluation of model performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba2fcc-833e-4743-8e6f-6c8b2485450d",
   "metadata": {},
   "source": [
    "    Scikit-learn provides user-friendly function to perform CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6bb763-7c31-4957-a82e-2de111e40ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Test r^2: 0.73\n",
      "Test r^2 with shuffle: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_regression(n_samples=100, n_features=100, n_informative=10, random_state=42)\n",
    "\n",
    "# Initialize the Ridge regression model\n",
    "estimator = lm.Ridge(alpha=10)\n",
    "\n",
    "# Perform cross-validation with default KFold (no shuffle)\n",
    "scores = cross_val_score(estimator=estimator, X=X, y=y, cv=5)\n",
    "print(\"             Test r^2: %.2f\" % scores.mean())  # Display the mean R-squared score across the test sets\n",
    "\n",
    "# Initialize KFold cross-validation with 5 splits, shuffle=True, and a fixed random state\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation using the custom KFold object and compute R-squared scores\n",
    "scores = cross_val_score(estimator=estimator, X=X, y=y, cv=cv)\n",
    "print(\"Test r^2 with shuffle: %.2f\" % scores.mean())  # Display the mean R-squared score across the test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564fa61-8162-4500-a1db-4fd7844174bc",
   "metadata": {},
   "source": [
    "      No shuffle, the model explains 73% of the variance in the data.\n",
    "    With shuffle, the model explains 67% of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12de4d-40f6-4598-82c7-02a26099a0fe",
   "metadata": {},
   "source": [
    "## CV for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f987f-9a9d-4eb8-bdc4-703d1aa7ecd3",
   "metadata": {},
   "source": [
    "With classification problems it is essential to sample folds where each set contains approximately the same percentage of samples of each target class as the complete set. This is called stratification. In this case, we will use `StratifiedKFold` with is a variation of k-fold which returns stratified folds.\n",
    "\n",
    "Usually the error function ùêø() are, at least, the sensitivity and the specificity. However other function could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ccb39e-2762-4726-8f4c-b49fde874089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Macro measures ===\n",
      "Train SPC:1.00; SEN:1.00\n",
      " Test SPC:0.76; SEN:0.82\n",
      " Test ACC:0.79, ballanced ACC:0.79 Folds: [0.9, 0.7, 0.9, 0.7, 0.75]\n",
      " Test ACC:0.79 Folds: [0.9, 0.7, 0.9, 0.7, 0.75]\n",
      "\n",
      "=== Micro measures ===\n",
      "Test SPC:0.76; SEN:0.82\n",
      "Test ACC:0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=100,     # Generate a classification dataset with 100 samples, 100 features\n",
    "                                    n_informative=10, random_state=42) # 10 informative features, Random seed set for reproducibility\n",
    "\n",
    "estimator = lm.LogisticRegression(C=1, solver='lbfgs')                 # Initialize LRRegression with C=1 (regularization) and solver 'lbfgs'\n",
    "cv = StratifiedKFold(n_splits=5)                                       # Use StratifiedKFold cv with 5 splits (preserving class distribution)\n",
    "\n",
    "# Lists to store scores by folds (for macro measure only)\n",
    "recalls_train, recalls_test, acc_test = list(), list(), list()         # Initialize empty lists to store recall (train/test) and accuracy (test) scores\n",
    "\n",
    "# Or vector of test predictions (for both macro and micro measures, not for training samples)\n",
    "y_test_pred = np.zeros(len(y))                                         # Create a 0 vector to store test predictions across all folds for micro measures\n",
    "\n",
    "for train, test in cv.split(X, y):                                     # Loop through each fold (train/test indices) in StratifiedKFold\n",
    "    estimator.fit(X[train, :], y[train])                               # Train the model on the current training fold\n",
    "    recalls_train.append(metrics.recall_score(y[train], estimator.predict(X[train, :]), average=None)) # Compute recall for training fold and store it\n",
    "    recalls_test.append(metrics.recall_score(y[test], estimator.predict(X[test, :]), average=None))    # Compute recall for test fold and store it\n",
    "    acc_test.append(metrics.accuracy_score(y[test], estimator.predict(X[test, :])))                    # Compute accuracy for the test fold and store it\n",
    "\n",
    "    # Store test predictions (for micro measures)\n",
    "    y_test_pred[test] = estimator.predict(X[test, :])                  # Save the test predictions for later evaluation (micro measures)\n",
    "    \n",
    "print(\"=== Macro measures ===\")\n",
    "\n",
    "# Use lists of scores\n",
    "recalls_train = np.array(recalls_train)                                # Convert training recall scores to a numpy array for easier calculation\n",
    "recalls_test = np.array(recalls_test)                                  # Convert test recall scores to a numpy array\n",
    "print(\"Train SPC:%.2f; SEN:%.2f\" % tuple(recalls_train.mean(axis=0)))  # Print mean specificity (SPC) and sensitivity (SEN) for the training folds\n",
    "print(\" Test SPC:%.2f; SEN:%.2f\" % tuple(recalls_test.mean(axis=0)), ) # Print mean SPC and SEN for the test folds\n",
    "print(\" Test ACC:%.2f, ballanced ACC:%.2f\" %                           # Print test accuracy and balanced accuracy (mean of recalls)\n",
    "(np.mean(acc_test), recalls_test.mean(axis=1).mean()), \"Folds:\", acc_test) # Print per-fold test accuracy\n",
    "\n",
    "# Or use vector to test predictions\n",
    "acc_test = [metrics.accuracy_score(y[test], y_test_pred[test]) for train, test in cv.split(X, y)] # Recompute accuracy using the stored test predictions\n",
    "print(\" Test ACC:%.2f\" % np.mean(acc_test), \"Folds:\", acc_test)         # Print average test accuracy and accuracy for each fold\n",
    "print()\n",
    "\n",
    "print(\"=== Micro measures ===\")\n",
    "\n",
    "print(\"Test SPC:%.2f; SEN:%.2f\" % tuple(metrics.recall_score(y, y_test_pred, average=None)))# micro-average specificity and sensitivity across all folds\n",
    "print(\"Test ACC:%.2f\" % metrics.accuracy_score(y, y_test_pred))                             # micro-average accuracy across all test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e698004-d145-47d7-9477-6d986b60a582",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "Cross-Validation (StratifiedKFold):\n",
    "\n",
    "* Splits the data into 5 folds, ensuring that each fold maintains the same class distribution as the overall dataset (stratification).\n",
    "* The model is trained and evaluated on different folds to assess its performance on unseen data.\n",
    "\n",
    "Recall (Sensitivity):\n",
    "\n",
    "* Measures the proportion of actual positives (class 1) correctly identified. It is calculated separately for training and test sets using recall_score.\n",
    "* Macro-average recalls are calculated per fold and then averaged.\n",
    "\n",
    "Specificity (SPC):\n",
    "\n",
    "* Measures the proportion of actual negatives (class 0) correctly identified. In this code, it's also calculated via recall_score, but for class 0.\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "* The proportion of correct predictions (both classes) across the test set for each fold. Calculated using accuracy_score.\n",
    "\n",
    "Macro and Micro Measures:\n",
    "\n",
    "* Macro: Averaging the metrics (recall, accuracy) for each fold. Gives equal weight to each class.\n",
    "* Micro: Evaluating metrics by combining predictions across all test samples, rather than averaging per fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c857cd3-44b5-4a48-9486-57c71b1e91a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC:0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "# Perform cross-validation on the estimator (Logistic Regression in this case)\n",
    "scores = cross_val_score(estimator=estimator,   # Use the estimator (model)\n",
    "                         X=X,                   # Feature matrix\n",
    "                         y=y,                   # Target vector\n",
    "                         cv=5)                  # Use 5-fold cross-validation\n",
    "\n",
    "scores.mean()                                   # Calculate the mean of the cross-validation scores\n",
    "\n",
    "# Define a custom balanced accuracy scorer\n",
    "def balanced_acc(estimator, X, y, **kwargs):  \n",
    "    '''\n",
    "    Balanced accuracy scorer: Computes the mean recall (balanced accuracy)\n",
    "    '''\n",
    "    return metrics.recall_score(y, estimator.predict(X), average=None).mean()  # Compute mean recall across classes\n",
    "\n",
    "# Perform cross-validation using the custom balanced accuracy scorer\n",
    "scores = cross_val_score(estimator=estimator,  # Estimator (model)\n",
    "                         X=X,                  # Feature matrix\n",
    "                         y=y,                  # Target vector\n",
    "                         cv=5,                 # 5-fold cross-validation\n",
    "                         scoring=balanced_acc) # Use custom balanced accuracy scorer\n",
    "\n",
    "# Print the average balanced accuracy across folds\n",
    "print(\"Test ACC:%.2f\" % scores.mean())         # Print the mean score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dea89-6188-4fad-a2ee-003f5c987b81",
   "metadata": {},
   "source": [
    "Note that with Scikit-learn user-friendly function we average the scores‚Äô average obtained on\n",
    "individual folds which may provide slightly different results that the overall average presented\n",
    "earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c9e41-1cb0-41e5-ae14-ada4078fea60",
   "metadata": {},
   "source": [
    "## Parallel Computation with Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f675b0-e558-449b-bce1-a20ade79b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 [0.5 0.5 1.  1.  1. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "# Import necessary modules\n",
    "X, y = datasets.make_classification(n_samples=20,     # Generate a classification dataset with 20 samples\n",
    "                                    n_features=5,     # 5 features for each sample\n",
    "                                    n_informative=2,  # 2 informative features (used for class separation)\n",
    "                                    random_state=42)  # Set random state for reproducibility\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)  # StratifiedKFold ensures each fold has a proportional number of samples from each class\n",
    "\n",
    "# Initialize the Logistic Regression estimator\n",
    "estimator = lm.LogisticRegression(C=1,             # C is the inverse of regularization strength (higher C means less regularization)\n",
    "                                  solver='lbfgs')  # 'lbfgs' is the optimization algorithm used to minimize the cost function\n",
    "\n",
    "# Perform cross-validation with the Logistic Regression model\n",
    "cv_results = cross_validate(estimator,     # The estimator (Logistic Regression) to use\n",
    "                            X=X,           # Feature matrix\n",
    "                            y=y,           # Target labels\n",
    "                            cv=cv,         # Cross-validation strategy (5-fold stratified)\n",
    "                            n_jobs=5)      # Run the cross-validation in parallel using 5 CPU cores\n",
    "\n",
    "# Print the mean of the test scores and the individual test scores for each fold\n",
    "print(np.mean(cv_results['test_score']),   # Print the average test score across all folds\n",
    "      cv_results['test_score'])            # Print the test scores from each fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce736c-8b27-427f-8a4f-c0df9b783973",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "These values represent the accuracy scores of the Logistic Regression model on the test set for each of the 5 folds during cross-validation:\n",
    "\n",
    "* Fold 1: Accuracy = 0.5\n",
    "* Fold 2: Accuracy = 0.5\n",
    "* Fold 3: Accuracy = 1.0\n",
    "* Fold 4: Accuracy = 1.0\n",
    "* Fold 5: Accuracy = 1.0\n",
    "\n",
    "Accuracy is calculated as the ratio of correct predictions to the total number of predictions for each fold's test set.\n",
    "\n",
    "    In the first two folds, the accuracy is lower (0.5), meaning that the model's predictions were only 50% correct.\n",
    "    In the last three folds, the model achieved perfect accuracy (1.0), predicting all test samples correctly.\n",
    "\n",
    "Mean Test Score (0.8) = (0.5 + 0.5 + 1.0 + 1.0 + 1.0) / 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f951217-bbee-4b2c-bea0-9690fe8c92b4",
   "metadata": {},
   "source": [
    "If we want have full control of the operations performed within each fold (retrieve the models parameters, etc.). We would like to parallelize the folowing sequetial code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2524c9c7-a962-42d1-8f24-26bb0fac6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean [accuracy for each fold]\n",
      "0.8 [0.5, 0.5, 1.0, 1.0, 1.0]\n",
      "\n",
      "Coefficient vectors from each fold\n",
      "[[[-0.87692648  0.62591974  1.18706081 -0.30704161 -0.38030241]]\n",
      "\n",
      " [[-0.74649131  0.62146103  1.10152459  0.19812029 -0.40119197]]\n",
      "\n",
      " [[-0.96021714  0.51137735  1.12112968  0.08043038 -0.26438637]]\n",
      "\n",
      " [[-0.85768977  0.52013118  1.06648343 -0.10983459 -0.29151814]]\n",
      "\n",
      " [[-0.89946124  0.51498228  1.08712223 -0.24754495 -0.27908142]]]\n",
      "\n",
      "Means of the coefficients\n",
      "[[-0.86815719  0.55877432  1.11266415 -0.07717409 -0.32329606]]\n",
      "\n",
      "Std Err of the coefficients\n",
      "[[0.03126917 0.02374501 0.01845607 0.08568738 0.02510189]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the logistic regression model with L2 regularization (default)\n",
    "estimator = lm.LogisticRegression(C=1, solver='lbfgs')\n",
    "\n",
    "# Initialize an array to store the test predictions in their original order\n",
    "y_test_pred_seq = np.zeros(len(y))  # Will hold predictions from each fold in the original order of the dataset\n",
    "\n",
    "# Initialize an empty list to store the coefficient estimates from each fold\n",
    "coefs_seq = list()\n",
    "\n",
    "# Perform cross-validation by splitting the data into training and test sets using the StratifiedKFold object `cv`\n",
    "for train, test in cv.split(X, y):  # For each train-test split (5 splits in total):\n",
    "    \n",
    "    # Extract the training and test data based on the current fold\n",
    "    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]  # Split data into training and testing sets\n",
    "    \n",
    "    # Fit the logistic regression model on the current training set\n",
    "    estimator.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the current test set and store them in `y_test_pred_seq` in the original order\n",
    "    y_test_pred_seq[test] = estimator.predict(X_test)\n",
    "    \n",
    "    # Append the coefficients (model weights) learned during this fold to the `coefs_seq` list\n",
    "    coefs_seq.append(estimator.coef_)\n",
    "\n",
    "# Calculate the accuracy scores for each fold by comparing the test set predictions to the true labels\n",
    "test_accs = [metrics.accuracy_score(y[test], y_test_pred_seq[test]) for train, test in cv.split(X, y)]\n",
    "\n",
    "# Print the mean accuracy and individual accuracy scores for each fold\n",
    "print('Mean [accuracy for each fold]')\n",
    "print(np.mean(test_accs), test_accs)  # Mean accuracy over all folds and individual fold accuracies\n",
    "print()\n",
    "\n",
    "# Convert the list of coefficients into a NumPy array for easier manipulation\n",
    "coefs_cv = np.array(coefs_seq)\n",
    "\n",
    "# Print all coefficient vectors from each fold\n",
    "print('Coefficient vectors from each fold')\n",
    "print(coefs_cv)\n",
    "\n",
    "# Print the mean of the coefficients (average model weights across folds)\n",
    "print('\\nMeans of the coefficients')\n",
    "print(coefs_cv.mean(axis=0))  # Calculate the mean of the coefficients across all folds\n",
    "\n",
    "# Print the standard error of the coefficients (how much the coefficients vary across folds)\n",
    "print(\"\\nStd Err of the coefficients\")\n",
    "print(coefs_cv.std(axis=0) / np.sqrt(coefs_cv.shape[0]))  # Standard error of the coefficients across folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ed2a3-a1dc-4761-8381-14b41fbc549d",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "These are the learned model coefficients (weights) for each feature from each fold of the cross-validation. Since this is logistic regression, these coefficients represent the influence of each feature on the model's decision. For example:\n",
    "\n",
    "    The first row ([-0.87692648, 0.62591974, 1.18706081, -0.30704161, -0.38030241]) corresponds to the learned coefficients from the first fold of cross-validation. \n",
    "    \n",
    "    Each subsequent row corresponds to the coefficients learned from the remaining folds.\n",
    "\n",
    "This represents the average of the coefficients across all the folds of cross-validation. The values show how much each feature contributes to the model's predictions on average across all folds. For example:\n",
    "\n",
    "    The first feature has an average coefficient of -0.86815719, meaning it generally has a negative influence on the prediction.\n",
    "    \n",
    "    The third feature has an average coefficient of 1.11266415, meaning it has a strong positive influence on the prediction.\n",
    "\n",
    "The standard error measures the variability in the coefficients across the different cross-validation folds. The smaller the standard error, the more consistent the coefficient estimates across the folds. For example:\n",
    "\n",
    "    The standard error for the third feature is 0.01845607, indicating that the coefficient for this feature is very stable across folds.\n",
    "    \n",
    "    The standard error for the fourth feature is 0.08568738, indicating that the coefficient for this feature varies more across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af4939ef-48d2-49a7-82e5-213a0a719b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 [0.5, 0.5, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import joblib  \n",
    "from joblib import Parallel, delayed \n",
    "from sklearn.base import is_classifier, clone\n",
    "\n",
    "def _split_fit_predict(estimator, X, y, train, test):  \n",
    "    \"\"\"Fit the estimator on training data and predict on test data.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]  # Split data\n",
    "    estimator.fit(X_train, y_train)                                                # Fit the model on training data\n",
    "    return [estimator.predict(X_test), estimator.coef_]                            # Return test predictions and coefficients\n",
    "\n",
    "# Initialize Logistic Regression estimator\n",
    "estimator = lm.LogisticRegression(C=1, solver='lbfgs')  # Regularized logistic regression\n",
    "\n",
    "# Set up parallel execution (using 5 jobs)\n",
    "parallel = Parallel(n_jobs=5)                           # Execute jobs in parallel with 5 processes\n",
    "\n",
    "# Perform cross-validation in parallel\n",
    "cv_ret = parallel(\n",
    "    delayed(_split_fit_predict)(clone(estimator), X, y, train, test)  # Fit and predict in parallel\n",
    "    for train, test in cv.split(X, y)                                 # Iterate through cross-validation splits\n",
    ")\n",
    "\n",
    "# Unpack results into predictions and coefficients\n",
    "y_test_pred_cv, coefs_cv = zip(*cv_ret)                 # Collect predictions and coefficients for each fold\n",
    "\n",
    "# Retrieve predictions in the original order\n",
    "y_test_pred = np.zeros(len(y))  # Initialize an array for storing predictions\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X, y)):      # Iterate through the cross-validation splits\n",
    "    y_test_pred[test] = y_test_pred_cv[i]               # Assign predictions back in original order\n",
    "\n",
    "# Calculate accuracy for each test fold\n",
    "test_accs = [metrics.accuracy_score(y[test], y_test_pred[test]) for train, test in cv.split(X, y)]\n",
    "\n",
    "# Print mean test accuracy and accuracies for each fold\n",
    "print(np.mean(test_accs), test_accs)                    # Display average accuracy and individual fold results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e81c7b-60c8-4060-b43c-e815135c0bbf",
   "metadata": {},
   "source": [
    "#### Test same predictions and same coeficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd979516-f360-45bf-8152-c4500808c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictions match between parallel and sequential executions.\n",
      "All coefficients match between parallel and sequential executions.\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions from parallel execution vs sequential execution\n",
    "if np.all(y_test_pred == y_test_pred_seq):\n",
    "    print(\"All predictions match between parallel and sequential executions.\")\n",
    "else:\n",
    "    print(\"There are differences in predictions between parallel and sequential executions.\")\n",
    "\n",
    "# Compare coefficients from parallel execution vs sequential execution\n",
    "if np.allclose(np.array(coefs_cv).squeeze(), np.array(coefs_seq).squeeze()):\n",
    "    print(\"All coefficients match between parallel and sequential executions.\")\n",
    "else:\n",
    "    print(\"There are differences in coefficients between parallel and sequential executions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94713381-3ce2-4a87-a73a-ddfb9272d705",
   "metadata": {},
   "source": [
    "## CV for Model Selection: Setting the Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdc34be8-27fa-4971-967d-3b5e72d6847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 2.6358469446381614\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Dataset generation: Create a synthetic regression dataset\n",
    "noise_sd = 10                                         # Standard deviation of the noise added to the dataset\n",
    "X, y, coef = datasets.make_regression(n_samples=50,   # Number of samples\n",
    "                                      n_features=100, # Number of features (input variables)\n",
    "                                      noise=noise_sd, # Add Gaussian noise to the target variable y\n",
    "                                      n_informative=2,# Only 2 features are actually informative\n",
    "                                      random_state=42,# Random seed for reproducibility\n",
    "                                      coef=True)      # Return the coefficients used to generate the data\n",
    "\n",
    "# Compute and display the Signal-to-Noise Ratio (SNR)\n",
    "# SNR is the ratio between the standard deviation of the signal (X.dot(coef)) and the noise (noise_sd)\n",
    "print(\"SNR:\", np.std(np.dot(X, coef)) / noise_sd)     # A lower SNR means the data is noisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf8b6673-fe7a-4ea6-8429-3175a0558a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2:0.96\n",
      "{'alpha': 1.0, 'l1_ratio': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Define a parameter grid for hyperparameter tuning (alpha and l1_ratio)\n",
    "# 'alpha' controls the strength of regularization, and 'l1_ratio' balances between L1 (lasso) and L2 (ridge) regularization\n",
    "param_grid = {'alpha': 10. ** np.arange(-3, 3),      # alpha values ranging from 0.001 to 100 (log scale)\n",
    "              'l1_ratio': [.1, .5, .9]}              # l1_ratio values: 0.1 (more ridge), 0.5 (balance), 0.9 (more lasso)\n",
    "\n",
    "# Wrap the ElasticNet model using GridSearchCV to search for the best combination of alpha and l1_ratio\n",
    "# ElasticNet is a linear regression model that combines L1 and L2 regularization (lasso + ridge)\n",
    "model = GridSearchCV(lm.ElasticNet(max_iter=10000),  # ElasticNet with maximum 10,000 iterations for convergence\n",
    "                     param_grid,                     # Grid of hyperparameters to search\n",
    "                     cv=5)                           # 5-fold cross-validation to evaluate performance\n",
    "\n",
    "model.fit(X, y)  # Fit the model to the training data (X, y)\n",
    "\n",
    "# Calculate and display the R-squared score on the training data\n",
    "# R-squared represents the proportion of variance explained by the model (closer to 1 is better)\n",
    "print(\"Train r2:%.2f\" % metrics.r2_score(y, model.predict(X)))  # R-squared score for model performance on training data\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "# These are the optimal values of alpha and l1_ratio based on cross-validation\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2270cc91-f970-4a16-b381-2072ce52085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r^2:0.98\n",
      " Test r^2:0.75\n",
      "\n",
      "Selected alphas: [{'alpha': 1.0, 'l1_ratio': 0.9}, {'alpha': 0.001, 'l1_ratio': 0.9}, {'alpha': 1.0, 'l1_ratio': 0.9}, {'alpha': 1.0, 'l1_ratio': 0.9}, {'alpha': 0.1, 'l1_ratio': 0.9}]\n"
     ]
    }
   ],
   "source": [
    "# Define K-fold cross-validation with 5 splits, shuffling enabled, and a fixed random seed for reproducibility\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Enable shuffling to use random_state\n",
    "\n",
    "# Initialize lists to store R-squared scores for training and testing sets, and the selected alpha values\n",
    "r2_train, r2_test = list(), list()  # Store R-squared scores for training and test sets\n",
    "alphas = list()                     # Store the best alpha values selected in each fold\n",
    "\n",
    "# Iterate through each train-test split of the data\n",
    "for train, test in cv.split(X, y):  # Split the data into training and test sets in each fold\n",
    "    # Split the data into training and testing sets based on the current fold\n",
    "    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]\n",
    "    \n",
    "    # Fit the ElasticNet model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate R-squared scores for both the training and testing sets\n",
    "    r2_test.append(metrics.r2_score(y_test, model.predict(X_test)))     # R-squared score for the test set\n",
    "    r2_train.append(metrics.r2_score(y_train, model.predict(X_train)))  # R-squared score for the training set\n",
    "    \n",
    "    # Append the best alpha (regularization parameter) selected by the GridSearchCV model in each fold\n",
    "    alphas.append(model.best_params_)\n",
    "\n",
    "# Print the mean R-squared score for the training data across all folds\n",
    "print(\"Train r^2:%.2f\" % np.mean(r2_train))  # Calculate and display the average R-squared score for the training set\n",
    "\n",
    "# Print the mean R-squared score for the testing data across all folds\n",
    "print(\" Test r^2:%.2f\" % np.mean(r2_test))   # Calculate and display the average R-squared score for the test set\n",
    "\n",
    "# Print the selected alpha values (best regularization parameters) from each fold\n",
    "print(\"\\nSelected alphas:\", alphas)          # Display the best alpha values selected by GridSearchCV in each fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531690f-181a-421c-93da-d6ba68ca9cca",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "`Train R¬≤= 0.98`: the model explains 98% of the variance in the target variable (y) for the training set.\n",
    "\n",
    "`Test R¬≤= 0.75`: the model explains 75% of the variance in the target variable for the test set.\n",
    "\n",
    "\n",
    "Selected Alphas:\n",
    "\n",
    "* Alpha: Controls the strength of regularization. Higher values of alpha apply stronger regularization, which helps in preventing overfitting by penalizing large coefficients.\n",
    "\n",
    "* l1_ratio: Controls the mix between L1 and L2 regularization. A value of 0.9 means the model is mostly using L1 regularization (which tends to shrink some coefficients to exactly zero, promoting sparsity), but also includes some L2 regularization (which tends to shrink coefficients without forcing them to zero).\n",
    "\n",
    "The model performed cross-validation across different folds, and for each fold, a different set of best parameters (alpha, l1_ratio) was chosen. The selected alphas across the 5 folds are:\n",
    "\n",
    "    {'alpha': 1.0, 'l1_ratio': 0.9} (chosen in 3 folds)\n",
    "    {'alpha': 0.001, 'l1_ratio': 0.9} (chosen in 1 fold)\n",
    "    {'alpha': 0.1, 'l1_ratio': 0.9} (chosen in 1 fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd0f0532-65f4-4021-9561-230be71f7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r^2: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation to evaluate the model's performance\n",
    "scores = cross_val_score(estimator=model,  # The model to be evaluated, in this case, 'model' (e.g., ElasticNet, Ridge)\n",
    "                         X=X,              # The feature matrix (input data) used for training and evaluation\n",
    "                         y=y,              # The target vector (output data) corresponding to the feature matrix\n",
    "                         cv=cv)            # The cross-validation splitting strategy (e.g., KFold, StratifiedKFold)\n",
    "\n",
    "# Print the average R-squared score across all the cross-validation folds\n",
    "print(\"Test r^2: %.2f\" % scores.mean())     # Format the mean R¬≤ score and display it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be109a4-746a-4976-833c-e938f4e8dee8",
   "metadata": {},
   "source": [
    "On average, 75% of the variance in the target variable y can be explained by the model when evaluated on the test data using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82d02c6e-4b1d-48e8-b213-2e8e73fe8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ridge (L2 penalty) ===\n",
      "Test r^2: 0.16\n",
      "\n",
      "=== Lasso (L1 penalty) ===\n",
      "Test r^2: 0.74\n",
      "\n",
      "=== ElasticNet (L1 penalty) ===\n",
      "Test r^2: 0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets               \n",
    "import sklearn.linear_model as lm         \n",
    "import sklearn.metrics as metrics         \n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "# Dataset generation\n",
    "X, y, coef = datasets.make_regression(n_samples=50,        # Generate a regression dataset with 50 samples\n",
    "                                      n_features=100,      # with 100 features (predictors)\n",
    "                                      noise=10,            # Add noise to the data (standard deviation of 10)\n",
    "                                      n_informative=2,     # Only 2 of the features are truly informative\n",
    "                                      random_state=42,     # Ensure reproducibility of results with a fixed random seed\n",
    "                                      coef=True)           # Return the true coefficients used to generate the data\n",
    "\n",
    "# Ridge Regression (L2 Regularization)\n",
    "print(\"=== Ridge (L2 penalty) ===\")\n",
    "model = lm.RidgeCV(cv=3)                                   # Use Ridge regression with built-in cross-validation\n",
    "                                                           # RidgeCV automatically chooses the best alpha (regularization strength) \n",
    "                                                           # from a range of values\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)  # Perform 5-fold cross-validation to evaluate the model\n",
    "print(\"Test r^2: %.2f\" % scores.mean())                    # Print the average R-squared score from the 5 cross-validation runs\n",
    "print()\n",
    "\n",
    "# Lasso Regression (L1 Regularization)\n",
    "print(\"=== Lasso (L1 penalty) ===\")\n",
    "model = lm.LassoCV(n_jobs=-1, cv=3)                        # Use Lasso regression with built-in cross-validation\n",
    "                                                           # LassoCV automatically selects the best alpha for L1 regularization\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)  # Perform 5-fold cross-validation to evaluate the model\n",
    "print(\"Test r^2: %.2f\" % scores.mean())                    # Print the average R-squared score from the 5 cross-validation runs\n",
    "print()\n",
    "\n",
    "# ElasticNet Regression (Combination of L1 and L2 Regularization)\n",
    "print(\"=== ElasticNet (L1 penalty) ===\")\n",
    "model = lm.ElasticNetCV(l1_ratio=[.1, .5, .9], n_jobs=-1, cv=3)  # Use ElasticNet regression with cross-validation\n",
    "                                                                 # l1_ratio=[.1, .5, .9] tests multiple L1/L2 combinations\n",
    "                                                                 # ElasticNetCV automatically chooses the best alpha and l1_ratio\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)  # Perform 5-fold cross-validation to evaluate the model\n",
    "print(\"Test r^2: %.2f\" % scores.mean())                    # Print the average R-squared score from the 5 cross-validation runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece22a05-0238-43ff-a2ef-f3a0c781df81",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "**The Ridge regression** model with L2 regularization performed poorly in this case, as it explains only 16% of the variance in the target variable (R¬≤ = 0.16). This could indicate that Ridge regression might not be the best choice for this dataset, especially since Ridge tends to spread the effect of regularization over all features, even those that may not be very informative.\n",
    "\n",
    "**The Lasso regression** model performs significantly better, explaining 74% of the variance in the target variable. Lasso (L1 regularization) helps with feature selection by driving some of the less important coefficients to zero. This suggests that many of the features in this dataset are irrelevant, and Lasso is better at isolating the truly important ones, which leads to improved predictive performance.\n",
    "\n",
    "**ElasticNet**, which combines both L1 and L2 regularization, performs moderately well with an R¬≤ score of 0.58. This result is better than Ridge but worse than Lasso. This may suggest that the combination of L1 and L2 regularization is not as effective as pure L1 regularization for this particular dataset, where Lasso's ability to select a sparse set of important features is more beneficial.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "**Lasso** clearly outperforms both Ridge and ElasticNet on this dataset, suggesting that the dataset likely has a few highly informative features, while many others contribute little to the prediction task. Lasso's feature selection capability (driving some coefficients to zero) is likely beneficial here. **Ridge** fails to perform well, likely because it doesn't perform feature selection and includes all features in its model, leading to lower predictive performance. **ElasticNet** strikes a balance between Ridge and Lasso, but in this case, the pure L1 regularization of Lasso works better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3504c-71e2-479a-9e9f-9f2857c8cfc6",
   "metadata": {},
   "source": [
    "## Classification Models with Built-in Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d62dca0-3e39-4742-8ea1-5cb02a6a2528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Ridge (L2 penalty) ===\n",
      "Test ACC: 0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets  \n",
    "import sklearn.linear_model as lm  \n",
    "import sklearn.metrics as metrics \n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "# Generate a synthetic classification dataset\n",
    "X, y = datasets.make_classification(n_samples=100,    # 100 samples\n",
    "                                    n_features=100,   # 100 features in total\n",
    "                                    n_informative=10, # 10 informative features\n",
    "                                    random_state=42)  # Ensure reproducibility with a fixed seed\n",
    "\n",
    "# Define a custom balanced accuracy scorer function\n",
    "def balanced_acc(estimator, X, y, **kwargs):\n",
    "    '''\n",
    "    Balanced accuracy scorer function that computes the mean recall score for each class.\n",
    "    This function is used as a custom scoring metric for evaluating the classifier's performance.\n",
    "    '''\n",
    "    return metrics.recall_score(y, estimator.predict(X), average=None).mean()  # Compute mean recall for each class\n",
    "\n",
    "print(\"=== Logistic Ridge (L2 penalty) ===\")\n",
    "\n",
    "# Initialize a Logistic Regression model with cross-validation\n",
    "model = lm.LogisticRegressionCV(class_weight='balanced',  # Set 'balanced' class weights to handle imbalanced data\n",
    "                                scoring=balanced_acc,     # Use the custom balanced accuracy scorer function\n",
    "                                n_jobs=-1,                # Use all available cores for parallel processing\n",
    "                                cv=3)                     # Perform 3-fold cross-validation\n",
    "\n",
    "# Perform cross-validation using 5-fold cross-validation\n",
    "scores = cross_val_score(estimator=model,  # The Logistic Regression model with CV\n",
    "                         X=X,              # Feature matrix\n",
    "                         y=y,              # Target vector\n",
    "                         cv=5)             # 5-fold cross-validation\n",
    "\n",
    "# Print the mean test balanced accuracy score across all folds\n",
    "print(\"Test ACC: %.2f\" % scores.mean())    # Display the average accuracy over the 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8d8a6-bfac-4c9c-b5a7-4b8878e3a651",
   "metadata": {},
   "source": [
    "## Random Permutations\n",
    "\n",
    "A permutation test is a type of non-parametric randomization test in which the null distribution of a test statistic is estimated by randomly permuting the observations.\n",
    "\r\n",
    "Permutation tests are highly attractive because they make no assumptions other than that th \r",
    "observations are independent and identically distributed under the null hypothesis\n",
    ".\r\n",
    "1. Compute a observed statistic ùë°ùëúùëèùë† on the data.\r\n",
    "2. Use randomization to compute the distribution of ùë° under the null hypothesis: Perform ùëÅ\r\n",
    "random permutation of the data. For each sample of permuted data, ùëñ the data com ute\r\n",
    "the statistic ùë°ùëñ. This procedure provides the distribution of ùë° under the null hypothesis ùêª0:\r\n",
    "ùëÉ (ùë°|ùêª0)\r\n",
    "3. Compute the p-value = ùëÉ (ùë° > ùë°ùëúùëèùë†|ùêª0) |{ùë°ùëñ > ùë°ùëúùëèùë†}|, where ùë°ùëñ‚Äôs include ùë°ùëúùëèùë†."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141b633-d1a3-4b69-beb9-16b7b4bb142e",
   "metadata": {},
   "source": [
    "### Example with a Correlation (the statistic is the correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09b75f61-4bdf-4b23-bcf0-95ca5e1118b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation two-tailed p-value = 0.06959. \n",
      "Pearson test p-value = 0.07355\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats  # Import statistical functions module for correlation tests\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)                                   # Fix the seed for reproducibility\n",
    "x = np.random.normal(loc=10, scale=1, size=100)      # Generate 100 samples from a normal distribution with mean=10 and std=1\n",
    "y = x + np.random.normal(loc=-3, scale=3, size=100)  # Generate 'y' as 'x' plus noise (mean=-3, std=3). Signal-to-noise ratio (snr) = 1/2\n",
    "\n",
    "# Permutation test: simulate the null hypothesis\n",
    "nperm = 10000                       # Number of permutations\n",
    "perms = np.zeros(nperm + 1)         # Array to store correlation values under the null hypothesis\n",
    "perms[0] = np.corrcoef(x, y)[0, 1]  # Compute the observed correlation coefficient between 'x' and 'y'\n",
    "for i in range(1, nperm):\n",
    "    perms[i] = np.corrcoef(np.random.permutation(x), y)[0, 1]  # Permute 'x' and calculate correlation under the null hypothesis\n",
    "\n",
    "# Plotting the results of the permutation test\n",
    "plt.figure(figsize=(10, 6))                                    # Set figure size to make the graph larger\n",
    "weights = np.ones(perms.shape[0]) / perms.shape[0]             # Set equal weights for the histogram bins\n",
    "\n",
    "plt.hist([perms[perms >= perms[0]], perms],                    # Plot the histogram of the correlation distribution\n",
    "         histtype='stepfilled',                                # Use filled steps for the histogram style\n",
    "         bins=100,                                             # Set the number of bins to 100\n",
    "         label=[\"t>t obs (p-value)\", \"t<t obs\"],               # Labels for areas representing p-value\n",
    "         weights=[weights[perms >= perms[0]], weights])        # Apply weights to the histogram \n",
    "\n",
    "plt.xlabel(\"Statistic distribution under null hypothesis\")                      # Label the x-axis\n",
    "plt.axvline(x=perms[0], color='blue', linewidth=1, label=\"observed statistic\")  # Draw a vertical line for the observed statistic\n",
    "_ = plt.legend(loc=\"upper left\")                                                # Add a legend in the upper left corner of the plot\n",
    "\n",
    "# Compute the empirical one-tailed p-value from the permutation test\n",
    "pval_perm = np.sum(perms >= perms[0]) / perms.shape[0]  # Count how often the permuted correlations are greater than or equal to the observed value\n",
    "\n",
    "# Compare with the p-value from Pearson's correlation test\n",
    "_, pval_test = stats.pearsonr(x, y)  # Perform Pearson correlation test on the data 'x' and 'y'\n",
    "print(\"Permutation two-tailed p-value = %.5f. \\nPearson test p-value = %.5f\" % (2*pval_perm, pval_test))  # Display the permutation p-value and \n",
    "print()                                                                                                   # the Pearson p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f3814-22af-464f-8c85-1a1f21d47789",
   "metadata": {},
   "source": [
    "#### Concepts Explained:\n",
    "\n",
    "Permutation Test:\n",
    "\n",
    "- This is a non-parametric method to estimate the distribution of a test statistic under the null hypothesis. In this code, the null hypothesis assumes no relationship between x and y.\n",
    "The perms array stores the correlation coefficients between permuted versions of x and y. The observed correlation is stored in perms[0].\n",
    "\n",
    "Null Hypothesis:\n",
    "\n",
    "- The permutation of x while keeping y fixed simulates the null hypothesis that there is no association between x and y.\n",
    "\n",
    "Empirical p-value:\n",
    "\n",
    "- The p-value is computed as the proportion of permutations where the correlation coefficient is greater than or equal to the observed value (perms[0]). This represents how likely the observed correlation is under the null hypothesis.\n",
    "\n",
    "Pearson‚Äôs Correlation:\n",
    "\n",
    "- The code also calculates the p-value from the Pearson correlation test using scipy.stats.pearsonr. This provides a parametric test for correlation based on assumptions of normality.\n",
    "\n",
    "Visualization:\n",
    "\n",
    "- The histogram shows the distribution of correlation values under the null hypothesis. The blue vertical line marks the observed correlation statistic, and the area to the right represents the empirical p-value.\n",
    "\n",
    "Two-Tailed Test:\n",
    "\n",
    "- The final p-value from the permutation test is multiplied by 2 to account for a two-tailed test, even though only the right tail is shown.\n",
    "\n",
    "#### Interpretation of the Output:\n",
    "\n",
    "The code compares the p-value obtained from the permutation test to the p-value from Pearson‚Äôs correlation test. If both p-values are low, it suggests that the observed correlation is significantly different from what would be expected under the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b836e85-d343-4111-aae8-e0a9688a5dcc",
   "metadata": {},
   "source": [
    "The final p-value from the permutation test is multiplied by 2 to account for a two-tailed test, even though only the right tail is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37d7aa-accb-4568-aefa-2ff1abf4fd26",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Given the logistic regression presented above and its validation given a 5 folds CV.\n",
    "\n",
    "1. Compute the p-value associated with the prediction accuracy using a permutation test.\n",
    "2. Compute the p-value associated with the prediction accuracy using a parametric test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f29b9b0-eeb7-45dd-8453-d9437240cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Accuracy: 0.7900\n",
      "Permutation test p-value: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametric binomial test p-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold  \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.datasets import make_classification  \n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Step 1: Create a dataset\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "X, y = make_classification(n_samples=100, n_features=20,         # Generate a classification dataset with 100 samples, 20 features\n",
    "                           n_informative=5, random_state=42)     # 5 informative features, random seed for reproducibility\n",
    "\n",
    "# Step 2: Logistic Regression with 5-fold cross-validation\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)        # Initialize logistic regression model with 'lbfgs' solver and 1000 iterations\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold stratified cross-validation with shuffled data\n",
    "\n",
    "# Compute accuracy using cross-validation\n",
    "accuracies = cross_val_score(model, X, y, cv=cv, scoring='accuracy')  # Perform cross-validation and calculate accuracy scores\n",
    "observed_accuracy = accuracies.mean()                                 # Compute mean accuracy across folds\n",
    "print(f\"Observed Accuracy: {observed_accuracy:.4f}\")                  # Print the observed accuracy\n",
    "\n",
    "# Exercise 1: Permutation Test\n",
    "\n",
    "# Step 3: Permutation test to calculate empirical p-value for accuracy\n",
    "n_permutations = 1000                       # Number of permutations for null hypothesis simulation\n",
    "perm_accuracies = np.zeros(n_permutations)  # Initialize array to store accuracies from permuted labels\n",
    "\n",
    "for i in range(n_permutations):                                               # Loop through each permutation\n",
    "    y_permuted = np.random.permutation(y)                                     # Shuffle labels to break association between X and y\n",
    "    perm_accuracies[i] = cross_val_score(model, X, y_permuted,                # Perform cross-validation with permuted labels\n",
    "                                         cv=cv, scoring='accuracy').mean()    # Store mean accuracy from permutation\n",
    "\n",
    "# Compute empirical p-value\n",
    "p_value_perm = np.sum(perm_accuracies >= observed_accuracy) / n_permutations  # Calc. p-value based on how often permuted accuracy >= observed accuracy\n",
    "print(f\"Permutation test p-value: {p_value_perm:.4f}\")                        # Print permutation test p-value\n",
    "\n",
    "# Plot permutation distribution\n",
    "plt.figure(figsize=(10, 6))                                                                  # Set figure size for plot\n",
    "plt.hist(perm_accuracies, bins=30, alpha=0.7, label=\"Permutation Accuracies\", color='grey')  # Plot histogram of permutation accuracies\n",
    "plt.axvline(observed_accuracy,        # Add a vertical line at the observed accuracy value\n",
    "            color='red',              # Set the line color to red\n",
    "            linestyle='--',           # Use a dashed line style\n",
    "            linewidth=2,              # Set the line width to 2 for better visibility\n",
    "            label=f\"Observed Accuracy: {observed_accuracy:.4f}\")  # Label the line with the observed accuracy value formatted to 4 decimal places\n",
    "\n",
    "plt.xlabel(\"Accuracy\")      # Label for x-axis\n",
    "plt.ylabel(\"Frequency\")     # Label for y-axis\n",
    "plt.title(\"Permutation Test Distribution for Accuracy\")           # Set plot title\n",
    "plt.legend()                # Show legend\n",
    "plt.show()                  # Display the plot\n",
    "\n",
    "# Exercise 2: Parametric Test\n",
    "\n",
    "# Step 4: Parametric test (Binomial Test)\n",
    "# Assuming the null hypothesis: accuracy = 50% (random guessing)\n",
    "n_samples = len(y)                                                # Get the total number of samples in the dataset\n",
    "observed_correct_predictions = int(observed_accuracy * n_samples) # Calculate the number of correct predictions based on observed accuracy\n",
    "p_value_param = stats.binomtest(observed_correct_predictions,     # Perform binomial test to compare observed correct predictions to chance level (50%)\n",
    "                                 n=n_samples, p=0.5,              # Null hypothesis assumes accuracy of 50%\n",
    "                                 alternative='greater')           # One-tailed test to see if accuracy is greater than 50%\n",
    "\n",
    "# Access p-value from the BinomTestResult object\n",
    "print(f\"Parametric binomial test p-value: {p_value_param.pvalue:.4f}\")   # Print parametric test p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc0db2-4839-42de-8230-d8c67d69b5cb",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "The results and graph you presented show the outcome of two statistical tests ‚Äî **Permutation Test** and **Parametric Binomial Test** ‚Äî both used to determine if the observed classification accuracy (79%) is significantly higher than the accuracy expected by chance (50%).\n",
    "\n",
    "1. `Observed Accuracy: 0.7900`\n",
    "\n",
    "- It means that the logistic regression classifier correctly predicted the class labels approximately 79% of the time.\n",
    " \n",
    "2. `Permutation Test p-value: 0.0000`\n",
    "\n",
    "- The **Permutation Test** assesses how often an accuracy equal to or greater than the observed accuracy (0.79) can be achieved by random chance when the labels are randomly shuffled.\n",
    "\n",
    "- In this case, none of the 1000 permutations resulted in an accuracy equal to or higher than 0.79. This results in a p-value of nearly zero (0.0000), meaning the observed accuracy is highly unlikely to be the result of chance, providing strong evidence that the model's accuracy is significantly better than random guessing.\n",
    "\n",
    "3. `Parametric Binomial Test p-value: 0.0000`\n",
    "\n",
    "- The Parametric Binomial Test assumes that under the null hypothesis, the model's accuracy is at chance level (50%). This test calculates the likelihood of getting the observed number of correct predictions (79%) or more by random chance if the actual accuracy is 50%.\n",
    "\n",
    "- The resulting p-value is also very close to zero (0.0000), reinforcing the conclusion that the observed accuracy is significantly better than random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d11ae0-13cd-44fe-afa6-87d0f5a30d1e",
   "metadata": {},
   "source": [
    "#### Graph Explanation:\n",
    "\n",
    "Histogram of Permutation Accuracies:\n",
    "\n",
    "- The gray bars represent the distribution of accuracies obtained by randomly permuting the labels and performing cross-validation. The majority of these permutation accuracies fall around 0.4 to 0.6, indicating that the accuracy of a random classifier would hover around these values.\n",
    "\n",
    "Red Line for Observed Accuracy:\n",
    "\n",
    "- The vertical red dashed line represents the observed accuracy (0.79). As you can see, none of the permuted datasets achieved an accuracy close to 0.79, which visually illustrates why the permutation test p-value is close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc42f3b-4218-4dee-997b-a0ac2c2563f2",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "Both the permutation test and the parametric binomial test strongly suggest that the observed accuracy (79%) is statistically significant and unlikely to have occurred by chance. This means the model is effectively learning to classify the data and is not simply making random guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9341128-d9c7-46b4-9be0-c161ad8507a0",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23585592-a5f8-4c1e-9b3e-95023209eecb",
   "metadata": {},
   "source": [
    "Bootstrapping is a random sampling with replacement strategy which provides an nonparametric method to assess the variability of performances scores such standard errors or *confidence intervals*.\n",
    "    \n",
    "A great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients.\n",
    "\n",
    "1. Perform ùêµ sampling, with replacement, of the dataset.\n",
    "2. For each sample ùëñ fit the model and compute the scores.\n",
    "3. Assess standard errors and confidence intervals of scores using the scores obtained on the ùêµ resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b555f18-7bbc-49c2-8c91-380f04fd5af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients on all data:\n",
      "[ 0.70103901  1.06311058  0.06684312 -0.08371484  0.18076706]\n",
      "\n",
      "R-squared: Mean=0.58, SE=0.11, CI=(0.39 0.75)\n",
      "\n",
      "Coefficients distribution:\n",
      "                0           1           2           3           4\n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
      "mean     0.688011    1.045577    0.074811   -0.082345    0.177704\n",
      "std      0.118569    0.095599    0.106067    0.078052    0.120496\n",
      "min      0.365485    0.715317   -0.219959   -0.377492   -0.115183\n",
      "2.5%     0.414031    0.874497   -0.108586   -0.214639   -0.059820\n",
      "50%      0.686023    1.046438    0.076616   -0.076997    0.171381\n",
      "97.5%    0.875860    1.223287    0.301039    0.045398    0.404360\n",
      "max      0.919940    1.299352    0.351070    0.092107    0.487630\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset parameters\n",
    "n_features = 5          # Number of features (5 features in the dataset)\n",
    "n_features_info = 2     # Number of informative features (only 2 out of 5 are informative)\n",
    "n_samples = 100         # Number of samples (100 data points)\n",
    "\n",
    "# Generate random dataset\n",
    "X = np.random.randn(n_samples, n_features)  # Generate random feature matrix with normal distribution\n",
    "beta = np.zeros(n_features)                 # Initialize coefficients (beta) as zero\n",
    "beta[:n_features_info] = 1                  # Set informative features (first two) to 1\n",
    "Xbeta = np.dot(X, beta)                     # Compute linear combination of X and beta (signal)\n",
    "eps = np.random.randn(n_samples)            # Generate random noise (epsilon)\n",
    "y = Xbeta + eps                             # Compute target values as signal + noise\n",
    "\n",
    "# Fit model on all data (!! risk of overfit)\n",
    "model = lm.RidgeCV()                        # Ridge regression with cross-validation to select alpha\n",
    "model.fit(X, y)                             # Fit the model to the entire dataset (risk of overfitting)\n",
    "\n",
    "print(\"Coefficients on all data:\")          # Display the coefficients learned from the entire dataset\n",
    "print(model.coef_)\n",
    "\n",
    "# Bootstrap loop\n",
    "nboot = 100                                         # Number of bootstrap iterations (should be at least 1000 for better results)\n",
    "scores_names = [\"r2\"]                               # We are interested in R-squared score\n",
    "scores_boot = np.zeros((nboot, len(scores_names)))  # Initialize array to store R-squared scores for each bootstrap\n",
    "coefs_boot = np.zeros((nboot, X.shape[1]))          # Initialize array to store coefficients for each bootstrap\n",
    "orig_all = np.arange(X.shape[0])                    # Create an array of indices for the original dataset\n",
    "\n",
    "for boot_i in range(nboot):                                                 # Perform bootstrap iterations\n",
    "    boot_tr = np.random.choice(orig_all, size=len(orig_all), replace=True)  # Sample indices with replacement\n",
    "    boot_te = np.setdiff1d(orig_all, boot_tr, assume_unique=False)          # Test set indices (not in the training set)\n",
    "    Xtr, ytr = X[boot_tr, :], y[boot_tr]                                    # Training data from bootstrap sample\n",
    "    Xte, yte = X[boot_te, :], y[boot_te]                                    # Test data from non-sampled indices\n",
    "    model.fit(Xtr, ytr)                                                     # Fit model on the bootstrap training set\n",
    "    y_pred = model.predict(Xte).ravel()                                     # Predict on the test set\n",
    "    scores_boot[boot_i, :] = metrics.r2_score(yte, y_pred)                  # Store the R-squared score for this bootstrap\n",
    "    coefs_boot[boot_i, :] = model.coef_                                     # Store the coefficients for this bootstrap\n",
    "\n",
    "# Compute Mean, SE, CI\n",
    "scores_boot = pd.DataFrame(scores_boot, columns=scores_names)               # Convert R-squared scores to DataFrame\n",
    "scores_stat = scores_boot.describe(percentiles=[.025, .5, .975])            # Compute percentiles (2.5%, 50%, and 97.5%)\n",
    "\n",
    "# Print R-squared statistics: mean, standard error, and confidence intervals (95% CI)\n",
    "print(\"\\nR-squared: Mean=%.2f, SE=%.2f, CI=(%.2f %.2f)\" % \n",
    "      tuple(scores_stat.loc[[\"mean\", \"std\", \"2.5%\", \"97.5%\"], \"r2\"]))       # Mean, Std, and 95% confidence interval\n",
    "\n",
    "# Coefficients analysis\n",
    "coefs_boot = pd.DataFrame(coefs_boot)                                       # Convert coefficients to DataFrame\n",
    "coefs_stat = coefs_boot.describe(percentiles=[.025, .5, .975])              # Compute percentiles for coefficients (2.5%, 50%, 97.5%)\n",
    "\n",
    "print(\"\\nCoefficients distribution:\")                                        # Print coefficient statistics\n",
    "print(coefs_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238baea-71fc-418d-865a-ccfebe6e7c3c",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "`Coefficients on all data`:\n",
    "\n",
    "The coefficients represent the weight each feature contributes to the prediction. Here's what the values indicate:\n",
    "\n",
    "- The first two coefficients (0.9536, 0.8813) are the most significant because, as per the data generation process, we know that the first two features are informative features. The model is assigning strong weights to these features.\n",
    "\n",
    "- The last three coefficients (0.0438, 0.0838, -0.0736) are closer to zero, indicating that these features are less informative or provide minimal contribution to the prediction. This makes sense since only two features are actually informative in the dataset.\n",
    "\n",
    "`R-squared: Mean=0.41, SE=0.14, CI=(0.09 0.58)`:\n",
    "\n",
    "- An R-squared value of 0.41 means that, on average, 41% of the variance in the target variable is explained by the model. This is a moderate fit‚Äîbetter than random, but far from perfect.\n",
    "\n",
    "- SE (Standard Error): The standard error of 0.14 reflects the variability of the R-squared values across the 100 bootstrap iterations. The higher the SE, the more variability in the performance of the model across different samples.\n",
    "\n",
    "- Confidence Interval (CI): The 95% confidence interval of the R-squared values is (0.09, 0.58), meaning that the true R-squared is likely to lie within this range. The wide confidence interval shows that the model's performance varies considerably across different bootstrapped samples. The lower bound (0.09) suggests that the model could perform poorly on some samples.\n",
    "\n",
    "`Coefficients distribution`:\n",
    "\n",
    "This table provides statistics on the distribution of the coefficients across the 100 bootstrap iterations.\n",
    "\n",
    "- Percentiles (2.5% and 97.5%): These are the boundaries for the 95% confidence interval for each coefficient. For instance, the first coefficient's 95% CI is (0.7102, 1.2447), meaning the true value of this coefficient is likely within this range. Similarly, for the other features, their CIs suggest a much larger variation for less important features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
