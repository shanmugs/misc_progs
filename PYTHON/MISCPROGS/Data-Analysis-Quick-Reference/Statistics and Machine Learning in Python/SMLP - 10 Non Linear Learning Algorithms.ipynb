{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e8aa49-4745-40ed-80fb-c52cb7510fa9",
   "metadata": {},
   "source": [
    "# Chapter 5.5 - Non Linear Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ebff2c-7587-4745-95f9-619d08532096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c7077-13b1-4bef-a8f4-95e8f6a6b4eb",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091c469-1065-4d34-9252-0db21f8e022e",
   "metadata": {},
   "source": [
    "Non linear SVM also exists for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab150bf-5d1a-4685-8025-8e5d56eaec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Errors: \n",
      "0\n",
      "\n",
      "The support vectors stored in dataset match exactly with support vectors found by the model?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC  # Import the Support Vector Classifier (SVC) from sklearn\n",
    "from sklearn import datasets  # Import datasets module from sklearn for generating sample datasets\n",
    "import numpy as np  # Import NumPy for numerical computations\n",
    "\n",
    "# Generate a binary classification dataset with 10 samples and 2 features\n",
    "X, y = datasets.make_classification(n_samples=10,    # Number of samples (10 samples)\n",
    "                                    n_features=2,    # Number of features (2 features per sample)\n",
    "                                    n_redundant=0,   # No redundant features\n",
    "                                    n_classes=2,     # Binary classification (2 classes)\n",
    "                                    random_state=1,  # Set seed for reproducibility\n",
    "                                    shuffle=False)   # Disable shuffling of the data\n",
    "\n",
    "# Initialize a Support Vector Classifier (SVC) with the Radial Basis Function (RBF) kernel\n",
    "clf = SVC(kernel='rbf')  # Radial Basis Function (RBF) kernel is the default\n",
    "# Optionally, you could set gamma (kernel coefficient), but it's not set here.\n",
    "\n",
    "# Fit the SVC model to the dataset\n",
    "clf.fit(X, y)  # Train the SVC model using the input data X and labels y\n",
    "\n",
    "# Print the number of errors (samples that were misclassified)\n",
    "print(\"Number of Errors: \\n%i\" % np.sum(y != clf.predict(X)))  # Compare the true labels with the predicted labels\n",
    "print()\n",
    "\n",
    "# Obtain the decision function values for the input data\n",
    "clf.decision_function(X)  # This returns the distance of the samples from the decision boundary\n",
    "\n",
    "# Useful Internals:\n",
    "\n",
    "# Array of support vectors (the critical points that lie on the edge of the margin)\n",
    "clf.support_vectors_\n",
    "\n",
    "# Indices of support vectors within the original dataset X\n",
    "print('The support vectors stored in dataset match exactly with support vectors found by the model?')\n",
    "np.all(X[clf.support_, :] == clf.support_vectors_)  # Verify that the support vectors match their indices in X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7a27c-9d8d-4d2a-9f4d-b615f3b6fc09",
   "metadata": {},
   "source": [
    "#### Concepts\n",
    "\n",
    "Number of Errors: This tells you how well the SVC classifier performed on the training dataset. Ideally, this number should be 0, indicating perfect classification.\n",
    "\n",
    "Support Vectors: These are the most critical points for defining the decision boundary, and examining them can provide insight into the model’s performance.\n",
    "\n",
    "Decision Function: The distance from the decision boundary shows how confident the model is in its predictions. Larger distances indicate higher confidence in the classification.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "Number of Errors = 0, indicating perfect classification.\n",
    "\n",
    "Since it returned True, it confirms that the model correctly identified the support vectors within the original dataset X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5369e1-00df-4d38-bdd5-6c53bc4885b8",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fecda-28b9-46bc-9d03-357d18f18aa0",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree learners on various\n",
    "sub-samples of the dataset and use averaging to improve the predictive accuracy and control\n",
    "over-fitting.\n",
    "\n",
    "Random forest models reduce the risk of overfitting by introducing randomness by:\n",
    "\n",
    "    • building multiple trees (n_estimators)\n",
    "    • drawing observations with replacement (i.e., a bootstrapped sample)\n",
    "    • splitting nodes on the best split among a random subset of the features selected at every node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24ba52c-1551-40c6-8774-41c51fba20c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Errors: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)             # Initialize the Random Forest Classifier with 100 trees (estimators)\n",
    "forest.fit(X, y)                                                # Fit the Random Forest model on the dataset X (features) and y (labels)\n",
    "print(\"Number of Errors: %i\" % np.sum(y != forest.predict(X)))  # Compare the true labels (y) with the predicted labels and calculate # of errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
