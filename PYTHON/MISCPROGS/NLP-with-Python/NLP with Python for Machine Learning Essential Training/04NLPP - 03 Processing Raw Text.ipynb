{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to be called from this point on in the book\n",
    "\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Accessing Text from the Web and from Disk\n",
    "\n",
    "*Getting from Project Gutenburg text number 2554, which is an English translation of __Crime and Punishment__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "\n",
    "# this line is different from the book\n",
    "# we need to decode without byte order mark (BOM)\n",
    "# so use this encoding instead\n",
    "\n",
    "raw = response.read().decode('utf-8-sig') \n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Slightly different from the book's answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176811"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokenizing the text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)  # Tokenize the raw text into individual words (tokens)\n",
    "type(tokens)                 # Check the type of the 'tokens' object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257058"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']"
     ]
    }
   ],
   "source": [
    "print(tokens[:10], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Converting to an NLTK text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: The Project Gutenberg eBook of Crime and Punishment...>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insight', 'impresses', 'us', 'as', 'wisdom', '...', 'that', 'wisdom', 'of', 'the', 'heart', 'which', 'we', 'seek', 'that', 'we', 'may', 'learn', 'from', 'it', 'how', 'to', 'live', '.', 'All', 'his', 'other', 'gifts', 'came', 'to', 'him', 'from', 'nature', ',', 'this', 'he', 'won', 'for']"
     ]
    }
   ],
   "source": [
    "print(text[1024:1062], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Project Gutenberg; Ilya\n",
      "Petrovitch; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n"
     ]
    }
   ],
   "source": [
    "text.collocations()  # Finds and prints collocations (frequent word pairs) in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting rid of header and footer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")  # Search for the substring \"PART I\" in the raw text and return its starting index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This returns `-1`, which means the search failed:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.rfind(\"End of Gutenberg's Crim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The reason is because older Project Gutenberg texts often use curly single quotes `’`, which are not used in many modern systems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.rfind(\"End of Project Gutenberg’s Crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[5335:1157811]  # Extracts a portion of the raw text from index 5335 to 1157811\n",
    "raw.find(\"PART I\")       # Search for the substring \"PART I\" in this extracted portion of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, \"PART I\" appears 239 characters from the start of the sliced portion of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All the HTML content:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use __BeautifulSoup__ to get text out of HTML:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK', 'Friday', ',', '27', 'September', ',', '2002', ',', '11:51', 'GMT', '12:51', 'UK', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'Scientists', 'believe', 'the', 'last', 'blondes', 'will', 'be', 'in', 'Finland', 'The', 'last', 'natural', 'blondes', 'will', 'die', 'out', 'within', '200', 'years', ',', 'scientists', 'believe', '.', 'A', 'study', 'by', 'experts', 'in', 'Germany', 'suggests', 'people', 'with', 'blonde', 'hair', 'are', 'an', 'endangered', 'species', 'and', 'will', 'become', 'extinct', 'by', '2202', '.', 'Researchers', 'predict', 'the', 'last', 'truly', 'natural', 'blonde', 'will', 'be', 'born', 'in', 'Finland', '-', 'the', 'country', 'with', 'the', 'highest', 'proportion', 'of', 'blondes', '.', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', 'Prof', 'Jonathan', 'Rees', ',', 'University', 'of', 'Edinburgh', 'But', 'they', 'say', 'too', 'few', 'people', 'now', 'carry', 'the', 'gene', 'for', 'blondes', 'to', 'last', 'beyond', 'the', 'next', 'two', 'centuries', '.', 'The', 'problem', 'is', 'that', 'blonde', 'hair', 'is', 'caused', 'by', 'a', 'recessive', 'gene', '.', 'In', 'order', 'for', 'a', 'child', 'to', 'have', 'blonde', 'hair', ',', 'it', 'must', 'have', 'the', 'gene', 'on', 'both', 'sides', 'of', 'the', 'family', 'in', 'the', 'grandparents', \"'\", 'generation', '.', 'Dyed', 'rivals', 'The', 'researchers', 'also', 'believe', 'that', 'so-called', 'bottle', 'blondes', 'may', 'be', 'to', 'blame', 'for', 'the', 'demise', 'of', 'their', 'natural', 'rivals', '.', 'They', 'suggest', 'that', 'dyed-blondes', 'are', 'more', 'attractive', 'to', 'men', 'who', 'choose', 'them', 'as', 'partners', 'over', 'true', 'blondes', '.', 'Bottle-blondes', 'like', 'Ann', 'Widdecombe', 'may', 'be', 'to', 'blame', 'But', 'Jonathan', 'Rees', ',', 'professor', 'of', 'dermatology', 'at', 'the', 'University', 'of', 'Edinburgh', 'said', 'it', 'was', 'unlikely', 'blondes', 'would', 'die', 'out', 'completely', '.', '``', 'Genes', 'do', \"n't\", 'die', 'out', 'unless', 'there', 'is', 'a', 'disadvantage', 'of', 'having', 'that', 'gene', 'or', 'by', 'chance', '.', 'They', 'do', \"n't\", 'disappear', ',', \"''\", 'he', 'told', 'BBC', 'News', 'Online', '.', '``', 'The', 'only', 'reason', 'blondes', 'would', 'disappear', 'is', 'if', 'having', 'the', 'gene', 'was', 'a', 'disadvantage', 'and', 'I', 'do', 'not', 'think', 'that', 'is', 'the', 'case', '.', '``', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', '.', \"''\", 'See', 'also', ':', '28', 'Mar', '01', '|', 'Education', 'What', 'is', 'it', 'about', 'blondes', '?', '09', 'Apr', '99', '|', 'Health', 'Platinum', 'blondes', 'are', 'labelled', 'as', 'dumb', '17', 'Apr', '02', '|', 'Health', 'Hair', 'dye', 'cancer', 'alert', 'Internet', 'links', ':', 'University', 'of', 'Edinburgh', 'The', 'BBC', 'is', 'not', 'responsible', 'for', 'the', 'content', 'of', 'external', 'internet', 'sites', 'Top', 'Health', 'stories', 'now', ':', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'Links', 'to', 'more', 'Health', 'stories', 'are', 'at', 'the', 'foot', 'of', 'the', 'page', '.', 'E-mail', 'this', 'story', 'to', 'a', 'friend', 'Links', 'to', 'more', 'Health', 'stories', 'In', 'This', 'Section', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'How', 'sperm', 'wriggle', 'Bollywood', 'told', 'to', 'stub', 'it', 'out', 'Fears', 'over', 'tuna', 'health', 'risk', 'to', 'babies', 'Public', 'can', 'be', 'taught', 'to', 'spot', 'strokes', '^^', 'Back', 'to', 'top', 'News', 'Front', 'Page', '|', 'Africa', '|', 'Americas', '|', 'Asia-Pacific', '|', 'Europe', '|', 'Middle', 'East', '|', 'South', 'Asia', '|', 'UK', '|', 'Business', '|', 'Entertainment', '|', 'Science/Nature', '|', 'Technology', '|', 'Health', '|', 'Talking', 'Point', '|', 'Country', 'Profiles', '|', 'In', 'Depth', '|', 'Programmes', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'To', 'BBC', 'Sport', '>', '>', '|', 'To', 'BBC', 'Weather', '>', '>', '|', 'To', 'BBC', 'World', 'Service', '>', '>', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '©', 'MMIII', '|', 'News', 'Sources', '|', 'Privacy']"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content and extract the raw text\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "\n",
    "# Tokenize the extracted text into individual words (tokens)\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "# Print the tokens in a continuous format without newlines\n",
    "print(tokens, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'africa', 'aid', 'alert', 'alien', 'also', 'americas', 'an', 'and', 'ann', 'apr', 'are', 'as', 'asia', 'at', 'attractive', 'babies', 'back', 'bbc', 'be', 'become', 'believe', 'beyond', 'big', 'blame', 'blonde', 'blondes', 'blood', 'bollywood', 'born', 'both', 'botox', 'bottle', 'business', 'but', 'by', 'campaign', 'can', 'cancer', 'carry', 'case', 'cause', 'caused', 'centuries', 'chance', 'change', 'child', 'choose', 'completely', 'confirmed', 'congo', 'content', 'country', 'daily', 'defect', 'demise', 'depth', 'dermatology', 'die', 'disadvantage', 'disappear', 'do', 'drop', 'drug', 'dumb', 'dye', 'dyed', 'east', 'ebola', 'edinburgh', 'editions', 'education', 'endangered', 'entertainment', 'europe', 'experts', 'explains', 'external', 'extinct', 'families', 'family', 'fears', 'feedback', 'few', 'finland', 'foot', 'for', 'frequency', 'friday', 'friend', 'front', 'gene', 'generation', 'genes', 'germany', 'gmt', 'grandparents', 'hair', 'have', 'having', 'he', 'health', 'heart', 'help', 'high', 'highest', 'how', 'i', 'if', 'in', 'index', 'internet', 'iraq', 'is', 'it', 'jonathan', 'labelled', 'last', 'launched', 'like', 'link', 'links', 'mar', 'may', 'medical', 'men', 'middle', 'mmiii', 'more', 'must', 'natural', 'new', 'news', 'next', 'not', 'notes', 'now', 'of', 'off', 'on', 'online', 'only', 'or', 'order', 'out', 'outbreak', 'over', 'page', 'pain', 'partners', 'people', 'platinum', 'point', 'polio', 'predict', 'pressure', 'privacy', 'problem', 'prof', 'professor', 'profiles', 'programmes', 'proportion', 'public', 'real', 'reason', 'recessive', 'rees', 'researchers', 'responsible', 'risk', 'rivals', 'said', 'say', 'scientists', 'search', 'section', 'see', 'september', 'service', 'services', 'show', 'sides', 'sites', 'sources', 'south', 'species', 'sperm', 'sport', 'spot', 'stories', 'story', 'strokes', 'stub', 'study', 'suggest', 'suggests', 'symptoms', 'talking', 'taught', 'technology', 'text', 'that', 'the', 'their', 'them', 'there', 'they', 'think', 'this', 'ticker', 'to', 'told', 'too', 'top', 'true', 'truly', 'tuna', 'two', 'uk', 'university', 'unless', 'unlikely', 'vegetables', 'ward', 'was', 'weather', 'what', 'who', 'widdecombe', 'will', 'with', 'within', 'wo', 'world', 'would', 'wriggle', 'years', 'you']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Parse the HTML content and extract the raw text\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "\n",
    "# Tokenize the extracted text into individual words (tokens)\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "# Convert tokens to lowercase and remove duplicates using set\n",
    "unique_tokens = sorted(set(token.lower() for token in tokens if token.isalpha()))\n",
    "\n",
    "# Print the unique tokens\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using trial and error to find the start/finish of the text.  Again, the indices given in the book are not correct.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[111:403]  # Select a subset of the tokens from index 111 to 403\n",
    "text = nltk.Text(tokens)  # Convert the token list into an NLTK Text object for text processing\n",
    "text.concordance('gene')  # Display the concordance (context) for the word 'gene'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing Search Engine Results\n",
    "\n",
    "*__Your Turn__: Search the web for `\"the of\"` (inside quotes). Based on the large count, can we conclude that the of is a frequent collocation in English?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I got 165,000,000 results, which of course would be different than the number that the authors got while they were writing the book.*\n",
    "\n",
    "*Despite the high number, it would be very wrong to assume \"the of\" is a common English collocation.  A large percentage of the hits were the result of transposition errors in the text: e.g. someone might type \"the of point this...\" when they meant to type \"the point of this...\"; another large contingent of results were phrases where 'the' and 'of' were separated by punctuation (e.g., \"the # of\"), which was presumably ignored by the search engine.  However, it has to be said that the vast majority of results did not feature \"the of\" in the summary, leading me to belief that Google does not always look at literal strings (i.e., strings inside quotes) when ranking results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "# Parse the Atom feed from the provided URL\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "\n",
    "# Access and print the title of the feed\n",
    "llog['feed']['title']  # Retrieve the title of the feed from the parsed feed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llog.entries)  # Get the number of entries (posts/articles) in the feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&quot;The Truth About English Grammar&quot;'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = llog.entries[2]  # Access the third post (entry) from the list of feed entries\n",
    "post.title              # Retrieve and print the title of the third post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>It\\'s past time for me to feature Geoff Pullum\\'s new book, <a href=\"https://www.amazon.c'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = post.content[0].value  # Access the content of the third post's first content section\n",
    "content[:90]                     # Display the first 90 characters of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'past', 'time', 'for', 'me', 'to', 'feature', 'Geoff', 'Pullum', \"'s\", 'new', 'book', ',', 'The', 'Truth', 'About', 'English', 'Grammar', '.', 'The', 'publisher', \"'s\", 'blurb', ':', 'Do', 'you', 'worry', 'that', 'your', 'understanding', 'of', 'English', 'grammar', 'isn', '’', 't', 'what', 'it', 'should', 'be', '?', 'It', 'may', 'not', 'be', 'your', 'fault', '.', 'For']"
     ]
    }
   ],
   "source": [
    "raw = BeautifulSoup(content, 'html.parser').get_text()  # Parse the HTML content and extract the raw text\n",
    "print(word_tokenize(raw)[:50], end='')                  # Tokenize the text and print the first 50 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Local Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Your Turn:__ Create a file called `document.txt` using a text editor, and type in a few lines of text, and save it as plain text. If you are using IDLE, select the __New Window__ command in the File menu, typing the required text into this window, and then saving the file as `document.txt` inside the directory that IDLE offers in the pop-up dialogue box. Next, in the Python interpreter, open the file using `f = open('document.txt')`, then inspect its contents using `print(f.read())`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Japan, eggplants reach their peak of flavor during a period of time known as zansho (literally \"lingering heat\"), the equivalent of mid-August through late September. Such aki nasu, or autumn eggplants, are especially tasty. And, because eggplant is thought to cool the body (probably due to an unusually high concentration of minerals and phytonutrients in late-harvest fruit), dishes made with them are particularly inviting on days when heat and humidity sap the appetite.\n",
      "\n",
      "Most varieties of Japanese eggplants boast tender, deeply purple skins and juicy, pale yellow-green flesh. They are all nearly seedless, and some varieties, such as Kamo nasu grown around Kyoto, are bulbous and squat. Others, such as Hakata nasu grown in Kyushu, are long and slender. All true Japanese varieties have a dark calyx, not a green one. Most people think of eggplant and other members of the nightshade family such as tomatoes as vegetables because of their savory taste â€” botanically, however, they are fruit.\n"
     ]
    }
   ],
   "source": [
    "f = open('data/New Text Document.txt')\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Text from PDF, MSWord and other Binary Formats\n",
    "\n",
    "*__No notes__*\n",
    "\n",
    "##### Capturing User Input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = input(\"Enter some text: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It seems the `?` was tokenized and counted as a word.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Strings: Text Processing at the Lowest Level\n",
    "\n",
    "##### Basic Operations with Strings\n",
    "\n",
    "*Use `\\` or `( )` to extend a string over several lines:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I was a little bitty boyMy grandmother bought me a cute little toy\n"
     ]
    }
   ],
   "source": [
    "couplet =   \"When I was a little bitty boy\" \\\n",
    "            \"My grandmother bought me a cute little toy\" \n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver bells hanging on a stringShe told me it was my ding-a-ling-a-ling\n"
     ]
    }
   ],
   "source": [
    "couplet =   (\"Silver bells hanging on a string\" \n",
    "            \"She told me it was my ding-a-ling-a-ling\")\n",
    "\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To get a newline between the lines we should use a triple-quoted string:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You know, then mama took me to Sunday school\n",
      "They tried to teach me the golden rule\n"
     ]
    }
   ],
   "source": [
    "couplet = \"\"\"You know, then mama took me to Sunday school\n",
    "They tried to teach me the golden rule\"\"\"\n",
    "\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__Your Turn__: Try running the following code, then try to use your understanding of the string `+` and `*` operations to figure out how it works. Be careful to distinguish between the string `' '`, which is a single whitespace character, and `''`, which is the empty string.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            very\n",
      "          veryvery\n",
      "        veryveryvery\n",
      "      veryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "veryveryveryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "      veryveryveryvery\n",
      "        veryveryvery\n",
      "          veryvery\n",
      "            very\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    "b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
    "\n",
    "for line in b:\n",
    "     print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing Strings\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "##### Accessing Individual Characters\n",
    "\n",
    "*Frequency Distribution of individual characters in a text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 117092),\n",
       " ('t', 87996),\n",
       " ('a', 77916),\n",
       " ('o', 69326),\n",
       " ('n', 65617),\n",
       " ('i', 65434),\n",
       " ('s', 64231),\n",
       " ('h', 62896),\n",
       " ('r', 52134),\n",
       " ('l', 42793)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg \n",
    "\n",
    "raw = gutenberg.raw('melville-moby_dick.txt')                    # Load the raw text of 'Moby Dick' from the Gutenberg corpus\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())  # Create a frequency distribution of the lowercase alphabetic characters\n",
    "fdist.most_common(10)                                             # Display the 5 most common characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical frequency of letters in the English language:\n",
    "\n",
    "    E - 12.7%\n",
    "    T - 9.1%\n",
    "    A - 8.2%\n",
    "    O - 7.5%\n",
    "    I - 7.0%\n",
    "    N - 6.7%\n",
    "    S - 6.3%\n",
    "    H - 6.1%\n",
    "    R - 6.0%\n",
    "    D - 4.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w', 'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']"
     ]
    }
   ],
   "source": [
    "print([char for (char, count) in fdist.most_common()], end = '')  # Print all characters from the frequency distribution, sorted by their frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As a plot:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fdist.plot(cumulative = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accessing Substrings\n",
    "\n",
    "*__Your Turn__: Make up a sentence and assign it to a variable, e.g. `sent = 'my sentence...'`. Now write slice expressions to pull out individual words. (This is obviously not a convenient way to process the words of a text!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "learning\n",
      "how\n",
      "to\n",
      "code\n",
      "in\n",
      "Python\n"
     ]
    }
   ],
   "source": [
    "sent = \"I am learning how to code in Python\"  # Assign a sentence to the variable\n",
    "\n",
    "words = sent.split()  # Split the sentence into words based on spaces\n",
    "\n",
    "# Print each word in the sentence\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More operations on strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       Method      |                            Functionality                            |\n",
    "|:------------------|:--------------------------------------------------------------------|\n",
    "| `s.find(t)`       | index of first instance of   string `t` inside `s` (`-1` if not found)    |\n",
    "| `s.rfind(t)`      | index of last instance of   string `t` inside `s` (`-1` if not found)     |\n",
    "| `s.index(t)`      | like `s.find(t)` except it raises `ValueError` if not   found           |\n",
    "| `s.rindex(t)`     | like `s.rfind(t)` except it raises `ValueError` if not   found          |\n",
    "| `s.join(text)`    | combine the words of the text into a string using `s` as the glue     |\n",
    "| `s.split(t)`      | split `s` into a list wherever a `t` is found (whitespace   by default) |\n",
    "| `s.splitlines()`  | split `s` into a list of strings, one per line                        |\n",
    "| `s.lower()`       | a lowercased version of the string `s`                                |\n",
    "| `s.upper()`       | an uppercased version of the string `s`                               |\n",
    "| `s.title()`       | a titlecased version of the string `s`                                |\n",
    "| `s.strip()`       | a copy of `s` without leading or trailing whitespace                  |\n",
    "| `s.replace(t, u)` | replace instances of `t` with `u` inside `s`                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Difference between Lists and Strings\n",
    "\n",
    "*__No Notes.__*\n",
    "\n",
    "#### 3.3 Text Processing with Unicode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting encoded text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path, encoding='latin2') as f: # Open the file using the found path with 'latin2' encoding\n",
    "    \n",
    "    for line in f:   # Iterate through each line and print it without cleaning or stripping\n",
    "        print(line)  # This will print the line with all its original formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
     ]
    }
   ],
   "source": [
    "f = open(path, encoding='latin2')  # Open the file at the given path with 'latin2' encoding\n",
    "\n",
    "for line in f:           # Iterate over each line in the file\n",
    "    line = line.strip()  # Remove any leading/trailing whitespace characters, including newlines\n",
    "    print(line)          # Print the cleaned line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Converting all non-ASCII characters into two-digit \\xXX and four-digit \\uXXXX representations:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
     ]
    }
   ],
   "source": [
    "f = open(path, encoding='latin2')  # Open the file with 'latin2' encoding\n",
    "\n",
    "for line in f:                            # Iterate through each line in the file\n",
    "    line = line.strip()                   # Remove leading and trailing whitespace\n",
    "    print(line.encode('unicode_escape'))  # Print the line, encoding it to 'unicode_escape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting the integer ordinal of a character:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('ń')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Converting hexadecimal to 4 digit notation:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x144'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Print 4 digit notation:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ń'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nacute = '\\u0144'\n",
    "nacute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To see the representation of bytes:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xc5\\x84'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nacute.encode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module unicodedata lets us inspect the properties of Unicode characters. In the\n",
    "following example, we select all characters in the third line of our Polish text outside\n",
    "the ASCII range and print their UTF-8 escaped value, followed by their code point\n",
    "integer using the standard Unicode convention (i.e., prefixing the hex digits with U+),\n",
    "followed by their Unicode name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
     ]
    }
   ],
   "source": [
    "# Again, just as reference.\n",
    "\n",
    "f = open(path, encoding='latin2')  # Open the file at the given path with 'latin2' encoding\n",
    "\n",
    "for line in f:           # Iterate over each line in the file\n",
    "    line = line.strip()  # Remove any leading/trailing whitespace characters, including newlines\n",
    "    print(line)          # Print the cleaned line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Open the file at the given path using 'latin2' encoding and read all lines\n",
    "lines = open(path, encoding='latin2').readlines()\n",
    "\n",
    "# Access the third line (index 2) from the file\n",
    "line = lines[2]\n",
    "\n",
    "# Print the line encoded as 'unicode_escape' to display Unicode escape sequences\n",
    "print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = lines[2]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "b'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "b'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "b'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "b'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n"
     ]
    }
   ],
   "source": [
    "for c in line:        # Loop through each character in the line\n",
    "    if ord(c) > 127:  # Check if the character is non-ASCII (ord value > 127)\n",
    "        \n",
    "        # Print the character encoded in UTF-8, its Unicode code point, and its official Unicode name\n",
    "        print(\"{} U+{:04x} {}\".format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With UTF-8:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ó U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "ś U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "Ś U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "ą U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "ł U+0142 LATIN SMALL LETTER L WITH STROKE\n"
     ]
    }
   ],
   "source": [
    "for c in line:  # Loop through each character in the line\n",
    "    if ord(c) > 127:  # Check if the character's Unicode code point is greater than 127 (non-ASCII)\n",
    "        \n",
    "        # Print the character, its Unicode code point (in hex), and its official Unicode name\n",
    "        print(\"{} U+{:04x} {}\".format(c, ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using `re` with Unicode characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.find('zosta\\u0142y')  # Find the position of the substring 'zostały' in the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niemców pod koniec ii wojny światowej na dolny śląsk, zostały\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = line.lower()\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'niemc\\\\xf3w pod koniec ii wojny \\\\u015bwiatowej na dolny \\\\u015bl\\\\u0105sk, zosta\\\\u0142y\\\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.encode('unicode_escape')  # Encode the string into 'unicode_escape' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'światowej'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.search('\\u015b\\w*', line)  # Search for a word starting with the letter 'ś' (represented as \\u015b in Unicode)\n",
    "m.group()                         # Return the matched word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NLTK tokenizers also work with Unicode strings:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niemców pod koniec ii wojny światowej na dolny śląsk, zostały\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niemców', 'pod', 'koniec', 'ii', 'wojny', 'światowej', 'na', 'dolny', 'śląsk', ',', 'zostały']"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(line), end = '')  # Tokenize the string 'line' into words and print the result without a newline at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using your local encoding in Python\n",
    "\n",
    "*__No notes.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4   Regular Expressions for Detecting Word Patterns\n",
    "\n",
    "*We'll use the Words Corpus, but we need to remove proper names:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension to create a list of words in the 'en' (English) corpus, converted to lowercase\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.lower()]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Basic Meta-Characters\n",
    "\n",
    "*Using RegEx to find the first fify words ending with __ed__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded', 'absorbed', 'abstracted', 'abstricted', 'accelerated', 'accepted', 'accidented', 'accoladed', 'accolated', 'accomplished', 'accosted', 'accredited', 'accursed', 'accused', 'accustomed', 'acetated', 'acheweed', 'aciculated', 'aciliated', 'acknowledged', 'acorned', 'acquainted', 'acquired', 'acquisited', 'acred', 'aculeated', 'addebted', 'added', 'addicted', 'addlebrained', 'addleheaded', 'addlepated', 'addorsed', 'adempted', 'adfected', 'adjoined', 'admired', 'admitted', 'adnexed', 'adopted', 'adossed']"
     ]
    }
   ],
   "source": [
    "# Find words in 'wordlist' that end with 'ed' and print the first 50\n",
    "print([w for w in wordlist if re.search('ed$', w)][:50], end = '')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using `.` as a __wildcard__ to find 8-letter words whose respective third and sixth letters are __j__ and __t__:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector', 'unjilted', 'unjolted', 'unjustly']\n"
     ]
    }
   ],
   "source": [
    "# Find words that match the pattern '^..j..t..$' and print them\n",
    "print([w for w in wordlist if re.search('^..j..t..$', w)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "^..j..t..$: This regular expression matches words with the following pattern:\n",
    "\n",
    "    ^: The start of the string.\n",
    "    ..: Any two characters (two dots).\n",
    "    j: The letter 'j' as the third character.\n",
    "    ..: Any two characters after the 'j'.\n",
    "    t: The letter 't' as the sixth character.\n",
    "    ..: Any two characters following the 't'.\n",
    "    $: The end of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aculeolate', 'aculeolus', 'Aleochara', 'anthropoteleoclogy', 'anthropoteleological', 'ateleological', 'Betsileos', 'binucleolate', 'bipaleolate', 'bleo', 'bradyteleocinesia', 'bradyteleokinesis', 'caryophylleous', 'celeomorph', 'Celeomorphae', 'celeomorphic', 'ceruleolactite', 'ceruleous', 'Chamaeleo', 'Chamaeleon', 'Chamaeleontidae', 'chameleon', 'chameleonic', 'chameleonize', 'chameleonlike', 'cholecystoileostomy', 'choleokinase', 'choraleon', 'cleoid', 'Cleome', 'Cleopatra', 'cochleous', 'coeruleolactite', 'Coleochaetaceae', 'coleochaetaceous', 'Coleochaete', 'Coleophora', 'Coleophoridae', 'coleopter', 'Coleoptera', 'coleopteral', 'coleopteran', 'coleopterist', 'coleopteroid', 'coleopterological', 'coleopterology', 'coleopteron', 'coleopterous', 'coleoptile', 'coleoptilum', 'coleorhiza', 'Coleosporiaceae', 'Coleosporium', 'dysteleological', 'dysteleologist', 'dysteleology', 'eccaleobion', 'eleoblast', 'Eleocharis', 'eleolite', 'eleomargaric', 'eleometer', 'eleonorite', 'eleoptene', 'eleostearate', 'eleostearic', 'empleomania', 'endonucleolus', 'Eopaleozoic', 'epipaleolithic', 'Galeodes', 'Galeodidae', 'galeoid', 'Galeopithecus', 'Galeopsis', 'Galeorchis', 'Galeorhinidae', 'Galeorhinus', 'galleon', 'Heracleonite', 'Heracleopolitan', 'Heracleopolite', 'ichthyopaleontology', 'ileocaecal', 'ileocaecum', 'ileocolic', 'ileocolitis', 'ileocolostomy', 'ileocolotomy', 'ileon', 'ileosigmoidostomy', 'ileostomy', 'ileotomy', 'intermalleolar', 'laparoileotomy', 'leoncito', 'leonhardite', 'leonine', 'leoninely', 'leonines', 'leonite', 'leontiasis', 'leontocephalous', 'leopard', 'leoparde', 'leopardess', 'leopardine', 'leopardite', 'leopardwood', 'leopoldite', 'leotard', 'maleo', 'malleoincudal', 'malleolable', 'malleolar', 'malleolus', 'melleous', 'microcoleoptera', 'micropaleontology', 'mononucleosis', 'multinucleolar', 'multinucleolate', 'multinucleolated', 'Myrmeleon', 'Myrmeleonidae', 'Myrmeleontidae', 'Napoleon', 'napoleon', 'Napoleonana', 'Napoleonic', 'Napoleonically', 'Napoleonism', 'Napoleonist', 'Napoleonistic', 'napoleonite', 'Napoleonize', 'Neopaleozoic', 'nonteleological', 'nucleoalbumin', 'nucleoalbuminuria', 'nucleofugal', 'nucleohistone', 'nucleohyaloplasm', 'nucleohyaloplasma', 'nucleoid', 'nucleoidioplasma', 'nucleolar', 'nucleolated', 'nucleole', 'nucleoli', 'nucleolinus', 'nucleolocentrosome', 'nucleoloid', 'nucleolus', 'nucleolysis', 'nucleomicrosome', 'nucleon', 'nucleone', 'nucleonics', 'nucleopetal', 'nucleoplasm', 'nucleoplasmatic', 'nucleoplasmic', 'nucleoprotein', 'nucleoside', 'nucleotide', 'oleo', 'oleocalcareous', 'oleocellosis', 'oleocyst', 'oleoduct', 'oleograph', 'oleographer', 'oleographic', 'oleography', 'oleomargaric', 'oleomargarine', 'oleometer', 'oleoptene', 'oleorefractometer', 'oleoresin', 'oleoresinous', 'oleosaccharum', 'oleose', 'oleosity', 'oleostearate', 'oleostearin', 'oleothorax', 'oleous', 'paleoalchemical', 'paleoandesite', 'paleoanthropic', 'paleoanthropography', 'paleoanthropological', 'paleoanthropologist', 'paleoanthropology', 'Paleoanthropus', 'paleoatavism', 'paleoatavistic', 'paleobiogeography', 'paleobiologist', 'paleobiology', 'paleobotanic', 'paleobotanical', 'paleobotanically', 'paleobotanist', 'paleobotany', 'paleoceanography', 'Paleocene', 'paleochorology', 'paleoclimatic', 'paleoclimatologist', 'paleoclimatology', 'Paleoconcha', 'paleocosmic', 'paleocosmology', 'paleocrystal', 'paleocrystallic', 'paleocrystalline', 'paleocrystic', 'paleocyclic', 'paleodendrologic', 'paleodendrological', 'paleodendrologically', 'paleodendrologist', 'paleodendrology', 'paleoecologist', 'paleoecology', 'paleoencephalon', 'paleoeremology', 'paleoethnic', 'paleoethnography', 'paleoethnologic', 'paleoethnological', 'paleoethnologist', 'paleoethnology', 'paleofauna', 'Paleogene', 'paleogenesis', 'paleogenetic', 'paleogeographic', 'paleogeography', 'paleoglaciology', 'paleoglyph', 'paleograph', 'paleographer', 'paleographic', 'paleographical', 'paleographically', 'paleographist', 'paleography', 'paleoherpetologist', 'paleoherpetology', 'paleohistology', 'paleohydrography', 'paleoichthyology', 'paleokinetic', 'paleola', 'paleolate', 'paleolatry', 'paleolimnology', 'paleolith', 'paleolithic', 'paleolithical', 'paleolithist', 'paleolithoid', 'paleolithy', 'paleological', 'paleologist', 'paleology', 'paleomammalogy', 'paleometallic', 'paleometeorological', 'paleometeorology', 'paleontographic', 'paleontographical', 'paleontography', 'paleontologic', 'paleontological', 'paleontologically', 'paleontologist', 'paleontology', 'paleopathology', 'paleopedology', 'paleophysiography', 'paleophysiology', 'paleophytic', 'paleophytological', 'paleophytologist', 'paleophytology', 'paleopicrite', 'paleoplain', 'paleopotamoloy', 'paleopsychic', 'paleopsychological', 'paleopsychology', 'paleornithological', 'paleornithology', 'paleostriatal', 'paleostriatum', 'paleostylic', 'paleostyly', 'paleotechnic', 'paleothalamus', 'paleothermal', 'paleothermic', 'Paleotropical', 'paleovolcanic', 'paleoytterbium', 'Paleozoic', 'paleozoological', 'paleozoologist', 'paleozoology', 'pantaleon', 'panteleologism', 'petroleous', 'phytopaleontologic', 'phytopaleontological', 'phytopaleontologist', 'phytopaleontology', 'pileolated', 'pileolus', 'pileorhiza', 'pileorhize', 'pileous', 'pleochroic', 'pleochroism', 'pleochroitic', 'pleochromatic', 'pleochromatism', 'pleochroous', 'pleocrystalline', 'pleodont', 'pleomastia', 'pleomastic', 'pleomazia', 'pleometrosis', 'pleometrotic', 'pleomorph', 'pleomorphic', 'pleomorphism', 'pleomorphist', 'pleomorphous', 'pleomorphy', 'pleon', 'pleonal', 'pleonasm', 'pleonast', 'pleonaste', 'pleonastic', 'pleonastical', 'pleonastically', 'pleonectic', 'pleonexia', 'pleonic', 'pleophyletic', 'pleopod', 'pleopodite', 'Pleospora', 'Pleosporaceae', 'polynucleolar', 'polynucleosis', 'pompoleon', 'prepaleolithic', 'Protocoleoptera', 'protocoleopteran', 'protocoleopteron', 'protocoleopterous', 'pseudonucleolus', 'roleo', 'scribbleomania', 'simoleon', 'speleological', 'speleologist', 'speleology', 'teleobjective', 'Teleocephali', 'teleocephalous', 'Teleoceras', 'Teleodesmacea', 'teleodesmacean', 'teleodesmaceous', 'teleodont', 'teleologic', 'teleological', 'teleologically', 'teleologism', 'teleologist', 'teleology', 'teleometer', 'teleophobia', 'teleophore', 'teleophyte', 'teleoptile', 'teleorganic', 'teleoroentgenogram', 'teleoroentgenography', 'teleosaur', 'teleosaurian', 'Teleosauridae', 'Teleosaurus', 'teleost', 'teleostean', 'Teleostei', 'teleosteous', 'teleostomate', 'teleostome', 'Teleostomi', 'teleostomian', 'teleostomous', 'teleotemporal', 'teleotrocha', 'teleozoic', 'teleozoon', 'theoteleological', 'theoteleology', 'Thylacoleo', 'tripaleolate', 'wereleopard', 'zoopaleontology']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('leo', w)])  # Find words that contain the string 'leo' and print them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Your Turn:__ The caret symbol `^` matches the start of a string, just like the `$` matches the end. What results do we get with the above example if we leave out both of these, and search for `«..j..t..»`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abjectedness', 'abjection', 'abjective', 'abjectly', 'abjectness', 'adjection', 'adjectional', 'adjectival', 'adjectivally', 'adjective', 'adjectively', 'adjectivism', 'adjectivitis', 'adjustable', 'adjustably', 'adjustage', 'adjustation', 'adjuster', 'adjustive', 'adjustment', 'antejentacular', 'antiprojectivity', 'bijouterie', 'coadjustment', 'cojusticiar', 'conjective', 'conjecturable', 'conjecturably', 'conjectural', 'conjecturalist', 'conjecturality', 'conjecturally', 'conjecture', 'conjecturer', 'coprojector', 'counterobjection', 'dejected', 'dejectedly', 'dejectedness', 'dejectile', 'dejection', 'dejectly', 'dejectory', 'dejecture', 'disjection', 'guanajuatite', 'inadjustability', 'inadjustable', 'injectable', 'injection']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('..j..t..', w)][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The result will be all words of at least eight letters with at least two letters before a __j__, two additional letters, a __t__, and at least two more letters.*\n",
    "\n",
    "*The `?` means the preceding character is optional.  The following code will find all the words with __judg__ or __judge__ in the wordlist:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abjudge', 'adjudge', 'adjudgeable', 'adjudger', 'adjudgment', 'cojudge', 'counterjudging', 'forejudge', 'forejudgment', 'forjudge', 'forjudger', 'interjudgment', 'judge', 'judgeable', 'judgelike', 'judger', 'judgeship', 'judgingly', 'judgmatic', 'judgmatical', 'judgmatically', 'judgment', 'misjudge', 'misjudgement', 'misjudger', 'misjudgingly', 'misjudgment', 'overjudge', 'overjudging', 'overjudgment', 'prejudge', 'prejudgement', 'prejudger', 'prejudgment', 'rejudge', 'stockjudging', 'subjudge', 'unadjudged', 'underjudge', 'unjudgable', 'unjudge', 'unjudged', 'unjudgelike', 'unjudging', 'unprejudged']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('judge?', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranges and Closures\n",
    "\n",
    "*The T9 system of entering text on older mobile phones:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finding words that could have been written with the sequence 4653:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gold', 'golf', 'hold', 'hole']\n"
     ]
    }
   ],
   "source": [
    "# Find words that match the pattern '^[ghi][mno][jkl][def]$'\n",
    "print([w for w in wordlist if re.search('^[ghi][mno][jkl][def]$', w)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ken']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^[jkl][def][mno]$', w)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pik', 'sil']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^[pqrs][ghi][jkl]$', w)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mug', 'nth']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^[mno][tuv][ghi]$', w)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monkey']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^[mno][mno][mno][jkl][def][wxyz]$', w)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rainbow']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^[pqrs][abc][ghi][mno][abc][mno][wxyz]$', w)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Examples of __Kleene closures__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))  # Get a sorted set of unique words from the NPS chat corpus\n",
    "\n",
    "# Find words in 'chat_words' that match the pattern '^m+i+n+e+$' \n",
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', 'hahahahaaa', 'hahahahahaha', 'hahahahahahaha', 'hahahahahahahahahahahahahahahaha', 'hahahhahah', 'hahhahahaha']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in chat_words if re.search('^[ha]+$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A `^` inside square brackets means to exclude everything with these characters.  `<<^[^aeiouAEIOU]+$>>` would match everything without a vowel:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '!!', '!!!', '!!!!', '!!!!!', '!!!!!!', '!!!!!!!', '!!!!!!!!', '!!!!!!!!!', '!!!!!!!!!!', '!!!!!!!!!!!', '!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!.', '!!!!!.', '!!!!....', '!!!.', '!!.', '!!...', '!.', '!...', '!=', '!?', '!??', '!???', '\"', '\"...', '\"?', '\"s', '#', '###', '####', '$', '$$', '$27', '&', '&^', \"'\", \"''\", \"'.\", \"'d\", \"'ll\", \"'m\", \"'n'\", \"'s\", '(', '(((', '((((', '(((((', '((((((', '(((((((', '((((((((', '(((((((((', '((((((((((', '(((((((((((', '((((((((((((', '(((((((((((((', '((((((((((((((', '(((((((((((((((', '(((((((((((((((((', '((((((((((((((((((', '((((((((((((((((((((', '(((((((((((((((((((((', '(((((((((((((((((((((((', '((((((((((((((((((((((((', '(((((((((((((((((((((((((', '((((((((((((((((((((((((((', '(((((..', '(*&(^', '(.', ')', ')))', '))))', ')))))', ')))))))', '))))))))', ')))))))))', '))))))))))', ')))))))))))', '))))))))))))', ')))))))))))))', '))))))))))))))', ')))))))))))))))', ')))))))))))))))))', ')))))))))))))))))))', ')))))))))))))))))))))', '))))))))))))))))))))))', '))))))))))))))))))))))))))))', ')))))))))))))))))))))))))))))))', ')?', '*', '******', '*VBS*', '+', '+*+*+*+*', '++', ',', ',,', ',,,', ',,,,', ',,,,,', ',,,,,,,', ',,,,,,,,,,,', '-', '-(', '--', '-------------', '--------------------', '--------->', '-->', '-...)...-', '-17', '-21', '-6', '-_-', '-s', '.', '. .', '. . .', '. ...', '.(.', '.(..(.vMp3 v1.7.4.).)', '.)', '.).', '..', '.. .', '..(..', '...', '....', '.....', '......', '.......', '........', '.........', '..........', '...........', '............', '.............', '................', '..................', '...................', '....................', '........................', '..............................', '.45', '.:', '.;)', '/', '//', '0', '05.', '06.', '1', '1.98', '1.99', '10', '100', '100%', '1012.', '1016.', '102.6', '10:49', '10th', '11', '12', '12%', '1200', '121.7', '1299', '13', '138', '14', '14-16', '147.7', '15', '16', '16.', '17', '18', '185', '18ST', '19', '1900', '1930', '1980', '1985', '1996', '2', '2.3', '20', '20.', '2006', '20S', '20s', '21', '22', '220', '224', '23', '24', '246', '247', '25', '26', '27', '28', '280', '28147', '29', '29.88.', '295', '29803', '2:55', '2nd', '3', '30', '30.', '30.00.', '300', '31', '32', '33', '3333333', '33982', '34', '35', '36', '360', '37', '38', '39', '39.3', '396', '3:45', '3~<-..4@.', '4', '4.20', '41', '423', '43', '43.', '45', '45.5', '453', '46', '46.', '47', '47.', '49', '4:03', '5', '50', '51', '53', '55', '55%', '55.', '56', '56.', '57', '57401', '579', '59', '59%', '6', '60', '60s', '64.8', '65%', '68%', '69', '6:38', '6:41', '6:51', '6:53', '7', '70%', '700', '73%', '73042', '75', '75%', '76%', '77', '7:45', '8', '80', '8082653953', '818', '85%', '9', '9.53', '90', '92129', '92780', '93', '93%', '95953', '98.5', '98.6', '99', '99701', '99703', '9:10', ':', ':(', ':)', ':):):)', ':-(', ':-)', ':-@', ':.', ':/', ':@', ':D', ':P', ':]', ':p', ':|', ';', '; ..', ';)', ';-(', ';-)', ';0', ';]', ';p', '<', '<,', '<-', '<--', '<---', '<----', '<----------', '<3', \"<3's\", '<33', '<333', '<3333', '<33333', '<333333333', '<3333333333333333', '<33333333333333333', '<<', '<<<', '<<<<', '<<<<,', '<<<<<', '<<<<<<', '<<<<<<,', '<<<<<<<', '<<<<<<<<<<<<<<', '<~~~', '=', \"='s\", '=(', '=)', '=-\\\\', '=/', '=D', '=[', '=]', '=p', '>', '>.>', '>.>->', '>:->', '>>>', '>>>>>>>>>>', '>>>>>>>>>>>', '>>>>>>>>>>>>', '>?', '>_>', '?', '?!', '?!?!', '?!?!?', \"?'\", '?.', '?..', '?....', '??', '??!!', '??!?!??!', '???', '????', '?????', '??????', '???????', '????????', '?????????', '??@', '@', '@$$', \"@-,'~\", \"@..3-,'~.\", 'B', 'C', 'CDT', 'CST', 'CT', 'Cry', 'Ct', 'Ctrl', 'D', 'DJ', 'DVD', 'Dr', 'Dr.', 'F', 'F5', 'FF', 'FL', 'G', 'GN', 'GNG', 'GrlZ', 'Gs', 'H', 'H0rny', 'Hmm', 'JRZ', 'K', 'LPN', 'M', 'MD', 'MP3', 'MSN', 'MY', 'Mmm', 'Mp3', 'Ms', 'My', 'N', 'N\"T', \"N'T\", 'NC', 'NTMN', 'NY', 'P', 'P.', 'PDT', 'PM', \"PM's\", 'PMSL', 'PMs', 'PS', 'PST', 'Plssss', 'PmS', 'QQ', 'R', \"RN's\", 'S', 'S.M.R.', 'S3x0r', 'St', 'TC', 'TX', 'TY', 'TYPR', 'Ty', 'W', 'WHY', 'WTF', 'Wb', 'Why', 'Wtf', 'X', 'XXXXXXXXXX', '[[[[[[[[[[[[[[[[[[', '\\\\', '\\\\ty', ']:)', ']]]]]]]]]]]]]]]]]]]]]', '^', '^^', '^^^', '^_^', '_', '`', 'b', 'b/c', 'b4', 'bbl', 'bbs', 'bc', 'bf', 'bj', 'brb', 'brbbb', 'brrrrrrr', 'brwn', 'btw', 'by', 'byb', 'c', \"c'm\", 'chp', 'ck', 'cpr', 'cry', 'cyb3r', 'd', 'd=', 'dd', 'dj', 'dl', 'dr', 'dry', 'dsklgjsdk', 'f', 'f.', 'fck', 'fl', 'fly', 'frm', 'frst', 'ft', 'ft.', 'fwd', \"g'\", 'gf', 'gm', 'gn', 'grrl', 'grrr', 'grrrrrrrr', 'grrrrrrrrr', 'grrrrrrrrrrrrrrrrr', 'gtg', 'h', 'h.s', 'hb', 'hfglhs', 'hgfhgfjgf', 'hm', 'hmm', 'hmmm', 'hmmmm', 'hmmmmm', 'hmmmmmmm', 'hmmmmmmmm', 'hmmmmmmmmmm', 'hmph', 'hr', 'hrs', 'http', 'hx', 'hyy', 'j', 'j/k', 'j/p', 'jk', 'jr', 'jw', 'k', \"k's\", 'kc', 'kmph', 'knw', 'kts', 'ky', 'l', 'lb', 'lbs', 'ldskdlsf', 'll', 'ltnc', 'ltns', 'ltr', 'm', 'md', 'mhm', 'mm', 'mmhmm', 'mmm', 'mmmm', 'mmmmk', 'mmmmm', 'mmmmmm', 'mmmmmmmmmm', 'mmmmmmmmmmmmm', 'mmmmmmmmmmmmmm', 'mp3', 'ms', 'ms.', 'msg', 'msn', 'my', 'n', 'n\"t', \"n't\", 'n.n', 'n/', 'n;t', 'nbc', 'nc', 'nd', 'nj', 'nm', 'ny', 'nyc', 'nz', 'p', 'pc', 'pffft', 'pfft', 'plz', 'pm', \"pm'n\", \"pm's\", 'pms', 'pmsl', 'pp', 'ppl', 'pr', 'prrty', 'ps2', 'psh', 'pssssh', 'pssst', 'pvt', 'pwns', 'px', 'r', 's', \"s'\", 'sdlfkjsj', 'sf', 'shhhh', 'sky', 'sldfjlsdf', 'slkfjsldkfjs', 'sp', 'sry', 'st', 'sw', 'syck', 't', 't/c', 't/y', 'tc', 'td', 'tdr', 'thnx', 'thx', 'tks', 'try', 'tv', 'tx', 'ty', 'tyvm', 'vm', 'vs.', 'w', 'w/', 'w/b', 'wb', 'wc', 'why', 'whys', 'wtf', 'wth', 'wv', 'wz', 'x', 'xD', 'xxxxxx', 'y', \"y'\", 'y/w', 'yr', 'yrs', 'ysssssssss', 'yvw', 'yw', \"yw's\", 'zzzzzzzz', '~', '~!']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in chat_words if re.search('^[^aeiouAEIOU]+$', w)])  # Print words that contain no vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5', '0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99', '1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', '1.20', '1.24', '1.25', '1.26', '1.28', '1.35', '1.39', '1.4', '1.457', '1.46', '1.49', '1.5', '1.50', '1.55', '1.56', '1.5755', '1.5805', '1.6', '1.61', '1.637', '1.64']\n"
     ]
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))  # Get a sorted set of unique words from the WSJ (Wall Street Journal) corpus\n",
    "\n",
    "# Find words that match the pattern of decimal numbers and print the first 50\n",
    "print([w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)][:50])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]  # Find words that are uppercase and end with a dollar sign ($)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`\\` is used as an escape character.  Normally, `.` is a wildcard character and `$` is used to designate the end of a string.  Here we'd like to search for strings composed of numbers with a decimal and for alphabetic strings with a dollar sign, so we need to escape the characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', '1934', '1948', '1953', '1955', '1956', '1961', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1975', '1976', '1977', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2005', '2009', '2017', '2019', '2029', '3057', '8300']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wsj if re.search('^[0-9]{4}$', w)])  # Find and print words that consist of exactly four digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', '14-hour', '15-day', '150-point', '190-point', '20-point', '20-stock', '21-month', '237-seat', '240-page', '27-year', '30-day', '30-point', '30-share', '30-year', '300-day', '36-day', '36-store', '42-year', '50-state', '500-stock', '52-week', '69-point', '84-month', '87-store', '90-day']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)])  # Find and print words that match a specific pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "`^[0-9]+`: This matches one or more digits at the beginning of the word.\n",
    "\n",
    "`-`: Matches a literal hyphen (-).\n",
    "\n",
    "`[a-z]{3,5}`: Matches between 3 and 5 lowercase letters.\n",
    "\n",
    "`$`: Matches the end of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]  # Find words that match the given pattern in the WSJ corpus\n",
    "\n",
    "#  ^[a-z]{5,}: Matches a word that starts with at least 5 lowercase letters.\n",
    "# -[a-z]{2,3}: The word must then have a hyphen followed by 2 or 3 lowercase letters.\n",
    "#  -[a-z]{,6}: After another hyphen, the word must end with up to 6 lowercase letters (can be 0 to 6 characters).\n",
    "#           $: The end of the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`{ }` are used to restrict the size of the returned hits. `{n}` means only those strings of length $n$.  `{n,}` means those strings at least as long as $n$.  `{,n}` means those strings no longer than $n$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `|` means to match either of these strings.  When used with parentheses, it can be combined with other operators.  E.g., in the example above `'(ed|ing)$'` returns strings that end with either `ed` or `ing`.  But without the parenthesis, the `$` operator is attached only to `ing`, so the code below will find strings that have `ed` anywhere within their string, or `ing` at their end.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', 'Alfred', 'Allied', 'Annualized', 'Anything', 'Arbitrage-related', 'Arbitraging', 'Asked', 'Assuming', 'Atlanta-based', 'Baking', 'Banking', 'Beginning', 'Beijing', 'Being', 'Bermuda-based', 'Betting', 'Boeing', 'Broadcasting', 'Bucking', 'Buying', 'Calif.-based', 'Change-ringing', 'Citing', 'Concerned', 'Confronted', 'Conn.based', 'Consolidated', 'Continued', 'Continuing', 'Declining', 'Defending', 'Depending', 'Designated', 'Determining', 'Developed', 'Died', 'During', 'Encouraged', 'Encouraging', 'English-speaking', 'Estimated', 'Everything', 'Excluding', 'Exxon-owned']\n"
     ]
    }
   ],
   "source": [
    "# Find words that end with 'ed' or 'ing' and print the first 50\n",
    "print([w for w in wsj if re.search('(ed|ing)$', w)][:50])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', 'Alfred', 'Allied', 'Annualized', 'Anything', 'Arbitrage-related', 'Arbitraging', 'Asked', 'Assuming', 'Atlanta-based', 'Baking', 'Banking', 'Beginning', 'Beijing', 'Being', 'Bermuda-based', 'Betting', 'Biedermann', 'Boeing', 'Breeden', 'Broadcasting', 'Bucking', 'Buying', 'Calif.-based', 'Cathedral', 'Cedric', 'Change-ringing', 'Citing', 'Concerned', 'Confederation', 'Confronted', 'Conn.based', 'Consolidated', 'Continued', 'Continuing', 'Credit', 'Declining', 'Defending', 'Depending', 'Designated', 'Determining', 'Developed', 'Died', 'During', 'Encouraged']\n"
     ]
    }
   ],
   "source": [
    "# Find words that contain 'ed' or end with 'ing' and print the first 50\n",
    "print([w for w in wsj if re.search('ed|ing$', w)][:50])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operator        | Behavior                                                 |\n",
    "|:-----------|:--------------------------------------------------------------------------------|\n",
    "| `.`        | Wildcard, matches any character                                                 |\n",
    "| `^abc`     | Matches some pattern $abc$ at the start of a string                               |\n",
    "| `abc$`     | Matches some pattern $abc$ at the end of a string                                 |\n",
    "| `[abc]`    | Matches one of a set of characters                                              |\n",
    "| `[A-Z0-9]` | Matches one of a range of characters                                            |\n",
    "| `ed|ing|s` | Matches one of the specified strings (disjunction)                              |\n",
    "| `*`        | Zero or more of previous item, e.g. `a*`, `[a-z]*` (also known   as $Kleene Closure$) |\n",
    "| `+`        | One or more of previous item, e.g. `a+`, `[a-z]+`                                   |\n",
    "| `?`        | Zero or one of the previous item (i.e. optional),   e.g. `a?`, `[a-z]?`             |\n",
    "| `{n}`      | Exactly $n$ repeats where $n$ is a non-negative integer                             |\n",
    "| `{n,}`     | At least $n$ repeats                                                              |\n",
    "| `{,n}`     | No more than $n$ repeats                                                          |\n",
    "| `{m,n}`    | At least $m$ and no more than $n$ repeats                                           |\n",
    "| `a(b|c)+`  | Parentheses that indicate the scope of the operators                            |\n",
    "|            |                                                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5   Useful Applications of Regular Expressions\n",
    "\n",
    "*Using `re.findall()` to find and count the vowels in a word:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'  # Define the word\n",
    "print(re.findall(r'[aeiou]', word))          # Find and print all vowels in the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(r'[aeiou]', word))  # Count the number of vowels in the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we only want the number of hits, we can also use this:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for r in re.findall(r'[aeiou]', word)])  # Count the number of vowels by summing 1 for each vowel found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sequences of two or more vowels and their relative frequencies:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253), ('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95)]\n"
     ]
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))  # Get a sorted set of unique words from the WSJ corpus\n",
    "\n",
    "# Create a frequency distribution of vowel sequences that appear two or more times in each word\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "                   for vs in re.findall(r'[aeiou]{2,}', word))  # Find vowel sequences of two or more vowels\n",
    "\n",
    "# Print the 12 most common vowel sequences found\n",
    "print(fd.most_common(12))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Your Turn:__ In the W3C Date Time Format, dates are represented like this: 2009-12-31. Replace the `?` in the following Python code with a regular expression, in order to convert the string `'2009-12-31'` to a list of integers `[2009, 12, 31]`:\n",
    "\n",
    "`[int(n) for n in re.findall(?, '2009-12-31')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2009, 12, 31]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract sequences of two or more digits and convert them to integers\n",
    "[int(n) for n in re.findall(r'[0-9]{2,}', '2009-12-31')]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Doing More with Word Pieces\n",
    "\n",
    "*Removing internal vowels from a text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Declaration of Human Rights Preamble Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom , justice and peace in the world , Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind , and the advent of a world in which human beings shall enjoy freedom of speech and\n"
     ]
    }
   ],
   "source": [
    "# Original Text:\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')  # Load the UDHR corpus in English\n",
    "\n",
    "text = ' '.join(english_udhr[:75])          # Join the first 75 words into a single string\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[^AEIOUaeiou]|[AEIOUaeiou]+$'  # Retaining any initial or final vowel sequences, throw away the others\n",
    "\n",
    "def compres(word):\n",
    "    pieces = re.findall(regexp, word)   # Find all matches in the word based on the regex\n",
    "    return ''.join(pieces)              # Join the matches into a compressed string\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')  # Load the UDHR corpus in English\n",
    "\n",
    "# Compress each word in the first 75 words of the corpus and print them in a wrapped format\n",
    "print(nltk.tokenwrap(compres(w) for w in english_udhr[:75]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s combine regular expressions with conditional frequency distributions. Here\n",
    "we will extract all consonant-vowel sequences from the words of Rotokas, such as ka\n",
    "and si. Since each of these is a pair, it can be used to initialize a conditional frequency\n",
    "distribution. We then tabulate the frequency of each pair:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finding frequencies of consonant-vowel sequences from the words of Rotokas:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')  # Load the Rotokas dictionary from the NLTK toolbox corpus\n",
    "\n",
    "# Extract consonant-vowel (CV) pairs from the words in the Rotokas dictionary\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "\n",
    "# Create a Conditional Frequency Distribution of the consonant-vowel pairs\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "\n",
    "# Tabulate the frequencies of the consonant-vowel pairs\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program processes each word w in turn, and for each one, finds every substring\n",
    "that matches the regular expression «[ptksvr][aeiou]»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Making an index of all possible consonant-vowel pairs in the language:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract consonant-vowel (CV) pairs from each word in the Rotokas dictionary and store them with their corresponding words\n",
    "cv_word_pairs = [(cv, w) for w in rotokas_words\n",
    "                         for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "\n",
    "# Create an index mapping each CV pair to the words that contain them\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "\n",
    "# Retrieve and print the words that contain the consonant-vowel pair 'su'\n",
    "cv_index['su']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', 'kapokapora', 'kapokaporo', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'karokaropo', 'karopo', 'kepo', 'kepoi', 'keposi', 'kepoto']\n"
     ]
    }
   ],
   "source": [
    "print(cv_index['po'])  # all words containing 'po'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_index['po']) # number of words containing 'po'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kakate', 'kakupute', 'kerete', 'kerete', 'kokoote', 'kokote', 'koriteira', 'kukue pute']\n"
     ]
    }
   ],
   "source": [
    "print(cv_index['te'])  # all words containing 'te'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_index['te']) # number of words containing 'te'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding Word Stems\n",
    "\n",
    "*A simple approach:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    # Loop through the list of suffixes to remove them from the word if they are found\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        \n",
    "        if word.endswith(suffix):       # Check if the word ends with the current suffix\n",
    "            return word[:-len(suffix)]  # Remove the suffix by slicing the word up to its length\n",
    "            \n",
    "    return word                         # Return the original word if no suffix is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A step-by-step outline of how we could use RegExp:*\n",
    "\n",
    "*Here's a disjunction of all the suffixes:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This just returns the suffix because the `( )` also selects which substring to return.  If we want the parentheses to specify the scope of the disjunction, but not select the material to be output, we have to add `?:`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since we want this split, we have to parenthesize both parts of the regular expression:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try it with a different form of the word:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Star operators are by default greedy and will try to 'consume' as much of the input as possible.  We can turn this off with `*?`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can allow for empty strings by adding `?`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Encasing this in a function:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    # Define a regular expression to capture the stem and optional suffix\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    \n",
    "    # Use re.findall() to apply the regex and extract the stem and suffix from the word\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    \n",
    "    # Return the stem of the word (without the suffix)\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)  # Tokenize the raw text into individual words\n",
    "\n",
    "# Apply the stem function (from your earlier code) to each token in the list\n",
    "print([stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Searching Tokenized Text\n",
    "\n",
    "*`<a> <man>` will find all instances of __a man__ in a text.  The angle brackets are used to mark token boundaries, and white space is ignored (this only applies to NLTK `findall()` method for texts.  Here, we'll look for all occurrences of \"a _ _ _ man\" in \"Moby Dick\".  The parentheses within the search term limits the returned string, so only the word between \"a\" and \"man\" is returned:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))  # Load the text of 'Moby Dick' as an NLTK Text object\n",
    "\n",
    "moby.findall(r\"<a> (<.*>) <man>\")  # Find all patterns in the form of \"<a> (something) <man>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we'll look at the chat corpus and find three-word phrases ending with \"bro\":*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())  # Load the words from the NPS Chat Corpus as an NLTK Text object\n",
    "\n",
    "moby.findall(r\"<.*><.*><bro>\")      # Find all sequences where two words are followed by \"bro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomorrow ya man; Aw , man; my eye man; a bisexual man; ever sleep man;\n",
      "bored ? man; of a man; OOH DAamn man; a real man; thats cold man; JOIN\n",
      "oh man; U26 U26 man; the U26 man; of macho man; if a man; good lookin\n",
      "man; with a man; U53 :-) man; JOIN ...... man; JOIN oh man; single /\n",
      "man; on heat man; um um man; U69 smart man; the egg man; you a man;\n",
      "Wtf , man; the comon man; the single man; mmmmmm U18 man; live there\n",
      "man; lol oh man; marry a man\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<.*><.*><man>\")      # Find all sequences where two words are followed by \"man\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This finds sequences of three or more words starting with __l__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")  # Find all sequences where a word starting with 'l' appears three or more times in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little lower layer; little lower layer; lances lie levelled; long\n",
      "lance lightly; like live legs\n"
     ]
    }
   ],
   "source": [
    "moby.findall(r\"<l.*>{3,}\")  # Find all sequences where a word starting with 'l' appears three or more times in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn:__ Consolidate your understanding of regular expression patterns and substitutions using `nltk.re_show(p, s)` which annotates the string `s` to show every place where pattern `p` was matched, and `nltk.app.nemo()` which provides a graphical interface for exploring regular expressions. For more practice, try some of the exercises on regular expressions at the end of this chapter.\n",
    "\n",
    "*For `nltk.re_show(p, s)`, `p` cannot be a regular expression (a fact which could have been made clearer...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you come to a {fork in the road, tak}e it.\n"
     ]
    }
   ],
   "source": [
    "yogi = \"When you come to a fork in the road, take it.\"  # Define the string containing the sentence by Yogi Berra\n",
    "\n",
    "nltk.re_show(r\"f.*k\", yogi)  # The regex pattern matches any word starting with \"f\", followed by any characters, and ending with \"k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you come to a {fork} in the road, take it.\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(\"fork\", yogi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you come to a {fork in the road, tak}e it.\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(\"f.*k\", yogi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`nltk.app.nemo()` can be used in jupyter notebooks, but it opens an external window which will prevent the other cells in the notebook from running until it's closed. Therefore, I'm not going to call it in this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use RegExs to search for linguistic phenomena.  E.g., we can find hypernyms by looking for __x and other ys__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Load words from the 'hobbies' and 'learned' categories of the Brown Corpus\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "\n",
    "# Find all sequences that match the pattern <word> <and> <other> <plural word>\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities\n",
      "water and other liquids\n"
     ]
    }
   ],
   "source": [
    "# Join the words from 'hobbies' and 'learned' categories to form a string\n",
    "hobbies_learned_text = ' '.join(brown.words(categories=['hobbies', 'learned']))\n",
    "\n",
    "# Use re.findall to extract patterns in the form \"<word> and other <plural word>\"\n",
    "matches = re.findall(r\"\\b\\w+\\b and other \\b\\w+s\\b\", hobbies_learned_text)\n",
    "\n",
    "print(matches[0])\n",
    "print(matches[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But this does result in false positives (e.g., \"demands and other factors\"), and can also result in false negitives.*\n",
    "\n",
    "__Your Turn__: Look for instances of the pattern *as x as y* to discover information about entities and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as soon as possible; as well as the; as soon as possible; as long as\n",
      "they; as well as representatives; as far as instructed; as long as\n",
      "there; as late as the; as well as a; as much as we; as many as 25; as\n",
      "many as 25; as thoroughly as the; as well as the; as well as his; as\n",
      "great as Mankowski; as long as their; as soon as possible; as soon as\n",
      "the; as early as the; as soon as trading; as well as its; as abrupt as\n",
      "in; as severe as in; as well as to; as carefully as she; as well as\n",
      "golden; as harmless as a; as far as a; as well as a; as good as those;\n",
      "as automatically as it; as well as the; as long as one; as many as\n",
      "six; as good as Hamilton; as large as Western; as hard as I; as much\n",
      "as possible; as early as 1950; as well as bound; as many as a; as much\n",
      "as the; as innocent as it; as well as wit; as much as my; as well as\n",
      "wit; as funny as Ed; as fast as we\n"
     ]
    }
   ],
   "source": [
    "brown_sample = nltk.Text(brown.words(categories=['humor', 'news']))  # Load the 'humor' and 'news' categories from the Brown corpus as a Text object.\n",
    "\n",
    "brown_sample.findall(r\"<as> <\\w*> <as> <\\w*>\")  # Find patterns of the form \"as <word> as <word>\" in the Brown corpus text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A number of idioms tend to dominate the results (i.e., \"as soon as possible\", \"as well as ...\"), so let's remove theses.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as long as they; as far as instructed; as long as there; as late as\n",
      "the; as much as we; as many as 25; as many as 25; as great as\n",
      "Mankowski; as long as their; as abrupt as in; as severe as in; as far\n",
      "as a; as good as those; as long as one; as many as six; as good as\n",
      "Hamilton; as large as Western; as hard as I; as much as possible; as\n",
      "many as a; as much as the; as innocent as it; as much as my; as funny\n",
      "as Ed; as fast as we\n"
     ]
    }
   ],
   "source": [
    "brown_sample = nltk.Text(brown.words(categories=['humor', 'news']))  # Load the 'humor' and 'news' categories from the Brown corpus as a Text object.\n",
    "\n",
    "brown_sample.findall(r\"<as> <\\w*[^well][^soon]> <as> <\\w*>\")  # Find patterns matching \"as <word> as <word>\" excluding \"well\" and \"soon\" in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We could also make the argument that most uses of \"as long as ...\" are also idiomatic and could be eliminated.  But this would lead to false negatives.*  \n",
    "\n",
    "*I would also say that these results might be more useful if we could also see more of the phrase from which these results were taken:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as long as they can; as far as instructed so; as long as there is; as\n",
      "late as the top; as much as we can; as many as 25 home; as many as 25\n",
      "bases; as great as Mankowski did; as long as their names; as abrupt as\n",
      "in 1958; as severe as in late; as far as a black; as good as those\n",
      "elsewhere; as long as one pretends; as many as six strokes; as good as\n",
      "Hamilton Holmes; as large as Western Europe; as hard as I could; as\n",
      "many as a thousand; as much as the ambiguous; as innocent as it looks;\n",
      "as much as my husband; as funny as Ed Wynn; as fast as we replenished\n"
     ]
    }
   ],
   "source": [
    "brown_sample = nltk.Text(brown.words(categories=['humor', 'news']))  # 'humor' and 'news' categories of the Brown corpus\n",
    "\n",
    "brown_sample.findall(r\"<as> <\\w*[^well][^soon]> <as> <\\w*>{2}\")  # Find \"as <word> as <word>\" but excluding words containing 'well' and 'soon'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6   Normalizing Text\n",
    "\n",
    "##### Stemmers\n",
    "\n",
    "The Porter and Lancaster stemmers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government. Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)               # Tokenize the raw text into individual words (tokens)\n",
    "porter = nltk.PorterStemmer()             # Initialize the Porter Stemmer (used for stemming words)\n",
    "lancaster = nltk.LancasterStemmer()       # Initialize the Lancaster Stemmer (another stemming algorithm)\n",
    "print([porter.stem(t) for t in tokens])   # Print the stemmed version of each token using the Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens])  # Apply Lancaster stemming to each token and print the resulting stemmed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Texts can be indexed using a stemmer, and from there concordances can be made:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):                               # Define a class 'IndexedText'\n",
    "    \n",
    "    def __init__(self, stemmer, text):                   # Constructor method to initialize the object with stemmer and text\n",
    "        self._text = text                                # Store the original text\n",
    "        self._stemmer = stemmer                          # Store the provided stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)   # Create an index of stemmed words and their positions\n",
    "                                 for (i, word) in enumerate(text))\n",
    "        \n",
    "    def concordance(self, word, width=40):               # Method to find concordance of a word in the text\n",
    "        key = self._stem(word)                           # Stem the word to match with the index\n",
    "        wc = int(width / 4)                              # Define the width for the context window\n",
    "        for i in self._index[key]:                       # Iterate through the indexed positions of the stemmed word\n",
    "            lcontext = ' '.join(self._text[i - wc : i])  # Get left context of the word\n",
    "            rcontext = ' '.join(self._text[i : i + wc])  # Get right context of the word\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)  # Format the left context display\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)    # Format the right context display\n",
    "            print(ldisplay, rdisplay)                    # Print the left and right context around the word\n",
    "            \n",
    "    def _stem(self, word):                               # Private method to stem the word\n",
    "        return self._stemmer.stem(word).lower()          # Stem the word and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "grail = nltk.corpus.webtext.words('grail.txt')  # Load the words from 'grail.txt' in the webtext corpus.\n",
    "text = IndexedText(porter, grail)               # Create an IndexedText object with Porter stemmer and the 'grail.txt' text.\n",
    "text.concordance('lie')                         # Find and display the concordance for the word 'lie' (stemmed using the Porter stemmer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "\n",
    "*The WordNet lemmatizer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()                   # Initialize the WordNet Lemmatizer\n",
    "\n",
    "print([wnl.lemmatize(t) for t in tokens])        # Lemmatize each token in the list 'tokens' and print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7   Regular Expressions for Tokenizing Text\n",
    "\n",
    "##### Simple Approaches to Tokenization\n",
    "\n",
    "*Just splitting on whitespace with the string method `.split`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "\n",
    "print(raw.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The regular expression is similar, but we need to add code so that it handles tabs and whitespaces:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r' ', raw))  # Split the 'raw' text into a list of tokens, using a space (' ') as the delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'[ \\t\\n]+', raw))  # Split the 'raw' text into tokens using spaces, tabs, or newlines as delimiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can do this more easily with the built-in `re` abbreviation `\\s`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'\\s+', raw))  # Split the 'raw' text into tokens using any whitespace character as a delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remember that when regular expressions are prefixed with `r`, the Python interpreter will treat the string literally.*\n",
    "\n",
    "*`\\w` is the character class (equivalent to `[a-zA-Z0-9_]`), and the complement to this is `\\W` (which seems very counter intuitive to me...).  We could use `\\W` to split a text on anything but a word character:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'\\W+', raw))  # Split the 'raw' text into tokens using any non-word characters as delimiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But this leaves us with empty strings at the beginning and end.  We could avoid this by using `re.findall(r'\\w+', raw)`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+', raw))  # Find all sequences of word characters (letters, digits, underscores) in 'raw' text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A more complicated RegEx will first try to match any sequence of word characters; if it can't find a match, it will use `\\S` to find non-whitespace charcters.  In this way, punctuation is grouped with following letters, but sequences of punctuation will be separated:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+|\\S\\w*', raw))  # Find all sequences of word characters or non-whitespace sequences in 'raw'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we want to include internal hyphens and apostrophes, we could use this: `re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)`.  `<<\\w+(?:[-']\\w+)*>>` means `\\w+` followed by zero or more instances of `[-']\\w+`, which matches strings like __hot-tempered__ and __it's__.  We have to include `?:` to return the entire string.  We also need to separate quote chatacters `|'|`, and `<<[-.(]+>>` will allow double hyphens, ellipses, etc... to be tokenized separately:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))  # Match complex word patterns, hyphenated words, contractions, punctuation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Regular Expression Symbols\n",
    "\n",
    "\n",
    "|Symbol| Function                                 |\n",
    "|:-----|:-------------------------------------------------------------|\n",
    "| `\\b` | Word boundary (zero width)                                   |\n",
    "| `\\d` | Any decimal digit (equivalent to `[0-9]`)                     |\n",
    "| `\\D` | Any non-digit character (equivalent to `[^0-9]`)               |\n",
    "| `\\s` | Any whitespace character (equivalent to `[ \\t\\n\\r\\f\\v]`)       |\n",
    "| `\\S` | Any non-whitespace character (equivalent to `[^ \\t\\n\\r\\f\\v]`)  |\n",
    "| `\\w` | Any alphanumeric character (equivalent to `[a-zA-Z0-9_]`)      |\n",
    "| `\\W` | Any non-alphanumeric character (equivalent to `[^a-zA-Z0-9_]`) |\n",
    "| `\\t` | The tab character                                            |\n",
    "| `\\n` | The newline character                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK's Regular Expression Tokenizer\n",
    "\n",
    "*`nltk.regexp_tokenizer()` is similar to `re.findall()`, but is more efficient, and avoids the need for special treatment of parentheses. The `(?x)` flag tells Python to strip out the embedded whitespace and comments:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "    (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "  | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "  | \\.\\.\\.             # ellipsis\n",
    "  | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "  '''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we use the verbose flag, we can't use `' '` to match a space character - we have to use `\\s`.  Also, `regexp_tokenizer()` has an optional `gaps` parameter that specifies the gaps between tokens:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can evaluate a tokenizer by seeing how many of the resulting tokens are not in a wordlist:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '...',\n",
       " 'hot-tempered',\n",
       " \"i'm\",\n",
       " \"it's\",\n",
       " 'makes',\n",
       " \"won't\"}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w.lower() for w in re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)]  # Tokenize 'raw' using regex, convert to lowercase\n",
    "set(tokens).difference(wordlist)                                                  # Find words in 'tokens' that are not in 'wordlist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further Issues with Tokenization\n",
    "\n",
    "### 3.8 Sementation\n",
    "\n",
    "##### Sentence Segmentation\n",
    "\n",
    "*Average number of word per sentence in the Brown Corpus:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average number of words per sentence in the Brown corpus\n",
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of the Punkt sentence segmeneter:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')  # Load raw text from Chesterton's 'The Man Who Was Thursday'\n",
    "sents = nltk.sent_tokenize(text)                             # Tokenize the text into sentences using NLTK's sentence tokenizer\n",
    "pprint.pprint(sents[79:89])                                  # Print sentences 79 to 89 using pretty printing (for better formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Segmentation\n",
    "\n",
    "*Example of a joined text and a simple segmenter:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []                                # Initialize an empty list to hold the segmented words\n",
    "    last = 0                                  # Variable to track the start of the next word segment\n",
    "    for i in range(len(segs)):                # Loop through the segmentation string\n",
    "        if segs[i] == '1':                    # If the segmentation character is '1', it indicates a split\n",
    "            words.append(text[last : i + 1])  # Append the substring from 'last' to 'i + 1' to the word list\n",
    "            last = i + 1                      # Update 'last' to start after the current split\n",
    "    words.append(text[last:])                 # Append any remaining characters after the final segment\n",
    "    return words                              # Return the list of segmented words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n"
     ]
    }
   ],
   "source": [
    "print(segment(text, seg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I've always found the explanation of the following to be a bit lacking.  The solution involves search, which is technically a machine learning topic.  Dealing with machine learning techniques just a couple chapters after covering Python basics such as printing strings and indexing is an incredible acceleration in the level of difficulty.  Suffice to say, the following section presents some functions that are __muuuuuuuuuuch__ too difficult for this part of the book.*\n",
    "\n",
    "*The __TLDR__ version of the topic is this: it's possible to create an objective function and that will score a segmentation of a text.  The segmentation divides the string into a lexicon and a derivation that uses the words in this lexicon.  The smaller the lengths of the lexical items and the derivations, the better the score.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The objective function looks quite straightforward:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)                               # Segment the text using the provided segmentation\n",
    "    text_size = len(words)                                    # Calculate the size of the text based on the number of words\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))  # Calculate the size of the lexicon (unique words) plus one for space\n",
    "    return text_size + lexicon_size                           # Return the total of the text size and lexicon size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We'll use the same text and segmentations as earlier, and add one more:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like', 'thekitt', 'y', 'like', 'thedogg', 'y']\n"
     ]
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
    "print(segment(text, seg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The fact that the second segmentation - which represents the correct segmentation in natural English - has a lower score than the third - which segments \"thekitty\" and \"thedoggy\" respectively into (\"thekitt\", \"y\") and (\"thedogg\", \"y\") - is to me concerning.  Either this heuristic or the example are not ideal.  It's possible with a longer text, the heuristic could isolate \"the\". I feel the example would have been that much easier to follow if the authors had picked a sample that did this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next, we have to search for the optimal string segmentation.  The first function is simple enough: it just flips a binary digit at a given position;*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(segs, pos):                                              # Define function with two parameters: 'segs' (string) and 'pos' (position)\n",
    "    return segs[:pos] + str(1 - int(segs[pos])) + segs[pos + 1:]  # Concatenate: part before 'pos', flipped bit, and part after 'pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11011'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg = \"11111\"\n",
    "flip(seg, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The second function flips n digits at random positions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint    \n",
    "\n",
    "def flip_n(segs, n):                                  # Define function with 'segs' (string) and 'n' (number of flips)\n",
    "    for i in range(n):                                # Loop 'n' times\n",
    "        segs = flip(segs, randint(0, len(segs) - 1))  # Flip a random bit in 'segs' using a random position\n",
    "    return segs                                       # Return the modified 'segs' after 'n' flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11100110111'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg = \"11111111111\"\n",
    "flip_n(seg, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It will almost certainly give different results every time it's called:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11111011111'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg = \"11111111111\"\n",
    "flip_n(seg, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11001111110'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg = \"11111111111\"\n",
    "flip_n(seg, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The third part is the clusterf\\*^k.  The number of possible segmentation strings in $2^n$, where $n$ is the length of the text.  Since our text is 56 characters long, the number of possible segmentations would be 72,057,594,037,927,936.  To evaluate all of those would take years on most machines.*\n",
    "\n",
    "*So the function below uses a technique called \"annealing\" (there's a decent wiki [here](https://en.wikipedia.org/wiki/Simulated_annealing \"simulated annealing\")).  Another __TLDR__ explanation:  instead of trying all 72,057,594,037,927,936 possibilities, we'll try a small subset - here, 5,000.  Then we'll take the best result of that subset, and use that for the basis of the next 5,000 trials.  At each step, the number of digits in our segmentation that is randomly flipped will become smaller and smaller, and as a result we'll slowly converge on a \"good\" estimate. (I say \"good\", because there's no guarantee we'll find the optimal solution).  The reference to \"annealing\" is because of our use of a \"cooling rate\" - a parameter that determines the speed at which we converge to our solution.  If the cooling rate is too high, we'll converge too quickly and the solution may not be very good; but if the rates too low, the convergence will be very slow.*\n",
    "\n",
    "*And yes, if you're wondering, I'm in no way qualified to be instructing anyone about machine learning.  But take a look for yourself at the code, and you'll see that this is what basically happens:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anneal(text, segs, iterations, cooling_rate):                # Define function with 'text', 'segs', 'iterations', and 'cooling_rate'\n",
    "    temperature = float(len(segs))                               # Initialize 'temperature' as the length of 'segs'\n",
    "    while temperature > 0.5:                                     # Continue while 'temperature' is greater than 0.5\n",
    "        best_segs, best = segs, evaluate(text, segs)             # Set current 'segs' and evaluation as the best\n",
    "        for i in range(iterations):                              # Loop through the number of iterations\n",
    "            guess = flip_n(segs, round(temperature))             # Make a new guess by flipping random bits in 'segs'\n",
    "            score = evaluate(text, guess)                        # Evaluate the new guess\n",
    "            if score < best:                                     # If the new score is better, update the best score and segments\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs                            # Set 'segs' and 'score' to the best found\n",
    "        temperature = temperature / cooling_rate                 # Reduce the temperature by the cooling rate\n",
    "        print(evaluate(text, segs), segment(text, segs))         # Print the evaluation and segmentation of 'text'\n",
    "    print()                                                      # Print a blank line at the end\n",
    "    return segs                                                  # Return the final segment structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "61 ['doyo', 'usee', 'thekitty', 'seethedoggydoyoul', 'ike', 'thekitty', 'likethedoggy']\n",
      "61 ['doyo', 'usee', 'thekitty', 'seethedoggydoyoul', 'ike', 'thekitty', 'likethedoggy']\n",
      "59 ['doyousee', 'thekitty', 'seet', 'hedoggydoyoulike', 'thekitty', 'likethedoggy']\n",
      "59 ['doyousee', 'thekitty', 'seet', 'hedoggydoyoulike', 'thekitty', 'likethedoggy']\n",
      "59 ['doyousee', 'thekitty', 'seet', 'hedoggydoyoulike', 'thekitty', 'likethedoggy']\n",
      "57 ['doyousee', 'thekitty', 'seethedoggydoyoulike', 'thekitty', 'likethedoggy']\n",
      "56 ['doyousee', 'thekitty', 'seethedoggydoyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "51 ['doyousee', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100000001001000000010000100010000000100010000000'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Because we're using random elements, the end result will likely be somewhat different every time.  Therefore, there's no guarantee this algorithm will find the optimal solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "60 ['doyou', 'se', 'e', 'thekitty', 'se', 'ethedoggy', 'do', 'you', 'l', 'ike', 'thekitty', 'lik', 'ethedoggy']\n",
      "58 ['do', 'you', 's', 'e', 'e', 'thekitty', 'se', 'ethedoggy', 'do', 'you', 'l', 'ik', 'e', 'thekitty', 'lik', 'ethedoggy']\n",
      "55 ['doyou', 'se', 'e', 'thek', 'itty', 'se', 'ethedoggy', 'doyou', 'l', 'ik', 'e', 'thek', 'itty', 'lik', 'ethedoggy']\n",
      "55 ['doyou', 'se', 'e', 'thek', 'itty', 'se', 'ethedoggy', 'doyou', 'l', 'ik', 'e', 'thek', 'itty', 'lik', 'ethedoggy']\n",
      "49 ['doyou', 'se', 'e', 'thek', 'itty', 'se', 'ethedoggy', 'doyou', 'lik', 'e', 'thek', 'itty', 'lik', 'ethedoggy']\n",
      "46 ['doyou', 'se', 'ethek', 'itty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethek', 'itty', 'lik', 'ethedoggy']\n",
      "46 ['doyou', 'se', 'ethek', 'itty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethek', 'itty', 'lik', 'ethedoggy']\n",
      "46 ['doyou', 'se', 'ethek', 'itty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethek', 'itty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000101000000001010000000010000100100000000100100000000'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11011'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg = \"11111\"\n",
    "flip(seg, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We might get better results if we lower the `cooling_rate`, but we'll also need more time to run the algorithm.  __Warning:__ The `cooling_rate` should never go to 1.0 or below.  If we do that, the algorithm will run forever.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "62 ['doyouseet', 'hekitty', 'seethedoggydoyo', 'uliket', 'hekitty', 'liket', 'hedoggy']\n",
      "57 ['doyo', 'useethekitty', 'seet', 'hedoggy', 'doyo', 'u', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "57 ['doyo', 'useethekitty', 'seet', 'hedoggy', 'doyo', 'u', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "57 ['doyo', 'useethekitty', 'seet', 'hedoggy', 'doyo', 'u', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "56 ['doyou', 'see', 'thekittysee', 't', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "56 ['doyou', 'see', 'thekittysee', 't', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "56 ['doyou', 'see', 'thekittysee', 't', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "54 ['doyou', 'seethekittyseet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "54 ['doyou', 'seethekittyseet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "54 ['doyou', 'seethekittyseet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "54 ['doyou', 'seethekittyseet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "52 ['doyou', 'seethe', 'kitty', 'seet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "52 ['doyou', 'seethe', 'kitty', 'seet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "46 ['doyou', 'seet', 'he', 'kitty', 'seet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "46 ['doyou', 'seet', 'he', 'kitty', 'seet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "46 ['doyou', 'seet', 'he', 'kitty', 'seet', 'hedoggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "43 ['doyou', 'seet', 'hekitty', 'seet', 'hedoggy', 'doyou', 'liket', 'hekitty', 'liket', 'hedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100010000001000100000010000100001000000100001000000'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anneal(text, seg1, 5000, 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9   Formatting: From Lists to Strings\n",
    "\n",
    "##### From Lists to Strings\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "##### Strings and Formats\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "##### Lining Things Up\n",
    "\n",
    "*We can add padding to formatted strings with `:`.  The number in the bracket refers to the width of the new string.  Numbers are right-justified by default, and strings are left-justified:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    41'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the number 41 with a minimum width of 6, adding spaces to the left if necessary\n",
    "'{:6}'.format(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob   '"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the string \"Bob\" with a minimum width of 6, adding spaces to the left if necessary\n",
    "'{:6}'.format(\"Bob\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use `:<` to right-justify numbers, and `:>` to left-justify strings:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41    '"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the number 41 with a minimum width of 6, left-aligning it and adding spaces to the right\n",
    "'{:<6}'.format(41)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   Bob'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the string \"Bob\" with a minimum width of 6, right-aligning it and adding spaces to the left\n",
    "'{:>6}'.format(\"Bob\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use these methods to format a table of Conditional Frequency Distributions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(cfdist, words, categories):                                # Define function to tabulate word frequencies per category\n",
    "    print('{:16}'.format('Category'), end = ' ')                        # Print 'Category' with a width of 16, followed by a space\n",
    "    for word in words:                                                  # Loop through each word in the 'words' list\n",
    "        print('{:>6}'.format(word), end = ' ')                          # Print each word right-aligned with a width of 6\n",
    "    print()                                                          \n",
    "    for category in categories:                                         # Loop through each category in the 'categories' list\n",
    "        print('{:16}'.format(category), end = ' ')                      # Print the category name with a width of 16\n",
    "        for word in words:                                              # Loop through each word again for the current category\n",
    "            print('{:6}'.format(cfdist[category][word]), end = ' ')     # Print the frequency of the word in the category, with a width of 6\n",
    "        print()                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category            can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown                                           \n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(                         # Create a Conditional Frequency Distribution (CFD)\n",
    "         (genre, word)                                  # Pair each genre with each word\n",
    "         for genre in brown.categories()                # Loop over each genre in the Brown corpus\n",
    "         for word in brown.words(categories = genre))   # Loop over each word in the genre\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']  # Define a list of specific genres\n",
    "\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']    # Define a list of modal verbs\n",
    "\n",
    "tabulate(cfd, modals, genres)                                # Call the 'tabulate' function to display word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*However, this function was crafted to work specifically with the words in `modals`.  If we use a new set of terms, the output isn't so nice:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category         Sunday Monday Tuesday Wednesday Thursday Friday Saturday \n",
      "news                 51     54     43     22     20     41     33 \n",
      "religion              8      0      0      0      0      2      0 \n",
      "hobbies               2      1      0      0      1      3      0 \n",
      "science_fiction       1      0      0      0      0      0      0 \n",
      "romance               5      2      3      3      1      3      4 \n",
      "humor                 0      1      0      0      0      0      3 \n"
     ]
    }
   ],
   "source": [
    "days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']  # Define a list of days of the week\n",
    "\n",
    "tabulate(cfd, days, genres)                        # Call the 'tabulate' function to display frequencies of days in genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The function below makes allowances for the size of the words and the categories by using the width of the longest word in each.  Notice how we have to use a second set of `{}` when we wish to use a variable inside the `format` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(cfdist, words, categories):                         # Define function to tabulate word frequencies per category\n",
    "    c_length = max([len(c) for c in categories]) + 2             # Calculate the maximum length of category names, add 2 for spacing\n",
    "    print('{:{}}'.format('Category', c_length), end = ' ')       # Print 'Category' header with dynamic width based on 'c_length'\n",
    "    w_length = max([len(w) for w in words]) + 1                  # Calculate the maximum length of word names, add 1 for spacing\n",
    "    for word in words:                                           # Loop through each word in 'words'\n",
    "        print('{:>{}}'.format(word, w_length), end = ' ')        # Print each word right-aligned with dynamic width 'w_length'\n",
    "    print()                                         \n",
    "    for category in categories:                                  # Loop through each category in 'categories'\n",
    "        print('{:{}}'.format(category, c_length), end = ' ')     # Print category name left-aligned with dynamic width 'c_length'\n",
    "        for word in words:                                       # Loop through each word for the current category\n",
    "            print('{:{}}'.format(cfdist[category][word], w_length), end = ' ')  # Print word frequency right-aligned with width 'w_length'\n",
    "        print()                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category              Sunday     Monday    Tuesday  Wednesday   Thursday     Friday   Saturday \n",
      "news                      51         54         43         22         20         41         33 \n",
      "religion                   8          0          0          0          0          2          0 \n",
      "hobbies                    2          1          0          0          1          3          0 \n",
      "science_fiction            1          0          0          0          0          0          0 \n",
      "romance                    5          2          3          3          1          3          4 \n",
      "humor                      0          1          0          0          0          0          3 \n"
     ]
    }
   ],
   "source": [
    "days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "tabulate(cfd, days, genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing Results to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will', 'ggggggggggg']\n",
    "\n",
    "max([len(w) for w in words ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Wrapping\n",
    "\n",
    "*Could not produce the same output as the book:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1), "
     ]
    }
   ],
   "source": [
    "# Define a list of words in a saying\n",
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']  \n",
    "\n",
    "for word in saying:                                      # Loop through each word in the saying\n",
    "    print(word, '(' + str(len(word)) + '),', end = ' ')  # Print the word followed by its length in parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5 all 3 is 2 said 4 and 3 done 4 , 1 more 4 is 2 said 4 than 4\n",
      "done 4 . 1\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill \n",
    "\n",
    "pieces = [\"{} {}\".format(word, len(word)) for word in saying]   # Create a list where each word is followed by its length\n",
    "output = ' '.join(pieces)                                       # Join the list of words into a single string with spaces between them\n",
    "wrapped = fill(output)                                          # Use 'fill' to wrap the output string into multiple lines (default width 70)\n",
    "print(wrapped)                                                  # Print the wrapped output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
