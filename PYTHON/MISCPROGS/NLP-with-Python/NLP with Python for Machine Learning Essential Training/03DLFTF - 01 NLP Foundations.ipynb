{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su3bfBDiRK9L"
   },
   "source": [
    "# Word Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing**: a formal analysis of a sentence into its constituents to produce a parse tree showing their syntactic relation to one another.\n",
    "\n",
    "**Stemming**: the process of reducing words to their stems, such as part of the word rid of all affixes.\n",
    "\n",
    "**Text segmentation**: the process of transforming text into meaningful componetes like word, intention, and sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebo6jG2X__lO"
   },
   "source": [
    "#### Import libraries and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dVGySTYgyVgW"
   },
   "outputs": [],
   "source": [
    "## import the tensorflow APIs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zclLXRIm_-jg"
   },
   "source": [
    "#### Define training sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oC-bEci9Q-EI"
   },
   "outputs": [],
   "source": [
    "##sentences to tokenize\n",
    "train_sentences1 = [\n",
    "             'It is a sunny day',\n",
    "             'It is a cloudy day',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOPlMrqrAGcf"
   },
   "source": [
    "#### Set up the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GbEn11WiT5Sp"
   },
   "outputs": [],
   "source": [
    "##instantiate the tokenizer\n",
    "tokenizer1 = Tokenizer(num_words=100)   # Setting this hyper parameter what the organizer will do is take up \n",
    "                                        # the top 100 words by volume and just encode those\n",
    "\n",
    "##train the tokenizer on training sentences\n",
    "tokenizer1.fit_on_texts(train_sentences1) \n",
    "\n",
    "##store word index for the words in the sentence\n",
    "word_index1 = tokenizer1.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zZR31LAUM4p",
    "outputId": "42680152-dcff-4d75-efaf-bbaf8b70879c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6}\n"
     ]
    }
   ],
   "source": [
    "print(word_index1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sequences of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training sentences in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of sentences to tokenize\n",
    "train_sentences2 = [\n",
    "             'It is a sunny day',\n",
    "             'It is a cloudy day',\n",
    "             'Will it rain today?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set up the tokenizer\n",
    "tokenizer2 = Tokenizer(num_words=100)\n",
    "\n",
    "##train the tokenizer on training sentences\n",
    "tokenizer2.fit_on_texts(train_sentences2)\n",
    "\n",
    "##store word index for the words in the sentence\n",
    "word_index2 = tokenizer2.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index -->{'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6, 'will': 7, 'rain': 8, 'today': 9}\n",
      "Sequences of words -->[[1, 2, 3, 5, 4], [1, 2, 3, 6, 4], [7, 1, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "##create sequences using tokenizer\n",
    "sequences = tokenizer2.texts_to_sequences(train_sentences2)\n",
    "\n",
    "##print word index dictionary and sequences\n",
    "print(f\"Word index -->{word_index2}\")\n",
    "print(f\"Sequences of words -->{sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a sunny day\n",
      "[1, 2, 3, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "##print sample sentence and sequence\n",
    "print(train_sentences2[0])\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a cloudy day\n",
      "[1, 2, 3, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences2[1])\n",
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will it rain today?\n",
      "[7, 1, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences2[2])\n",
    "print(sequences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing new data using the same tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index -->{'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6, 'will': 7, 'rain': 8, 'today': 9}\n",
      "\n",
      "['Will it be raining today?', 'It is a pleasant day.']\n",
      "[[7, 1, 9], [1, 2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\n",
    "                 'Will it be raining today?',\n",
    "                 'It is a pleasant day.'\n",
    "]\n",
    "\n",
    "new_sequences = tokenizer2.texts_to_sequences(new_sentences)\n",
    "\n",
    "print(f\"Word index -->{word_index2}\")\n",
    "print()\n",
    "print(new_sentences)\n",
    "print(new_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing newly encountered words with special values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'it': 2, 'is': 3, 'a': 4, 'day': 5, 'sunny': 6, 'cloudy': 7, 'will': 8, 'rain': 9, 'today': 10}\n",
      "['Will it be raining today?', 'It is a pleasant day.']\n",
      "[[8, 2, 1, 1, 10], [2, 3, 4, 1, 5]]\n"
     ]
    }
   ],
   "source": [
    "##set up the tokenizer again with oov_token\n",
    "tokenizer3 = Tokenizer(num_words=100, oov_token = \"<oov>\")\n",
    "\n",
    "##train the new tokenizer on training sentences\n",
    "tokenizer3.fit_on_texts(train_sentences2)\n",
    "\n",
    "##store word index for the words in the sentence\n",
    "word_index3 = tokenizer3.word_index\n",
    "\n",
    "\n",
    "##create sequences of the new sentences\n",
    "new_sequences = tokenizer3.texts_to_sequences(new_sentences)\n",
    "\n",
    "print(word_index3)\n",
    "print(new_sentences)\n",
    "print(new_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required APIs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences4 = [\n",
    "             'It will rain',\n",
    "             'The weather is cloudy!',\n",
    "             'Will it be raining today?',\n",
    "             'It is a super hot day!',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the tokenizer again with oov_token\n",
    "tokenizer4 = Tokenizer(num_words=100, oov_token='<oov>')\n",
    "\n",
    "# train the tokenizer on training sentences\n",
    "tokenizer4.fit_on_texts(train_sentences4)\n",
    "\n",
    "# store word index for the words in the sentence\n",
    "word_index4 = tokenizer4.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences\n",
    "sequences4 = tokenizer4.texts_to_sequences(train_sentences4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'it': 2, 'will': 3, 'is': 4, 'rain': 5, 'the': 6, 'weather': 7, 'cloudy': 8, 'be': 9, 'raining': 10, 'today': 11, 'a': 12, 'super': 13, 'hot': 14, 'day': 15}\n",
      "\n",
      "['It will rain', 'The weather is cloudy!', 'Will it be raining today?', 'It is a super hot day!']\n",
      "\n",
      "[[2, 3, 5], [6, 7, 4, 8], [3, 2, 9, 10, 11], [2, 4, 12, 13, 14, 15]]\n",
      "\n",
      "[[ 0  0  0  2  3  5]\n",
      " [ 0  0  6  7  4  8]\n",
      " [ 0  3  2  9 10 11]\n",
      " [ 2  4 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "padded_seqs4 = pad_sequences(sequences4)\n",
    "\n",
    "\n",
    "print(word_index4)\n",
    "print()\n",
    "print(train_sentences4)\n",
    "print()\n",
    "print(sequences4)\n",
    "print()\n",
    "print(padded_seqs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customising your padded sequence with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  5  0  0]\n",
      " [ 6  7  4  8  0]\n",
      " [ 3  2  9 10 11]\n",
      " [ 2  4 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "## Pad sequences with parameters\n",
    "padded_seqs = pad_sequences(\n",
    "                            sequences4,         # Apply padding to the sequences\n",
    "                            padding=\"post\",     # Add padding to the end of each sequence\n",
    "                            maxlen=5,           # Ensure each sequence has a maximum length of 5\n",
    "                            truncating=\"post\",  # Truncate sequences that are longer than maxlen from the end\n",
    "                            )\n",
    "\n",
    "# Output the padded sequences\n",
    "print(padded_seqs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the padded sequence, you see all of the sequences are all of the length five. And you see the last sentence which had 2, 4 12, 13, 14, 15, as you can see over here. Now the 15 has been truncated from the end because we are using *post truncating*. And all the zeros have been added at the end because we were using *post padding*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Tokenizing news headlines for data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation include the following steps:\n",
    "\n",
    "1. Download and read the data\n",
    "2. Segregate the headlines and their labels.\n",
    "3. Tokenize the headlines\n",
    "4. Create sequences and add padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download Kaggle Dataset\n",
    "data = pd.read_json('data/Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segregating the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       " 'j.k. rowling wishes snape happy birthday in the most magical way']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lists to store the headlines and labels\n",
    "headlines = list(data['headline'])\n",
    "labels = list(data['is_sarcastic'])\n",
    "\n",
    "headlines[:5]  # First 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required APIs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the tokenizer\n",
    "tokenizerc = Tokenizer(oov_token=\"<oov>\")\n",
    "tokenizerc.fit_on_texts(headlines)\n",
    "\n",
    "word_indexc = tokenizerc.word_index\n",
    "\n",
    "#print(word_indexc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: <oov>, Index: 1\n",
      "Word: to, Index: 2\n",
      "Word: of, Index: 3\n",
      "Word: the, Index: 4\n",
      "Word: in, Index: 5\n",
      "Word: for, Index: 6\n",
      "Word: a, Index: 7\n",
      "Word: on, Index: 8\n",
      "Word: and, Index: 9\n",
      "Word: with, Index: 10\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 elements from word_indexc\n",
    "first_10_items = list(word_indexc.items())[:10]\n",
    "for word, index in first_10_items:\n",
    "    print(f'Word: {word}, Index: {index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# create sequences of the headlines\n",
    "seqsc = tokenizerc.texts_to_sequences(headlines)\n",
    "\n",
    "# post-pad sequences\n",
    "padded_seqsc = pad_sequences(seqsc, padding=\"post\")\n",
    "\n",
    "# printing padded sequence of the first headline \n",
    "print(padded_seqsc[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJAklrN1kdCylM8/lUZYAF",
   "include_colab_link": true,
   "name": "01_02_end.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
