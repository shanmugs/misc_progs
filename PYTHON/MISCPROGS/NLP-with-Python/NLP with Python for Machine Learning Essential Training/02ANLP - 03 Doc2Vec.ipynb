{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: How too implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of creating a vector for each word, this technique will create a vector for each document or collection of text, whether it's a sentence or a paragraph. The goal is the same as `word2vec`. To create a numeric representation of a set of texts to feed to Python to help it better understand the meaning. \n",
    "\n",
    "Recall that `word2vec` is a shallow two-layer neural network that accepts a text corpus as an input, and it returns a set of vectors, also known as embeddings. Each vector is a numeric representation of a given word. `doc2vec` is basically the same thing, but instead of returning a numeric vector for each word, it returns a numeric vector for each sentence or paragraph.\n",
    "\n",
    "The real benefit of `Doc2Vec` is it captures information about a sentence or paragraph, which is what we need, in a much more sophisticated way than creating word vectors and then averaging them. So in `Word2Vec`, we lose information by averaging the word vectors together to create a sentence or text level representation. `Doc2Vec` is able to capture the sentence level representation in a much more sophisticated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Our Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsoares\\.ipython\\profile_default\n"
     ]
    }
   ],
   "source": [
    "!ipython locate profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...  \n",
       "1                                                                          [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...  \n",
       "3                                                       [dun, say, so, early, hor, already, then, say]  \n",
       "4                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data, clean it, and then split into train and test sets\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],  # Cleaned text data for training\n",
    "                                                    messages['label'],       # Corresponding target labels\n",
    "                                                    test_size=0.2)           # Use 20% of the data for testing\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one of the differences between `word2vec` and `doc2vec` is that `doc2vec` requires you to create tagged documents. This tagged document, expects a list of words and a tag for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to iterate through X_train using this enumerate function and that'll return the index and the value for each text message in X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['dont', 'thnk', 'its', 'wrong', 'calling', 'between', 'us'], tags=[0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tagged document objects to prepare to train the model\n",
    "tagged_docs = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]\n",
    "\n",
    "# Look at what a tagged document looks like\n",
    "tagged_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic doc2vec model\n",
    "d2v_model = gensim.models.Doc2Vec(tagged_docs,     # Tagged documents for training\n",
    "                                 vector_size=100,  # Dimensionality of the document vectors\n",
    "                                 window=5,         # Maximum distance between the current and predicted word within a sentence\n",
    "                                 min_count=2)      # Minimum number of occurrences of a word to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00798229,  0.01140039, -0.01043457, -0.01564807,  0.0034668 ,\n",
       "       -0.03537234,  0.01152923,  0.05926319, -0.00906062, -0.0155829 ,\n",
       "       -0.01331935, -0.02749657, -0.00572696,  0.00810215,  0.00133955,\n",
       "       -0.03076872,  0.00469946, -0.02882499,  0.0082822 , -0.04647089,\n",
       "        0.01412934,  0.01810895,  0.0142637 , -0.00232068,  0.00919335,\n",
       "       -0.00369618, -0.0025273 , -0.01774842, -0.02490121, -0.01018221,\n",
       "        0.01805401, -0.00242939,  0.01390081, -0.00753134, -0.01360392,\n",
       "        0.03192316,  0.00762084, -0.0185664 , -0.00654213, -0.0283397 ,\n",
       "       -0.01166244, -0.01969059, -0.00832936,  0.01400114,  0.01149535,\n",
       "       -0.0071169 , -0.01649878,  0.00077341,  0.00119541,  0.02045974,\n",
       "        0.00764342, -0.02728012,  0.01257649,  0.00521487, -0.00386214,\n",
       "        0.01318282,  0.01741318, -0.00256588, -0.02155077,  0.00336554,\n",
       "        0.01338103,  0.00300988,  0.00979202, -0.00682402, -0.0317105 ,\n",
       "        0.02232279,  0.00080215,  0.01393274, -0.02342951,  0.02264915,\n",
       "       -0.00198628,  0.0203368 ,  0.01208786, -0.00318581,  0.02587952,\n",
       "        0.00229452,  0.00692188,  0.00334305, -0.02507075,  0.00196061,\n",
       "       -0.00714256,  0.0099399 , -0.01546206,  0.02453174, -0.0131499 ,\n",
       "        0.01061202, -0.0060417 ,  0.01329126,  0.02502892,  0.01103535,\n",
       "        0.0299344 ,  0.00682684,  0.00315978,  0.02379128,  0.03030499,\n",
       "        0.02265839,  0.01044783, -0.01817363,  0.00363527,  0.00487367],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if we pass in a single word like we did for word2vec?\n",
    "d2v_model.infer_vector(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.4133963e-03,  1.1715040e-02, -5.1480252e-03, -9.6279001e-03,\n",
       "       -1.4430014e-03, -3.2380532e-02,  1.8065844e-02,  4.9476977e-02,\n",
       "       -6.2905448e-03, -1.6607398e-02, -7.7082855e-03, -2.9892946e-02,\n",
       "       -5.2800183e-03,  6.7910198e-03,  2.2265767e-03, -1.7420869e-02,\n",
       "        6.7678122e-03, -2.0868720e-02,  1.1361656e-03, -4.6570800e-02,\n",
       "        8.9863259e-03,  7.8002485e-03,  1.6005872e-02, -1.1420925e-02,\n",
       "        1.5199286e-03, -8.6489422e-03, -3.3598340e-03, -1.8516034e-02,\n",
       "       -2.0690057e-02, -8.1399987e-03,  2.2926476e-02,  4.3311116e-05,\n",
       "        2.6588293e-02, -1.8358380e-02, -7.4351230e-03,  3.4932412e-02,\n",
       "        4.0876816e-04, -6.6025234e-03, -1.2406030e-02, -3.4672942e-02,\n",
       "       -8.1659677e-03, -2.0882104e-02,  1.1908055e-03,  8.4546031e-03,\n",
       "        1.6867347e-02, -1.1269779e-02, -1.4139514e-02, -1.2567131e-02,\n",
       "        7.0599164e-03,  2.2530489e-02,  1.1712081e-02, -1.6853958e-02,\n",
       "        4.2819671e-04, -1.1067763e-02, -1.3313100e-02,  7.2365846e-03,\n",
       "        3.6493798e-03,  1.1236489e-06, -2.6675552e-02,  7.0174197e-03,\n",
       "        5.1428555e-03, -3.1923109e-03, -2.6620999e-03, -1.0068962e-02,\n",
       "       -2.3700409e-02,  2.4963396e-02,  1.1596348e-02,  6.8454859e-03,\n",
       "       -3.0894479e-02,  2.3233116e-02, -4.5044152e-03,  1.7390717e-02,\n",
       "        2.0847546e-02,  2.1086608e-03,  3.6055703e-02,  2.1085171e-03,\n",
       "        3.6583131e-03, -6.3151033e-03, -1.8204533e-02,  8.3194353e-04,\n",
       "       -8.6316355e-03,  4.7634202e-03, -2.0783443e-02,  2.8029637e-02,\n",
       "       -1.7012592e-02, -8.5467165e-03, -6.4694567e-04,  1.6685987e-02,\n",
       "        2.1300070e-02,  8.1495764e-03,  2.8226914e-02,  1.3163869e-02,\n",
       "        5.8093476e-03,  1.3599055e-02,  4.0504076e-02,  1.7439147e-02,\n",
       "        7.8408252e-03, -2.4324514e-02,  7.4033239e-03, -6.0911458e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if we pass in a list of words?\n",
    "d2v_model.infer_vector(['i','am','learning','nlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What About Pre-trained Document Vectors?\n",
    "\n",
    "There are not as many options as there are for word vectors. There also is not an easy API to read these in like there is for `word2vec` so it is more time consuming.\n",
    "\n",
    "Pre-trained vectors from training on Wikipedia and Associated Press News can be found [here](https://github.com/jhlau/doc2vec). Feel free to explore on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Prep Document Vectors For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data, clean it, split it into train/test, and then train a doc2vec model\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],  # Cleaned text data for training\n",
    "                                                    messages['label'],       # Corresponding target labels\n",
    "                                                    test_size=0.2)           # Use 20% of the data for testing\n",
    "\n",
    "\n",
    "# Create tagged document objects to prepare to train the model\n",
    "tagged_docs_tr = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]\n",
    "\n",
    "\n",
    "# Train a Doc2Vec model on the tagged documents\n",
    "d2v_model = gensim.models.Doc2Vec(tagged_docs_tr,     # Tagged documents for training the model\n",
    "                                  vector_size=50,     # Dimensionality of the document vectors\n",
    "                                  window=2,           # Maximum distance between the current and predicted words within a sentence\n",
    "                                  min_count=2)        # Minimum number of occurrences of a word to be considered in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read in our data, clean it, split it into train and test set, and then we'll train our doc to vec model on our training set. As that's training, recall that we can create a document vector by passing a list of words into the infer_vector method for the trained model. Again, this returns a single vector of length 100 that is prepared to be passed directly into a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01124724,  0.01765448, -0.0173507 ,  0.01307189,  0.00111577,\n",
       "       -0.00714126,  0.0196327 ,  0.04415387, -0.03992124, -0.01049349,\n",
       "        0.01006596, -0.03300128, -0.00194706,  0.02483031, -0.0105353 ,\n",
       "        0.01793564,  0.03844629, -0.00639612, -0.02700614, -0.0373323 ,\n",
       "        0.00343428,  0.01769488,  0.04617324,  0.02278028,  0.03255073,\n",
       "        0.02028335, -0.02793016, -0.0056128 , -0.02779184,  0.01132937,\n",
       "       -0.00465862,  0.00612979, -0.01565399,  0.0041665 , -0.0126611 ,\n",
       "        0.04156687,  0.00331445,  0.00062365,  0.01735365, -0.00391612,\n",
       "        0.03749568, -0.00858164, -0.00824811,  0.00185709,  0.03762186,\n",
       "        0.00539917, -0.00381365, -0.02473323, -0.00387513,  0.01493936],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does a document vector look like again?\n",
    "d2v_model.infer_vector(['convert', 'words', 'to', 'vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "`d2v_model`: This is your trained Doc2Vec model. It has learned to map documents and words to vectors in a continuous vector space.\n",
    "\n",
    "`infer_vector`: This method is used to infer a vector for a new document that was not part of the training set. The model generates a vector that represents the content of the document.\n",
    "\n",
    "`['convert', 'words', 'to', 'vectors']`: This is the list of words (tokens) in the new document you want to infer a vector for. The list should be preprocessed in the same way as the training documents (e.g., tokenized and cleaned).\n",
    "\n",
    "#### What Happens Internally\n",
    "**Inference**: The method infer_vector performs inference to generate a vector for the provided list of words. This is done by comparing the new document with the documents the model has seen during training and finding a vector that best represents the new document.\n",
    "\n",
    "**Result**: The output is a fixed-length vector (with dimensions specified during training) that captures the semantic meaning of the input words. This vector can be used for various tasks, such as document similarity, classification, or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is aimed at converting text data into numerical vectors that can be used for training and prediction in machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.05731815,  0.01634658,  0.040213  , -0.01886609,  0.04126149,\n",
       "        -0.00377404,  0.00072359,  0.0173449 ,  0.00927827,  0.00431059,\n",
       "        -0.02341248, -0.01482392, -0.03337708,  0.02598854,  0.05156513,\n",
       "         0.00017511,  0.00793516,  0.02812459,  0.07645095, -0.02688686,\n",
       "         0.01720904, -0.06830827, -0.01210206,  0.04015526,  0.06324875,\n",
       "         0.02678719,  0.02162048,  0.04558533,  0.04820809,  0.02977867,\n",
       "        -0.00435448,  0.04377825,  0.02617526,  0.00801478,  0.04583059,\n",
       "         0.00318846,  0.03561984,  0.01070106,  0.06706019,  0.00595404,\n",
       "         0.01573588,  0.01055892,  0.04312979, -0.00069565, -0.00442941,\n",
       "         0.02150692, -0.0069403 , -0.00156944,  0.06485177, -0.04376391],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do we prepare these vectors to be used in a machine learning model?\n",
    "vects=[[d2v_model.infer_vector(words)] for words in X_test]\n",
    "\n",
    "vects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers may seem random to the human eye but there's a pattern here that was learned by the doc to vec model as a way to encode the meaning as a set of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "`d2v_model.infer_vector(words)`:\n",
    "\n",
    "**d2v_model**: This is your trained *Doc2Vec* model.\n",
    "\n",
    "**infer_vector**: This method infers a vector for the provided document (a list of words in this case) based on what the model has learned during training.\n",
    "\n",
    "**words**: Each entry in *X_test* is expected to be a list of words (tokens) from a document in your test set.\n",
    "\n",
    "`[[d2v_model.infer_vector(words)] for words in X_test]`:\n",
    "\n",
    "* This list comprehension iterates over each document in the *X_test* dataset.\n",
    "* For each document, it calls *d2v_model.infer_vector(words)*, which generates a vector representation for that document.\n",
    "* Each inferred vector is wrapped in a list (hence the double brackets [[]]).\n",
    "\n",
    "`vects[0]`:\n",
    "\n",
    "* This retrieves the first element of the vects list, which is the vector representation for the first document in *X_test*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
