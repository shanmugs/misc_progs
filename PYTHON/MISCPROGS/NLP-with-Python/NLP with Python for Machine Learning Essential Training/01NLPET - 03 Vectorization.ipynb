{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df845bd6-5424-449b-bc0d-8d668ea11976",
   "metadata": {},
   "source": [
    "# Vectorizing Raw Data: Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abea2d-fe9e-4184-8ecb-5a8d97290a28",
   "metadata": {},
   "source": [
    "#### Count vectorization \n",
    "\n",
    "Creates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865072b-661b-461e-a01b-438aa5303c20",
   "metadata": {},
   "source": [
    "#### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a59bfa2-db31-4944-aac3-47d798fff6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "ps = nltk.PorterStemmer()   # Just because is faster than Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2394a7e8-f6f9-47a6-b9f9-8d91b72183fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a9a28-ef69-47d2-82ac-ed84710aa68d",
   "metadata": {},
   "source": [
    "#### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d1e3e6-a18b-4237-89e1-4f9af544e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) # Remove punctuation\n",
    "    tokens = re.split('\\W+', text)                                            # Tokenize it\n",
    "    text = [word for word in tokens if word not in stopwords]                 # Remove stop words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4238594-ac24-443d-aaaf-5d79e8581ea5",
   "metadata": {},
   "source": [
    "## Apply CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9122172-2de2-4555-8936-9df25edb513b",
   "metadata": {},
   "source": [
    "Count vectorization creates the document-term matrix and then simply counts the number of times each word appears in that given document, or text message in our case, and that's what's stored in the given cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd8d55d-dfde-4358-858d-469647c32c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 11516)\n",
      "\n",
      "['' '0' '008704050406' ... 'ü' 'üll' '〨ud']\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer class from the sklearn.feature_extraction.text module.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer with a custom analyzer.\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Apply the CountVectorizer to the 'body_text' column in the 'data' DataFrame.\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "\n",
    "# Print the shape of the resulting sparse matrix (number of documents, number of unique tokens).\n",
    "print(X_counts.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "# Use get_feature_names_out() to get the list of feature names (tokens).\n",
    "# This method returns the list of tokens that represent the columns in the sparse matrix.\n",
    "print(count_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097ede3-948d-4ca3-bbd5-6f58ed8c8dbf",
   "metadata": {},
   "source": [
    "#### Apply CountVectorizer to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fea765b-d90a-48bb-bcdb-eecc3c9f8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 223)\n",
      "['08002986030' '08452810075over18s' '09061701461' '1' '100' '100000' '11'\n",
      " '12' '150pday' '16' '2' '20000' '2005' '21st' '3' '4' '4403LDNW1A7RW18'\n",
      " '4txtú120' '6days' '81010' '87077' '87121' '87575' '9' '900' 'A' 'Aft'\n",
      " 'Alright' 'Ard' 'As' 'CASH' 'CLAIM' 'CSH11' 'Call' 'Callers' 'Callertune'\n",
      " 'Claim' 'Co' 'Cost' 'Cup' 'DATE' 'ENGLAND' 'Eh' 'England' 'Even' 'FA'\n",
      " 'FREE' 'Ffffffffff' 'Fine' 'Free' 'From' 'HAVE' 'HL' 'Had' 'He' 'I' 'Im'\n",
      " 'Is' 'Ive' 'Jackpot' 'KL341' 'LCCLTD' 'Macedonia' 'May' 'Melle'\n",
      " 'Minnaminunginte' 'Mobile' 'Nah' 'No' 'Nurungu' 'ON' 'Oh' 'Oru' 'POBOX'\n",
      " 'POBOXox36504W45WQ' 'Press' 'Prize' 'R' 'Reply' 'SCOTLAND' 'SIX' 'SUNDAY'\n",
      " 'So' 'TC' 'Text' 'That' 'The' 'Then' 'They' 'To' 'TryWALES' 'TsandCs'\n",
      " 'Txt' 'U' 'URGENT' 'Update' 'Valid' 'Vettam' 'WAP' 'WILL' 'WINNER' 'WITH'\n",
      " 'XXXMobileMovieClub' 'Yes' 'You' 'aids' 'already' 'anymore' 'apply'\n",
      " 'around' 'b' 'brother' 'call' 'callertune' 'camera' 'chances' 'claim'\n",
      " 'click' 'code' 'colour' 'comin' 'comp' 'copy' 'credit' 'cried' 'customer'\n",
      " 'da' 'dont' 'eg' 'enough' 'entitled' 'entry' 'feel' 'final' 'finish'\n",
      " 'first' 'friends' 'go' 'goalsteam' 'goes' 'going' 'gonna' 'gota' 'ha'\n",
      " 'home' 'hours' 'httpwap' 'info' 'joking' 'k' 'kim' 'lar' 'latest' 'like'\n",
      " 'link' 'lives' 'lor' 'lunch' 'make' 'meet' 'membership' 'message' 'miss'\n",
      " 'mobile' 'mobiles' 'months' 'name' 'national' 'naughty' 'network' 'news'\n",
      " 'next' 'patent' 'pay' 'per' 'pounds' 'prize' 'questionstd' 'rateTCs'\n",
      " 'receive' 'receivea' 'remember' 'request' 'reward' 'selected' 'send'\n",
      " 'seriously' 'set' 'smth' 'soon' 'sooner' 'speak' 'spell' 'stock' 'str'\n",
      " 'stuff' 'talk' 'team' 'think' 'though' 'tkts' 'today' 'tonight' 'treat'\n",
      " 'try' 'txt' 'u' 'ur' 'use' 'usf' 'v' 'valued' 'want' 'watching' 'way'\n",
      " 'week' 'wet' 'win' 'wkly' 'word' 'wwwdbuknet'\n",
      " 'xxxmobilemovieclubcomnQJKGIGHJJGCBL' 'ü']\n"
     ]
    }
   ],
   "source": [
    "# Select a sample of the data\n",
    "data_sample = data[0:20]\n",
    "\n",
    "# Create a CountVectorizer instance with a custom analyzer\n",
    "count_vect_sample = CountVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Fit the vectorizer to the sample data and transform the text data into a sparse matrix of token counts\n",
    "X_counts_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
    "\n",
    "# Print the shape of the resulting document-term matrix\n",
    "print(X_counts_sample.shape)  # Outputs the dimensions (number of documents, number of unique words)\n",
    "\n",
    "# Print the feature names (unique words) extracted from the sample data\n",
    "print(count_vect_sample.get_feature_names_out())  # Outputs the list of feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "437cf898-95ef-4752-8512-fc04c4d46062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 45)\n",
      "['08452810075over18s' '2' '2005' '21st' '87121' 'A' 'Cup' 'DATE' 'Even'\n",
      " 'FA' 'Free' 'HAVE' 'I' 'May' 'Nah' 'ON' 'SUNDAY' 'Text' 'They' 'WILL'\n",
      " 'WITH' 'aids' 'apply' 'around' 'brother' 'comp' 'dont' 'entry' 'final'\n",
      " 'goes' 'like' 'lives' 'patent' 'questionstd' 'rateTCs' 'receive' 'speak'\n",
      " 'think' 'though' 'tkts' 'treat' 'txt' 'usf' 'win' 'wkly']\n"
     ]
    }
   ],
   "source": [
    "# Select a sample of the data\n",
    "data_sample = data[0:4]\n",
    "\n",
    "# Create a CountVectorizer instance with a custom analyzer\n",
    "count_vect_sample = CountVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Fit the vectorizer to the sample data and transform the text data into a sparse matrix of token counts\n",
    "X_counts_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
    "\n",
    "# Print the shape of the resulting document-term matrix\n",
    "print(X_counts_sample.shape)  # Outputs the dimensions (number of documents, number of unique words)\n",
    "\n",
    "# Print the feature names (unique words) extracted from the sample data\n",
    "print(count_vect_sample.get_feature_names_out())  # Outputs the list of feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fed11-8ef9-47bb-9fff-4e8955fd9abc",
   "metadata": {},
   "source": [
    "### Vectorizers output sparse matrices\n",
    "\n",
    "_**Sparse Matrix**: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0db2a4e-4963-473a-8e87-f0094a13e9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x45 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9a1df3-1305-4d5e-bef6-e293fa235adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  35  36  37  38  39  40  41  \\\n",
       "0   1   1   1   1   1   0   1   0   0   2  ...   1   0   0   0   1   0   1   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...   0   0   1   1   0   0   0   \n",
       "2   0   0   0   0   0   0   0   0   1   0  ...   0   1   0   0   0   1   0   \n",
       "3   0   0   0   0   0   1   0   1   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   42  43  44  \n",
       "0   0   1   1  \n",
       "1   1   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "\n",
       "[4 rows x 45 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts_sample.toarray())\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83300d33-186e-4b59-8348-221a2586c5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08452810075over18s</th>\n",
       "      <th>2</th>\n",
       "      <th>2005</th>\n",
       "      <th>21st</th>\n",
       "      <th>87121</th>\n",
       "      <th>A</th>\n",
       "      <th>Cup</th>\n",
       "      <th>DATE</th>\n",
       "      <th>Even</th>\n",
       "      <th>FA</th>\n",
       "      <th>...</th>\n",
       "      <th>receive</th>\n",
       "      <th>speak</th>\n",
       "      <th>think</th>\n",
       "      <th>though</th>\n",
       "      <th>tkts</th>\n",
       "      <th>treat</th>\n",
       "      <th>txt</th>\n",
       "      <th>usf</th>\n",
       "      <th>win</th>\n",
       "      <th>wkly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   08452810075over18s  2  2005  21st  87121  A  Cup  DATE  Even  FA  ...  \\\n",
       "0                   1  1     1     1      1  0    1     0     0   2  ...   \n",
       "1                   0  0     0     0      0  0    0     0     0   0  ...   \n",
       "2                   0  0     0     0      0  0    0     0     1   0  ...   \n",
       "3                   0  0     0     0      0  1    0     1     0   0  ...   \n",
       "\n",
       "   receive  speak  think  though  tkts  treat  txt  usf  win  wkly  \n",
       "0        1      0      0       0     1      0    1    0    1     1  \n",
       "1        0      0      1       1     0      0    0    1    0     0  \n",
       "2        0      1      0       0     0      1    0    0    0     0  \n",
       "3        0      0      0       0     0      0    0    0    0     0  \n",
       "\n",
       "[4 rows x 45 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the column names to the feature names (unique words) from the CountVectorizer\n",
    "X_counts_df.columns = count_vect_sample.get_feature_names_out()\n",
    "\n",
    "# Now we just have the actual column names here\n",
    "X_counts_df        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1331dd1-1241-497c-ab28-862eb4cc56a7",
   "metadata": {},
   "source": [
    "# Vectorizing Raw Data: N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2bd59a-a333-4966-b511-3afa649e2acb",
   "metadata": {},
   "source": [
    "### N-Grams \n",
    "\n",
    "Creates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n",
    "\n",
    "\"NLP is an interesting topic\"\n",
    "\n",
    "| n | Name      | Tokens                                                         |\n",
    "|---|-----------|----------------------------------------------------------------|\n",
    "| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n",
    "| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n",
    "| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f817422-b1d2-4101-a189-e1114c797f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"data/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41100d2-7d53-4856-8508-39dd5da6720c",
   "metadata": {},
   "source": [
    "Important: With **n-grams**, it wants a string pest into it so that it can look for the adjacent words in the string and chunk them together rather than taking an already tokenized list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ccd5d9-ee93-4a1f-9ec2-ab3024da0683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even brother like speak they treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>as per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi fri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                          cleaned_text  \n",
       "0  free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...  \n",
       "1                                                          nah i dont think goe usf live around though  \n",
       "2                                                   even brother like speak they treat like aid patent  \n",
       "3                                                                    i have a date on sunday with will  \n",
       "4  as per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi fri...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])    # Remove punctuation\n",
    "    tokens = re.split('\\W+', text)                                               # Tokenize it\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords]) # New Remove Stopwords (Now, it is a string) \n",
    "    #text = [word for word in tokens if word not in stopwords]                   # Old Remove Stopwords (It was a list of words)\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e03818-cb6c-43ae-b22c-76c2ff2d3597",
   "metadata": {},
   "source": [
    "We just took the tokenized list and created the string out of it. Now we can see this clean text column has all the tokens from our tokenized list. It's just reconstructed back into a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56f6be-9d06-468c-bd99-e9b7f167b255",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer (w/ N-Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01099738-66c3-4537-bd28-34e083e0c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 34114)\n",
      "['008704050406 sp' '0089mi last' '0121 2025050' ... 'üll submit'\n",
      " 'üll take' '〨ud even']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with ngram_range set to (2,2) for bigrams\n",
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Fit the vectorizer to the cleaned text data and transform it into a document-term matrix\n",
    "X_counts = ngram_vect.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Print the shape of the resulting matrix (number of documents, number of unique bigrams)\n",
    "print(X_counts.shape)\n",
    "\n",
    "# Print the feature names (unique bigrams) extracted from the data\n",
    "print(ngram_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ba77e0d-f450-4ad0-b6ab-42c77116b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['008704050406 sp' '0089mi last' '0121 2025050' '01223585236 xx'\n",
      " '01223585334 cum' '0125698789 ring' '02 user' '020603 2nd' '020603 thi'\n",
      " '0207 153' '02072069400 bx' '02073162414 cost' '02085076972 repli'\n",
      " '020903 thi' '021 3680' '021 3680offer' '050703 tcsbcm4235wc1n3xx'\n",
      " '06 good' '07046744435 arrang' '07090298926 reschedul'\n",
      " '07099833605 reschedul' '07123456789 87077' '0721072 find'\n",
      " '07732584351 rodger' '07734396839 ibh' '07742676969 show'\n",
      " '07753741225 show' '0776xxxxxxx uve' '077xxx won' '07801543489 guarante'\n",
      " '07808 xxxxxx' '07808247860 show' '07808726822 award' '07815296484 show'\n",
      " '0784987 show' '0789xxxxxxx today' '0796xxxxxx today' '07973788240 show'\n",
      " '07xxxxxxxxx 2000' '07xxxxxxxxx show' '0800 0721072' '0800 169' '0800 18'\n",
      " '0800 195' '0800 1956669' '0800 505060' '0800 542' '08000407165 18'\n",
      " '08000776320 repli' '08000839402 2stoptx']\n"
     ]
    }
   ],
   "source": [
    "# Display the first 100 bigrams in the list\n",
    "print(ngram_vect.get_feature_names_out()[:50])\n",
    "\n",
    "# Alternatively, display all bigrams\n",
    "# for bigram in ngram_vect.get_feature_names_out():\n",
    "#     print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa782471-f492-4322-8ff4-4b7ac02ae79a",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer (w/ N-Grams) to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23a6e8e7-811b-4f28-a24e-0b30332164c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 217)\n",
      "['09061701461 claim' '100 20000' '100000 prize' '11 month' '12 hour'\n",
      " '150pday 6day' '16 tsandc' '20000 pound' '2005 text' '21st may'\n",
      " '4txtú120 poboxox36504w45wq' '6day 16' '81010 tc' '87077 eg'\n",
      " '87077 trywal' '87121 receiv' '87575 cost' '900 prize' 'aft finish'\n",
      " 'aid patent' 'alright way' 'anymor tonight' 'appli 08452810075over18'\n",
      " 'appli repli' 'ard smth' 'around though' 'as per' 'as valu'\n",
      " 'brother like' 'call 09061701461' 'call the' 'caller press'\n",
      " 'callertun caller' 'camera free' 'cash from' 'chanc win' 'claim call'\n",
      " 'claim code' 'claim no' 'click httpwap' 'click wap' 'co free'\n",
      " 'code kl341' 'colour mobil' 'comp win' 'copi friend' 'cost 150pday'\n",
      " 'credit click' 'cri enough' 'csh11 send' 'cup final' 'custom select'\n",
      " 'da stock' 'date on' 'dont miss' 'dont think' 'dont want' 'eg england'\n",
      " 'eh rememb' 'england 87077' 'england macedonia' 'enough today'\n",
      " 'entitl updat' 'entri questionstd' 'entri wkli' 'even brother' 'fa 87121'\n",
      " 'fa cup' 'feel that' 'ffffffffff alright' 'final tkt' 'fine way'\n",
      " 'finish lunch' 'finish ur' 'first lar' 'free 08002986030' 'free call'\n",
      " 'free entri' 'free membership' 'friend callertun' 'from 100' 'go str'\n",
      " 'go tri' 'goalsteam news' 'goe usf' 'gonna home' 'ha ha' 'ha joke'\n",
      " 'had mobil' 'have date' 'he naughti' 'hl info' 'home soon'\n",
      " 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl' 'im gonna' 'is serious'\n",
      " 'ive cri' 'jackpot txt' 'kim watch' 'kl341 valid' 'lar then'\n",
      " 'latest colour' 'lccltd pobox' 'like aid' 'like speak' 'link next'\n",
      " 'live around' 'lor ard' 'lor finish' 'lunch alreadi' 'lunch go'\n",
      " 'macedonia dont' 'make wet' 'may 2005' 'meet sooner' 'mell mell'\n",
      " 'mell oru' 'membership 100000' 'messag click' 'minnaminungint nurungu'\n",
      " 'miss goalsteam' 'mobil 11' 'mobil camera' 'mobil updat' 'month entitl'\n",
      " 'month ha' 'nah dont' 'name ye' 'nation team' 'naughti make'\n",
      " 'network custom' 'news txt' 'next txt' 'no 81010' 'nurungu vettam'\n",
      " 'oh kim' 'on sunday' 'oru minnaminungint' 'pay first' 'per request'\n",
      " 'pobox 4403ldnw1a7rw18' 'poboxox36504w45wq 16' 'pound txt' 'press copi'\n",
      " 'prize jackpot' 'prize reward' 'questionstd txt' 'ratetc appli'\n",
      " 'receiv entri' 'receivea 900' 'rememb spell' 'repli hl' 'request mell'\n",
      " 'reward to' 'scotland 4txtú120' 'select receivea' 'send 87575'\n",
      " 'serious spell' 'set callertun' 'six chanc' 'smth lor' 'so pay'\n",
      " 'soon dont' 'speak they' 'spell name' 'stock comin' 'str lor'\n",
      " 'stuff anymor' 'sunday with' 'talk stuff' 'tc wwwdbuknet' 'team 87077'\n",
      " 'text fa' 'that way' 'the mobil' 'then da' 'they treat' 'think goe'\n",
      " 'tkt 21st' 'to claim' 'to use' 'tonight ive' 'treat like' 'tri month'\n",
      " 'trywal scotland' 'tsandc appli' 'txt csh11' 'txt messag' 'txt ratetc'\n",
      " 'txt ur' 'txt word' 'updat co' 'updat latest' 'ur lunch' 'ur nation'\n",
      " 'urgent you' 'use credit' 'usf live' 'valid 12' 'valu network'\n",
      " 'vettam set' 'want talk' 'wap link' 'way feel' 'way gota' 'way meet'\n",
      " 'week free' 'win cash' 'win fa' 'winner as' 'with will' 'wkli comp'\n",
      " 'word claim' 'wwwdbuknet lccltd' 'xxxmobilemovieclub to' 'ye he'\n",
      " 'you week']\n"
     ]
    }
   ],
   "source": [
    "# Select a sample of 20 documents from the dataset for analysis\n",
    "data_sample = data[0:20]\n",
    "\n",
    "# Initialize a CountVectorizer with ngram_range set to (2,2) to extract bigrams\n",
    "ngram_vect_sample = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Fit the vectorizer to the cleaned text data in the sample and transform it into a document-term matrix\n",
    "X_counts_sample = ngram_vect_sample.fit_transform(data_sample['cleaned_text'])\n",
    "\n",
    "# Print the shape of the resulting matrix (number of documents, number of unique bigrams)\n",
    "print(X_counts_sample.shape)\n",
    "\n",
    "# Print the list of bigrams (unique word pairs) extracted from the sample\n",
    "print(ngram_vect_sample.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fe7cc54-1c44-4812-8dfb-00991b86ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>09061701461 claim</th>\n",
       "      <th>100 20000</th>\n",
       "      <th>100000 prize</th>\n",
       "      <th>11 month</th>\n",
       "      <th>12 hour</th>\n",
       "      <th>150pday 6day</th>\n",
       "      <th>16 tsandc</th>\n",
       "      <th>20000 pound</th>\n",
       "      <th>2005 text</th>\n",
       "      <th>21st may</th>\n",
       "      <th>...</th>\n",
       "      <th>win cash</th>\n",
       "      <th>win fa</th>\n",
       "      <th>winner as</th>\n",
       "      <th>with will</th>\n",
       "      <th>wkli comp</th>\n",
       "      <th>word claim</th>\n",
       "      <th>wwwdbuknet lccltd</th>\n",
       "      <th>xxxmobilemovieclub to</th>\n",
       "      <th>ye he</th>\n",
       "      <th>you week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    09061701461 claim  100 20000  100000 prize  11 month  12 hour  \\\n",
       "0                   0          0             0         0        0   \n",
       "1                   0          0             0         0        0   \n",
       "2                   0          0             0         0        0   \n",
       "3                   0          0             0         0        0   \n",
       "4                   0          0             0         0        0   \n",
       "5                   1          0             0         0        1   \n",
       "6                   0          0             0         1        0   \n",
       "7                   0          0             0         0        0   \n",
       "8                   0          1             0         0        0   \n",
       "9                   0          0             1         0        0   \n",
       "10                  0          0             0         0        0   \n",
       "11                  0          0             0         0        0   \n",
       "12                  0          0             0         0        0   \n",
       "13                  0          0             0         0        0   \n",
       "14                  0          0             0         0        0   \n",
       "15                  0          0             0         0        0   \n",
       "16                  0          0             0         0        0   \n",
       "17                  0          0             0         0        0   \n",
       "18                  0          0             0         0        0   \n",
       "19                  0          0             0         0        0   \n",
       "\n",
       "    150pday 6day  16 tsandc  20000 pound  2005 text  21st may  ...  win cash  \\\n",
       "0              0          0            0          1         1  ...         0   \n",
       "1              0          0            0          0         0  ...         0   \n",
       "2              0          0            0          0         0  ...         0   \n",
       "3              0          0            0          0         0  ...         0   \n",
       "4              0          0            0          0         0  ...         0   \n",
       "5              0          0            0          0         0  ...         0   \n",
       "6              0          0            0          0         0  ...         0   \n",
       "7              0          0            0          0         0  ...         0   \n",
       "8              1          1            1          0         0  ...         1   \n",
       "9              0          0            0          0         0  ...         0   \n",
       "10             0          0            0          0         0  ...         0   \n",
       "11             0          0            0          0         0  ...         0   \n",
       "12             0          0            0          0         0  ...         0   \n",
       "13             0          0            0          0         0  ...         0   \n",
       "14             0          0            0          0         0  ...         0   \n",
       "15             0          0            0          0         0  ...         0   \n",
       "16             0          0            0          0         0  ...         0   \n",
       "17             0          0            0          0         0  ...         0   \n",
       "18             0          0            0          0         0  ...         0   \n",
       "19             0          0            0          0         0  ...         0   \n",
       "\n",
       "    win fa  winner as  with will  wkli comp  word claim  wwwdbuknet lccltd  \\\n",
       "0        1          0          0          1           0                  0   \n",
       "1        0          0          0          0           0                  0   \n",
       "2        0          0          0          0           0                  0   \n",
       "3        0          0          1          0           0                  0   \n",
       "4        0          0          0          0           0                  0   \n",
       "5        0          1          0          0           0                  0   \n",
       "6        0          0          0          0           0                  0   \n",
       "7        0          0          0          0           0                  0   \n",
       "8        0          0          0          0           0                  0   \n",
       "9        0          0          0          0           1                  1   \n",
       "10       0          0          0          0           0                  0   \n",
       "11       0          0          0          0           0                  0   \n",
       "12       0          0          0          0           0                  0   \n",
       "13       0          0          0          0           0                  0   \n",
       "14       0          0          0          0           0                  0   \n",
       "15       0          0          0          0           0                  0   \n",
       "16       0          0          0          0           0                  0   \n",
       "17       0          0          0          0           0                  0   \n",
       "18       0          0          0          0           0                  0   \n",
       "19       0          0          0          0           0                  0   \n",
       "\n",
       "    xxxmobilemovieclub to  ye he  you week  \n",
       "0                       0      0         0  \n",
       "1                       0      0         0  \n",
       "2                       0      0         0  \n",
       "3                       0      0         0  \n",
       "4                       0      0         0  \n",
       "5                       0      0         0  \n",
       "6                       0      0         0  \n",
       "7                       0      0         0  \n",
       "8                       0      0         0  \n",
       "9                       0      0         1  \n",
       "10                      1      0         0  \n",
       "11                      0      0         0  \n",
       "12                      0      1         0  \n",
       "13                      0      0         0  \n",
       "14                      0      0         0  \n",
       "15                      0      0         0  \n",
       "16                      0      0         0  \n",
       "17                      0      0         0  \n",
       "18                      0      0         0  \n",
       "19                      0      0         0  \n",
       "\n",
       "[20 rows x 217 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the sparse matrix of bigram counts into a dense array and create a DataFrame from it\n",
    "X_counts_df = pd.DataFrame(X_counts_sample.toarray())\n",
    "\n",
    "# Assign the bigram feature names as column headers in the DataFrame\n",
    "X_counts_df.columns = ngram_vect_sample.get_feature_names_out()\n",
    "\n",
    "# Display the DataFrame with bigram counts for each document in the sample\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa5bf8-7db1-45a3-ba93-c24d6c3ed0c4",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/tfidf.png\"     style=\" width:800px;  \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c4ef7-d692-4e06-81b9-de4d3d3c2929",
   "metadata": {},
   "source": [
    "**Term Frequency-Inverse Document Frequency (TF- IDF)** creates a document-term matrix where the columns single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document. It is a inverse document frequency weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5db560-3b90-42ee-bbb6-702d5007cb82",
   "metadata": {},
   "source": [
    "### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc242002-b4fc-4ea5-a9e3-d582457dd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"data/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610a1fa-6b18-4ac0-8952-5d9049a80619",
   "metadata": {},
   "source": [
    "#### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "562817cd-c782-4ef8-8d91-0043ec21f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036b39d-d784-4390-bf74-0682884b5b7c",
   "metadata": {},
   "source": [
    "## Apply TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9434c17-9755-479a-9785-e357604b8fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 8104)\n",
      "['' '0' '008704050406' ... 'ü' 'üll' '〨ud']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer with a custom analyzer function for text preprocessing\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Fit the vectorizer to the raw text data and transform it into a TF-IDF matrix\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "# Print the shape of the resulting matrix (number of documents, number of unique terms)\n",
    "print(X_tfidf.shape)\n",
    "\n",
    "# Print the list of feature names (terms) extracted from the data\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9bef4-686f-4b5c-a372-ff64b9384727",
   "metadata": {},
   "source": [
    "TF-IDF creates a document term matrix, where there's still one row per text message and the columns still represent single unique terms. But instead of the cells representing the count, the cells represent a weighting that's meant to identify **how important a word is to an individual text message**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a79ac7-29ab-4345-a5d2-9c6b269a5e3f",
   "metadata": {},
   "source": [
    "#### Apply TfidfVectorizer to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5596140-6290-424d-af8f-c6b3b8690d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 192)\n",
      "['08002986030' '08452810075over18' '09061701461' '1' '100' '100000' '11'\n",
      " '12' '150pday' '16' '2' '20000' '2005' '21st' '3' '4' '4403ldnw1a7rw18'\n",
      " '4txtú120' '6day' '81010' '87077' '87121' '87575' '9' '900' 'aft' 'aid'\n",
      " 'alreadi' 'alright' 'anymor' 'appli' 'ard' 'around' 'b' 'brother' 'call'\n",
      " 'caller' 'callertun' 'camera' 'cash' 'chanc' 'claim' 'click' 'co' 'code'\n",
      " 'colour' 'comin' 'comp' 'copi' 'cost' 'credit' 'cri' 'csh11' 'cup'\n",
      " 'custom' 'da' 'date' 'dont' 'eg' 'eh' 'england' 'enough' 'entitl' 'entri'\n",
      " 'even' 'fa' 'feel' 'ffffffffff' 'final' 'fine' 'finish' 'first' 'free'\n",
      " 'friend' 'go' 'goalsteam' 'goe' 'gonna' 'gota' 'ha' 'hl' 'home' 'hour'\n",
      " 'httpwap' 'im' 'info' 'ive' 'jackpot' 'joke' 'k' 'kim' 'kl341' 'lar'\n",
      " 'latest' 'lccltd' 'like' 'link' 'live' 'lor' 'lunch' 'macedonia' 'make'\n",
      " 'may' 'meet' 'mell' 'membership' 'messag' 'minnaminungint' 'miss' 'mobil'\n",
      " 'month' 'nah' 'name' 'nation' 'naughti' 'network' 'news' 'next' 'nurungu'\n",
      " 'oh' 'oru' 'patent' 'pay' 'per' 'pobox' 'poboxox36504w45wq' 'pound'\n",
      " 'press' 'prize' 'questionstd' 'r' 'ratetc' 'receiv' 'receivea' 'rememb'\n",
      " 'repli' 'request' 'reward' 'scotland' 'select' 'send' 'serious' 'set'\n",
      " 'six' 'smth' 'soon' 'sooner' 'speak' 'spell' 'stock' 'str' 'stuff'\n",
      " 'sunday' 'talk' 'tc' 'team' 'text' 'think' 'though' 'tkt' 'today'\n",
      " 'tonight' 'treat' 'tri' 'trywal' 'tsandc' 'txt' 'u' 'updat' 'ur' 'urgent'\n",
      " 'use' 'usf' 'v' 'valid' 'valu' 'vettam' 'want' 'wap' 'watch' 'way' 'week'\n",
      " 'wet' 'win' 'winner' 'wkli' 'word' 'wwwdbuknet' 'xxxmobilemovieclub'\n",
      " 'xxxmobilemovieclubcomnqjkgighjjgcbl' 'ye' 'ü']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data[0:20]\n",
    "\n",
    "# Initialize TfidfVectorizer with a custom analyzer function for text preprocessing\n",
    "tfidf_vect_sample = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Fit the vectorizer to the text data in the sample and transform it into a TF-IDF matrix\n",
    "X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])\n",
    "\n",
    "# Print the shape of the resulting matrix (number of documents, number of unique terms)\n",
    "print(X_tfidf_sample.shape)\n",
    "\n",
    "# Print the list of feature names (terms) extracted from the data\n",
    "print(tfidf_vect_sample.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629f790-a91a-4b1c-8c3f-4e885fa98973",
   "metadata": {},
   "source": [
    "#### Vectorizers output sparse matrices\n",
    "\n",
    "_**Sparse Matrix**: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c4a991-1df8-4a59-9dee-b23e6d3cf838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08002986030</th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>09061701461</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>100000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>150pday</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>wet</th>\n",
       "      <th>win</th>\n",
       "      <th>winner</th>\n",
       "      <th>wkli</th>\n",
       "      <th>word</th>\n",
       "      <th>wwwdbuknet</th>\n",
       "      <th>xxxmobilemovieclub</th>\n",
       "      <th>xxxmobilemovieclubcomnqjkgighjjgcbl</th>\n",
       "      <th>ye</th>\n",
       "      <th>ü</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.197682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224905</td>\n",
       "      <td>0.197695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252972</td>\n",
       "      <td>0.252972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291197</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    08002986030  08452810075over18  09061701461         1       100    100000  \\\n",
       "0      0.000000           0.198986     0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "2      0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "3      0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "4      0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "5      0.000000           0.000000     0.231645  0.000000  0.000000  0.000000   \n",
       "6      0.197682           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "7      0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "8      0.000000           0.000000     0.000000  0.000000  0.224905  0.000000   \n",
       "9      0.000000           0.000000     0.000000  0.252972  0.000000  0.252972   \n",
       "10     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "11     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "12     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "13     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "14     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "15     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "16     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "17     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "18     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "19     0.000000           0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          11        12   150pday        16  ...       wet       win    winner  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.174912  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.231645  0.000000  0.000000  ...  0.000000  0.000000  0.231645   \n",
       "6   0.197682  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.224905  0.197695  ...  0.000000  0.197695  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  ...  0.291197  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  0.185730  ...  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        wkli      word  wwwdbuknet  xxxmobilemovieclub  \\\n",
       "0   0.198986  0.000000    0.000000            0.000000   \n",
       "1   0.000000  0.000000    0.000000            0.000000   \n",
       "2   0.000000  0.000000    0.000000            0.000000   \n",
       "3   0.000000  0.000000    0.000000            0.000000   \n",
       "4   0.000000  0.000000    0.000000            0.000000   \n",
       "5   0.000000  0.000000    0.000000            0.000000   \n",
       "6   0.000000  0.000000    0.000000            0.000000   \n",
       "7   0.000000  0.000000    0.000000            0.000000   \n",
       "8   0.000000  0.000000    0.000000            0.000000   \n",
       "9   0.000000  0.252972    0.252972            0.000000   \n",
       "10  0.000000  0.000000    0.000000            0.272652   \n",
       "11  0.000000  0.000000    0.000000            0.000000   \n",
       "12  0.000000  0.000000    0.000000            0.000000   \n",
       "13  0.000000  0.000000    0.000000            0.000000   \n",
       "14  0.000000  0.000000    0.000000            0.000000   \n",
       "15  0.000000  0.000000    0.000000            0.000000   \n",
       "16  0.000000  0.000000    0.000000            0.000000   \n",
       "17  0.000000  0.000000    0.000000            0.000000   \n",
       "18  0.000000  0.000000    0.000000            0.000000   \n",
       "19  0.000000  0.000000    0.000000            0.000000   \n",
       "\n",
       "    xxxmobilemovieclubcomnqjkgighjjgcbl        ye         ü  \n",
       "0                              0.000000  0.000000  0.000000  \n",
       "1                              0.000000  0.000000  0.000000  \n",
       "2                              0.000000  0.000000  0.000000  \n",
       "3                              0.000000  0.000000  0.000000  \n",
       "4                              0.000000  0.000000  0.000000  \n",
       "5                              0.000000  0.000000  0.000000  \n",
       "6                              0.000000  0.000000  0.000000  \n",
       "7                              0.000000  0.000000  0.000000  \n",
       "8                              0.000000  0.000000  0.000000  \n",
       "9                              0.000000  0.000000  0.000000  \n",
       "10                             0.272652  0.000000  0.000000  \n",
       "11                             0.000000  0.000000  0.000000  \n",
       "12                             0.000000  0.291197  0.000000  \n",
       "13                             0.000000  0.000000  0.000000  \n",
       "14                             0.000000  0.000000  0.000000  \n",
       "15                             0.000000  0.000000  0.000000  \n",
       "16                             0.000000  0.000000  0.000000  \n",
       "17                             0.000000  0.000000  0.377964  \n",
       "18                             0.000000  0.000000  0.000000  \n",
       "19                             0.000000  0.000000  0.000000  \n",
       "\n",
       "[20 rows x 192 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the sparse TF-IDF matrix of the sample into a dense array and create a DataFrame from it\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())\n",
    "\n",
    "# Assign the term feature names as column headers in the DataFrame\n",
    "X_tfidf_df.columns = tfidf_vect_sample.get_feature_names_out()\n",
    "\n",
    "# Display the DataFrame with TF-IDF values for each document in the sample\n",
    "X_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3de95-f69d-44e0-b9be-42e6c896f95f",
   "metadata": {},
   "source": [
    "Instead of regular integers in the cells, you have decimals. This .2316 is likely more important than this .1977. This .2316 is likely more important than this .1976. What that means is, either 12 occurs more frequently in the 5th text message than 11 does in the 6th text message, or it means 12 occurs less frequently across all the other text messages than 11 does across all the other text messages. \n",
    "\n",
    "So in summary, we created this false choice here, indicating that there are three different ways to vectorize. These are all very closely related, though, and some can actually be used together. **TF-IDF** is basically a count vectorizer that includes some consideration for the length of the document, and also how common the word is across other text messages. And then **n-grams** is just used within either of these two methods to look for groups of adjacent words instead of just looking for single terms. They're all just slight modifications of each other, and typically you'll test different vectorization methods depending on your problem, and then you let the results determine which one you use. That wraps up our vectorization section. Next, we're going to learn about feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
